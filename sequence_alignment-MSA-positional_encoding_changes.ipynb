{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') #to avoid issues in the dataloading\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1378df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20891a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 4\n",
    "rep = torch.tensor([4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0, 0, 0, 0, 0, 0, 0,0, 0])\n",
    "#max 8 \n",
    "rep = torch.tensor([8, 8, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 5, 7, 8, 8, 8, 8, 8])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0.2, 0.5, 0.3, 0.9, 0.8, 0.5, 0.9, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=2000,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by precomputing a bunch of data on the sequence families\n",
    "        \"\"\"\n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto',rep=rep,rand=rand): \n",
    "        \"\"\"\n",
    "        input idx of the family of the sample\n",
    "        return a Tensor containing several samples from the family corresponding to the index\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        bin_n = len(rep) #for biasing the sampling\n",
    "        \n",
    "        precomputed_pos = [] #positions of the amino-acids\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        #sampling more for big families and long sequences\n",
    "        if type(sample_size) != int:\n",
    "            sample_s = min(n * l,25_000)\n",
    "            coef = round((sample_s)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1): #computing PID and removing aligned '-' ##might need to compute the actual column num\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_i != '-' and aa_j != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0 #case 0/0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = [] #possible position to take\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "            \n",
    "            # biasing for more diverse PID  \n",
    "            bin_idx = int(PID_ij//(1/bin_n))\n",
    "            rep_number = rep[bin_idx].clone()\n",
    "            if torch.rand(1) < rand[bin_idx]:\n",
    "                rep_number+=1\n",
    "            \n",
    "            for _ in range(rep_number):\n",
    "                try:   \n",
    "                    k = np.random.choice(possible_k)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #adding to the output\n",
    "                lengths.append(align_l)\n",
    "                pos_ij = (k + precomputed_pos)\n",
    "                pos.append(pos_ij)\n",
    "                \n",
    "                #computing the windows\n",
    "                window_i = ''\n",
    "                window_j = ''\n",
    "                for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                    if w < 0 or w >= align_l: #case of the edges\n",
    "                        window_i += 'Z'\n",
    "                        window_j += 'Z'\n",
    "                    else:\n",
    "                        window_i += seq_i[w]\n",
    "                        window_j += seq_j[w]\n",
    "\n",
    "                y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "                X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "\n",
    "                X.append(X_i)\n",
    "                y.append(y_j)\n",
    "                PIDs.append(PID_ij)\n",
    "                #computing the local PID\n",
    "                local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "\n",
    "                loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "                try:\n",
    "                    tmp = local_PID_ij/loc_comp  \n",
    "                except:\n",
    "                    tmp = 0 #case 0/0\n",
    "\n",
    "                local_PIDs.append(tmp)\n",
    "                pfreqs.append(self.aa_freqs[idx])\n",
    "                p_i_freqs = self.p_aa_freqs[idx][i]\n",
    "                p_j_freqs = self.p_aa_freqs[idx][j]\n",
    "\n",
    "                local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "\n",
    "                assert y_j < 20\n",
    "                assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache() #clearing the cache\n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        out = X,y.long(),PIDs,local_PIDs,pfreqs,local_pfreqs,pos,lengths\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run This for new data\n",
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "#To load datasets with all features computed\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_dataset.cont_size = CONT_SIZE\n",
    "test_dataset.cont_size = CONT_SIZE\n",
    "val_dataset.cont_size = CONT_SIZE\n",
    "\n",
    "train_dataset.div = 2000\n",
    "test_dataset.div = 2000\n",
    "val_dataset.div = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Transforms a list of tensors to a batch tensor\n",
    "    \"\"\"\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[4] for item in batch],dim=0)\n",
    "    lpfreqs = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[6] for item in batch],dim=0)\n",
    "    length = torch.cat([item[7] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID,pfreqs,lpfreqs, pos, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06957f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.Q_w_tgt = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w_tgt = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w_tgt = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att_tgt = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.att_src = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        Q_tgt = self.Q_w_tgt(tgt)\n",
    "        K_tgt = self.K_w_tgt(tgt)\n",
    "        V_tgt = self.V_w_tgt(tgt)\n",
    "        \n",
    "        Q_src = self.V_w_src(src)\n",
    "        K_src = self.K_w_src(src)\n",
    "        V_src = self.V_w_src(src)\n",
    "        \n",
    "        out_src,_ = self.att_src(Q_src,K_tgt,V_tgt,need_weights=False)\n",
    "        out_tgt,_ = self.att_tgt(Q_tgt,K_src,V_src,need_weights=False)\n",
    "        \n",
    "        out = torch.stack((out_src,out_tgt),dim=1)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb64bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastCrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        Q = self.Q_w(tgt)\n",
    "        K = self.K_w(src)\n",
    "        V = self.V_w(src)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1da4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Row-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*n,l,d))).view(b,n,l,d)\n",
    "        return out\n",
    "    \n",
    "class ColAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Column-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*l,n,d))).view((b,n,l,d))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971917c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward2D(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or 2*in_features\n",
    "        hidden_dim = wide_factor * 2*in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(2*in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = x.view(b,l,n*d)\n",
    "        out = self.lin1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        return out.view(b,n,l,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(from the timm library)\n",
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.row_att_block = RowAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff2d = FeedFoward2D(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.row_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff2d(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd38d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Last Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cross_att_block = LastCrossAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x[:,1] + self.drop_path(self.cross_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ec3ee",
   "metadata": {},
   "source": [
    "ADD Linear + Act d-wise before clf_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier Head of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    \"\"\"\n",
    "    Returns the initialization parameters of the Transformer\n",
    "    \"\"\"\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "085855a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-like neural net\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,num_seq=2,seq_len=2*CONT_SIZE+4,clf_dims=[256,64],cont_size=CONT_SIZE):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        blocks = []\n",
    "        r = min(len(num_heads),len(head_dims),len(wide_factors),len(drops))\n",
    "        for i,(n_h, h_d,w,d) in enumerate(zip(num_heads,head_dims,wide_factors,drops)):\n",
    "            if i < (r-1):\n",
    "                blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "            else:\n",
    "                #blocks.append(LastBlock(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "                blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "                \n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.clf = Classifier_Head(2*in_features,clf_dims,out_size=out_size,seq_len=seq_len)\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,2*in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "        \n",
    "    def to_input(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        #aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1) # modifier 0 1 pour src tgt ?\n",
    "        #aa_pos = aa_pos.unsqueeze(2)                            #\n",
    "        \n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2 \n",
    "        \n",
    "        src = torch.zeros_like(pos[:,:2*self.cont_size+1].unsqueeze(2))\n",
    "        tgt = torch.ones_like(pos[:,:2*self.cont_size+1].unsqueeze(2))\n",
    "        seq1 = torch.cat([seq1,src],dim=2)\n",
    "        seq2 = torch.cat([seq2,tgt],dim=2)\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(4000**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(4000**(2*i/pos_dim))).unsqueeze(2)\n",
    "            seq1 = torch.cat([seq1,p,ip],dim=2)\n",
    "            seq2 = torch.cat([seq2,p,ip],dim=2)\n",
    "        \n",
    "        out = torch.stack([seq1,seq2],dim=1)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pf = repeat(pf,\"b 1 d -> b 2 1 d\")\n",
    "        \n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        pid = rearrange(pid,\"b 1 (n e) -> b n 1 e\",n=2)\n",
    "        \n",
    "        lpf = F.pad(lpfreqs,pad=pad, mode='constant', value=0).unsqueeze(2)\n",
    "        out = torch.cat([out,pf,pid,lpf],dim=2)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def forward(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_input = self.to_input(x,PID,pfreqs,lpfreqs,pos,length)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        out = self.clf(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e37da1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y, PID, lPID,pfreqs,lpfreqs, pos, length in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e151260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ca1b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([506, 2, 64, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = model.to_input(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Used for checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b97a85f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "AttNet                                             [1, 20]                   400\n",
       "├─Sequential: 1-1                                  [1, 64]                   --\n",
       "│    └─Linear: 2-1                                 [1, 64]                   128\n",
       "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
       "├─Sequential: 1-2                                  [1, 2, 64, 32]            --\n",
       "│    └─Block: 2-3                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-2                          [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-4                      [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-5                          [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-4                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-7                       [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-8                          [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-10                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-11                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-5                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-14                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-16                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-17                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-6                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-19                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-20                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-22                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-23                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-7                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-25                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-26                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-27                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-28                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-29                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-30                        [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-8                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-31                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-32                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-33                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-34                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-35                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-36                        [1, 2, 64, 32]            64\n",
       "├─Classifier_Head: 1-3                             [1, 20]                   --\n",
       "│    └─Sequential: 2-9                             [1, 20]                   --\n",
       "│    │    └─Linear: 3-37                           [1, 512]                  2,097,664\n",
       "│    │    └─GELU: 3-38                             [1, 512]                  --\n",
       "│    │    └─Dropout: 3-39                          [1, 512]                  --\n",
       "│    │    └─Linear: 3-40                           [1, 128]                  65,664\n",
       "│    │    └─GELU: 3-41                             [1, 128]                  --\n",
       "│    │    └─Dropout: 3-42                          [1, 128]                  --\n",
       "│    │    └─Linear: 3-43                           [1, 32]                   4,128\n",
       "│    │    └─GELU: 3-44                             [1, 32]                   --\n",
       "│    │    └─Dropout: 3-45                          [1, 32]                   --\n",
       "│    │    └─Linear: 3-46                           [1, 20]                   660\n",
       "====================================================================================================\n",
       "Total params: 4,143,748\n",
       "Trainable params: 4,143,748\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.76\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 6.30\n",
       "Params size (MB): 10.26\n",
       "Estimated Total Size (MB): 16.57\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32])\n",
    "torchinfo.summary(model,[(1,4*CONT_SIZE+1,21),(1,),(1,20),(1,2,20),(1,4*CONT_SIZE+1),(1,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Substitution Moddef eval_model(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos, length in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                lPID = lPID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0982994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4d60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_acc_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "                \n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True,cont_size=CONT_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    \"\"\"\n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    LossMut = nn.BCELoss(reduction='sum')\n",
    "    EvalLoss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            lpfreqs = lpfreqs.to(device)\n",
    "            length = length.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "            \n",
    "\n",
    "            if use_mut:\n",
    "                X_idx = torch.argmax(X[:,cont_size],dim=1)\n",
    "                y_true = (X_idx == y).float().unsqueeze(1)  #0 if a mutation happens else 1 \n",
    "                y_pred = F.softmax(y_hat,dim=1)\n",
    "                y_pred = y_pred.gather(1,X_idx.view(-1,1)) #the Xth component of y_hat should be predicting ^\n",
    "                l = (Loss(y_hat,y) + LossMut(y_pred,y_true))/440 \n",
    "            else:\n",
    "                l = Loss(y_hat,y)/440 \n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ff1bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'models/state_6L_RowAtt_no_Cross_ATT.pth': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm \"models/state_6L_RowAtt_no_Cross_ATT.pth\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.9164950847625732, val_loss = 0.9301793575286865\n",
      "epoch n°1 : train_loss = 2.5058746337890625, val_loss = 0.7765276432037354\n",
      "epoch n°2 : train_loss = 2.2735297679901123, val_loss = 0.7460964322090149\n",
      "epoch n°3 : train_loss = 2.2090163230895996, val_loss = 0.7403761744499207\n",
      "epoch n°4 : train_loss = 2.188349962234497, val_loss = 0.7283726930618286\n",
      "epoch n°5 : train_loss = 2.1773993968963623, val_loss = 0.7259782552719116\n",
      "epoch n°6 : train_loss = 2.1596715450286865, val_loss = 0.7215684652328491\n",
      "epoch n°7 : train_loss = 2.1551859378814697, val_loss = 0.7139826416969299\n",
      "epoch n°8 : train_loss = 2.128491163253784, val_loss = 0.7142098546028137\n",
      "epoch n°9 : train_loss = 2.1392762660980225, val_loss = 0.7155492305755615\n",
      "epoch n°10 : train_loss = 2.1196136474609375, val_loss = 0.7118426561355591\n",
      "epoch n°11 : train_loss = 2.1152021884918213, val_loss = 0.7188133001327515\n",
      "epoch n°12 : train_loss = 2.130100965499878, val_loss = 0.7227359414100647\n",
      "epoch n°13 : train_loss = 2.111664295196533, val_loss = 0.7149474620819092\n",
      "epoch n°14 : train_loss = 2.115717649459839, val_loss = 0.7209199666976929\n",
      "epoch n°15 : train_loss = 2.107879877090454, val_loss = 0.7124382853507996\n",
      "epoch n°16 : train_loss = 2.121358871459961, val_loss = 0.7168287634849548\n",
      "epoch n°17 : train_loss = 2.1204190254211426, val_loss = 0.7231807112693787\n",
      "epoch n°18 : train_loss = 2.1249117851257324, val_loss = 0.7161170840263367\n",
      "epoch n°19 : train_loss = 2.1254305839538574, val_loss = 0.7199714183807373\n",
      "epoch n°20 : train_loss = 2.1052298545837402, val_loss = 0.7145064473152161\n",
      "epoch n°21 : train_loss = 2.104937791824341, val_loss = 0.7116875648498535\n",
      "epoch n°22 : train_loss = 2.105215072631836, val_loss = 0.7157837152481079\n",
      "epoch n°23 : train_loss = 2.0999209880828857, val_loss = 0.7124027013778687\n",
      "epoch n°24 : train_loss = 2.0954809188842773, val_loss = 0.7171018123626709\n",
      "epoch n°25 : train_loss = 2.0900769233703613, val_loss = 0.7108842134475708\n",
      "epoch n°26 : train_loss = 2.0873148441314697, val_loss = 0.7080904841423035\n",
      "epoch n°27 : train_loss = 2.0765023231506348, val_loss = 0.7085962295532227\n",
      "epoch n°28 : train_loss = 2.090299367904663, val_loss = 0.7127761840820312\n",
      "epoch n°29 : train_loss = 2.085083246231079, val_loss = 0.7052140235900879\n",
      "epoch n°30 : train_loss = 2.0696773529052734, val_loss = 0.7108474969863892\n",
      "epoch n°31 : train_loss = 2.073197841644287, val_loss = 0.7019228935241699\n",
      "epoch n°32 : train_loss = 2.0808966159820557, val_loss = 0.70406174659729\n",
      "epoch n°33 : train_loss = 2.0671353340148926, val_loss = 0.7059429287910461\n",
      "epoch n°34 : train_loss = 2.055373430252075, val_loss = 0.7050012946128845\n",
      "epoch n°35 : train_loss = 2.070025682449341, val_loss = 0.7052364349365234\n",
      "epoch n°36 : train_loss = 2.073793888092041, val_loss = 0.7029798030853271\n",
      "epoch n°37 : train_loss = 2.0559678077697754, val_loss = 0.7040230631828308\n",
      "epoch n°38 : train_loss = 2.0561881065368652, val_loss = 0.7050784826278687\n",
      "epoch n°39 : train_loss = 2.0585620403289795, val_loss = 0.6995364427566528\n",
      "epoch n°40 : train_loss = 2.0598108768463135, val_loss = 0.7043960094451904\n",
      "epoch n°41 : train_loss = 2.0641591548919678, val_loss = 0.7058595418930054\n",
      "epoch n°42 : train_loss = 2.0444278717041016, val_loss = 0.7033169269561768\n",
      "epoch n°43 : train_loss = 2.059950351715088, val_loss = 0.7011430859565735\n",
      "epoch n°44 : train_loss = 2.0566556453704834, val_loss = 0.7015629410743713\n",
      "epoch n°45 : train_loss = 2.044320821762085, val_loss = 0.6941426396369934\n",
      "epoch n°46 : train_loss = 2.048187732696533, val_loss = 0.7035230398178101\n",
      "epoch n°47 : train_loss = 2.0398075580596924, val_loss = 0.6996421217918396\n",
      "epoch n°48 : train_loss = 2.0729758739471436, val_loss = 0.7174361944198608\n",
      "epoch n°49 : train_loss = 2.069403648376465, val_loss = 0.7087805271148682\n",
      "epoch n°50 : train_loss = 2.062830924987793, val_loss = 0.6949509978294373\n",
      "epoch n°51 : train_loss = 2.060561418533325, val_loss = 0.7075994610786438\n",
      "epoch n°52 : train_loss = 2.0592238903045654, val_loss = 0.6985423564910889\n",
      "epoch n°53 : train_loss = 2.0700457096099854, val_loss = 0.7069441676139832\n",
      "epoch n°54 : train_loss = 2.0643744468688965, val_loss = 0.7079454064369202\n",
      "epoch n°55 : train_loss = 2.0514113903045654, val_loss = 0.7054228782653809\n",
      "epoch n°56 : train_loss = 2.066692352294922, val_loss = 0.7017900347709656\n",
      "epoch n°57 : train_loss = 2.054807186126709, val_loss = 0.6989795565605164\n",
      "epoch n°58 : train_loss = 2.0620439052581787, val_loss = 0.7052759528160095\n",
      "epoch n°59 : train_loss = 2.0489611625671387, val_loss = 0.6993529200553894\n",
      "epoch n°60 : train_loss = 2.050877094268799, val_loss = 0.699207067489624\n",
      "epoch n°61 : train_loss = 2.0490636825561523, val_loss = 0.6998525261878967\n",
      "epoch n°62 : train_loss = 2.045182228088379, val_loss = 0.7001944184303284\n",
      "epoch n°63 : train_loss = 2.042407751083374, val_loss = 0.6963211894035339\n",
      "epoch n°64 : train_loss = 2.035918712615967, val_loss = 0.6941405534744263\n",
      "epoch n°65 : train_loss = 2.0194456577301025, val_loss = 0.7023817300796509\n",
      "epoch n°66 : train_loss = 2.0221145153045654, val_loss = 0.6883995532989502\n",
      "epoch n°67 : train_loss = 2.015275716781616, val_loss = 0.6938889026641846\n",
      "epoch n°68 : train_loss = 2.0213756561279297, val_loss = 0.6905044913291931\n",
      "epoch n°69 : train_loss = 2.0221147537231445, val_loss = 0.6934300661087036\n",
      "epoch n°70 : train_loss = 2.014817714691162, val_loss = 0.6829415559768677\n",
      "epoch n°71 : train_loss = 2.0136663913726807, val_loss = 0.6819177269935608\n",
      "epoch n°72 : train_loss = 2.011594772338867, val_loss = 0.682293713092804\n",
      "epoch n°73 : train_loss = 1.9969356060028076, val_loss = 0.6876736283302307\n",
      "epoch n°74 : train_loss = 1.9868885278701782, val_loss = 0.6786983609199524\n",
      "epoch n°75 : train_loss = 1.998401403427124, val_loss = 0.6742051839828491\n",
      "epoch n°76 : train_loss = 1.9950175285339355, val_loss = 0.6881036758422852\n",
      "epoch n°77 : train_loss = 1.991607666015625, val_loss = 0.6783422231674194\n",
      "epoch n°78 : train_loss = 1.9975502490997314, val_loss = 0.6805680990219116\n",
      "epoch n°79 : train_loss = 1.9778218269348145, val_loss = 0.6778630018234253\n",
      "epoch n°80 : train_loss = 1.973261833190918, val_loss = 0.6845649480819702\n",
      "epoch n°81 : train_loss = 1.9882278442382812, val_loss = 0.6744199395179749\n",
      "epoch n°82 : train_loss = 1.982214093208313, val_loss = 0.675015926361084\n",
      "epoch n°83 : train_loss = 1.9789680242538452, val_loss = 0.6834396123886108\n",
      "epoch n°84 : train_loss = 1.9682608842849731, val_loss = 0.6722531318664551\n",
      "epoch n°85 : train_loss = 1.9683942794799805, val_loss = 0.6799461245536804\n",
      "epoch n°86 : train_loss = 1.9759876728057861, val_loss = 0.6786338090896606\n",
      "epoch n°87 : train_loss = 1.9761853218078613, val_loss = 0.6681915521621704\n",
      "epoch n°88 : train_loss = 1.9624347686767578, val_loss = 0.6743029952049255\n",
      "epoch n°89 : train_loss = 1.9631340503692627, val_loss = 0.6704172492027283\n",
      "epoch n°90 : train_loss = 1.9676949977874756, val_loss = 0.6804525256156921\n",
      "epoch n°91 : train_loss = 1.9790934324264526, val_loss = 0.680691659450531\n",
      "epoch n°92 : train_loss = 1.9647915363311768, val_loss = 0.6715466380119324\n",
      "epoch n°93 : train_loss = 1.9533573389053345, val_loss = 0.673181414604187\n",
      "epoch n°94 : train_loss = 1.9574449062347412, val_loss = 0.6706003546714783\n",
      "epoch n°95 : train_loss = 1.9505783319473267, val_loss = 0.6774895191192627\n",
      "epoch n°96 : train_loss = 1.9560768604278564, val_loss = 0.6765419840812683\n",
      "epoch n°97 : train_loss = 1.9479100704193115, val_loss = 0.6808173656463623\n",
      "epoch n°98 : train_loss = 1.9561271667480469, val_loss = 0.6719484925270081\n",
      "epoch n°99 : train_loss = 1.9671698808670044, val_loss = 0.6731560230255127\n",
      "epoch n°100 : train_loss = 1.9471290111541748, val_loss = 0.678614616394043\n",
      "epoch n°101 : train_loss = 1.9418246746063232, val_loss = 0.6676356196403503\n",
      "epoch n°102 : train_loss = 1.954766869544983, val_loss = 0.6712799668312073\n",
      "epoch n°103 : train_loss = 1.9472321271896362, val_loss = 0.6701743006706238\n",
      "epoch n°104 : train_loss = 1.962254524230957, val_loss = 0.6725561618804932\n",
      "epoch n°105 : train_loss = 1.9490548372268677, val_loss = 0.6746481657028198\n",
      "epoch n°106 : train_loss = 1.9476637840270996, val_loss = 0.6728798747062683\n",
      "epoch n°107 : train_loss = 1.9443814754486084, val_loss = 0.6682778000831604\n",
      "epoch n°108 : train_loss = 1.9585694074630737, val_loss = 0.672617495059967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 1.952756404876709, val_loss = 0.6716246008872986\n",
      "epoch n°110 : train_loss = 1.946796178817749, val_loss = 0.6738977432250977\n",
      "epoch n°111 : train_loss = 1.9537872076034546, val_loss = 0.6719339489936829\n",
      "epoch n°112 : train_loss = 1.9876617193222046, val_loss = 0.6798166632652283\n",
      "epoch n°113 : train_loss = 1.9968239068984985, val_loss = 0.6784310340881348\n",
      "epoch n°114 : train_loss = 1.9906690120697021, val_loss = 0.6800185441970825\n",
      "epoch n°115 : train_loss = 1.9929101467132568, val_loss = 0.680077314376831\n",
      "epoch n°116 : train_loss = 1.9815495014190674, val_loss = 0.6814583539962769\n",
      "epoch n°117 : train_loss = 1.98116934299469, val_loss = 0.6803761124610901\n",
      "epoch n°118 : train_loss = 1.990407109260559, val_loss = 0.6779404282569885\n",
      "epoch n°119 : train_loss = 1.976846694946289, val_loss = 0.6723864674568176\n",
      "epoch n°120 : train_loss = 1.9743103981018066, val_loss = 0.6814124584197998\n",
      "epoch n°121 : train_loss = 1.9891349077224731, val_loss = 0.6835874915122986\n",
      "epoch n°122 : train_loss = 1.983232021331787, val_loss = 0.6756060719490051\n",
      "epoch n°123 : train_loss = 1.9791797399520874, val_loss = 0.6778444647789001\n",
      "epoch n°124 : train_loss = 1.9760730266571045, val_loss = 0.6755744814872742\n",
      "epoch n°125 : train_loss = 1.975193738937378, val_loss = 0.6823691129684448\n",
      "epoch n°126 : train_loss = 1.9717150926589966, val_loss = 0.6729257106781006\n",
      "epoch n°127 : train_loss = 1.9697469472885132, val_loss = 0.6877562999725342\n",
      "epoch n°128 : train_loss = 1.9756044149398804, val_loss = 0.6751481890678406\n",
      "epoch n°129 : train_loss = 1.968329668045044, val_loss = 0.6697342395782471\n",
      "epoch n°130 : train_loss = 1.9713366031646729, val_loss = 0.6834648251533508\n",
      "epoch n°131 : train_loss = 1.9577326774597168, val_loss = 0.6819406747817993\n",
      "epoch n°132 : train_loss = 1.9690264463424683, val_loss = 0.6765310168266296\n",
      "epoch n°133 : train_loss = 1.962697982788086, val_loss = 0.6774872541427612\n",
      "epoch n°134 : train_loss = 1.9654773473739624, val_loss = 0.6771097779273987\n",
      "epoch n°135 : train_loss = 1.9721873998641968, val_loss = 0.6758160591125488\n",
      "epoch n°136 : train_loss = 1.9609853029251099, val_loss = 0.6728613972663879\n",
      "epoch n°137 : train_loss = 1.9707868099212646, val_loss = 0.6792053580284119\n",
      "epoch n°138 : train_loss = 1.9543750286102295, val_loss = 0.670131266117096\n",
      "epoch n°139 : train_loss = 1.972106695175171, val_loss = 0.6787556409835815\n",
      "epoch n°140 : train_loss = 1.9549450874328613, val_loss = 0.6697165369987488\n",
      "epoch n°141 : train_loss = 1.965223789215088, val_loss = 0.6799362301826477\n",
      "epoch n°142 : train_loss = 1.954613447189331, val_loss = 0.6748940348625183\n",
      "epoch n°143 : train_loss = 1.9530529975891113, val_loss = 0.6761192083358765\n",
      "epoch n°144 : train_loss = 1.9412342309951782, val_loss = 0.6713857054710388\n",
      "epoch n°145 : train_loss = 1.9567828178405762, val_loss = 0.6756544709205627\n",
      "epoch n°146 : train_loss = 1.959710955619812, val_loss = 0.672307014465332\n",
      "epoch n°147 : train_loss = 1.9566941261291504, val_loss = 0.6704668998718262\n",
      "epoch n°148 : train_loss = 1.9522885084152222, val_loss = 0.6706797480583191\n",
      "epoch n°149 : train_loss = 1.956766963005066, val_loss = 0.6705256700515747\n",
      "epoch n°150 : train_loss = 1.954206109046936, val_loss = 0.6729423403739929\n",
      "epoch n°151 : train_loss = 1.9602642059326172, val_loss = 0.6730980277061462\n",
      "epoch n°152 : train_loss = 1.9482955932617188, val_loss = 0.6703106164932251\n",
      "epoch n°153 : train_loss = 1.950579047203064, val_loss = 0.6761474609375\n",
      "epoch n°154 : train_loss = 1.9450386762619019, val_loss = 0.6732346415519714\n",
      "epoch n°155 : train_loss = 1.9429374933242798, val_loss = 0.6735823750495911\n",
      "epoch n°156 : train_loss = 1.9483054876327515, val_loss = 0.6689864993095398\n",
      "epoch n°157 : train_loss = 1.9506903886795044, val_loss = 0.674309253692627\n",
      "epoch n°158 : train_loss = 1.9469435214996338, val_loss = 0.6696858406066895\n",
      "epoch n°159 : train_loss = 1.9376864433288574, val_loss = 0.6774430274963379\n",
      "epoch n°160 : train_loss = 1.937461495399475, val_loss = 0.6670781373977661\n",
      "epoch n°161 : train_loss = 1.945733904838562, val_loss = 0.6723529100418091\n",
      "epoch n°162 : train_loss = 1.9391515254974365, val_loss = 0.6759276986122131\n",
      "epoch n°163 : train_loss = 1.9280565977096558, val_loss = 0.6721169948577881\n",
      "epoch n°164 : train_loss = 1.9317803382873535, val_loss = 0.6695590615272522\n",
      "epoch n°165 : train_loss = 1.9483238458633423, val_loss = 0.674791157245636\n",
      "epoch n°166 : train_loss = 1.9276118278503418, val_loss = 0.670802116394043\n",
      "epoch n°167 : train_loss = 1.9402821063995361, val_loss = 0.6732238531112671\n",
      "epoch n°168 : train_loss = 1.9351664781570435, val_loss = 0.6695699691772461\n",
      "epoch n°169 : train_loss = 1.9313645362854004, val_loss = 0.6754770278930664\n",
      "epoch n°170 : train_loss = 1.9361838102340698, val_loss = 0.6675423383712769\n",
      "epoch n°171 : train_loss = 1.9399665594100952, val_loss = 0.6785301566123962\n",
      "epoch n°172 : train_loss = 1.9398128986358643, val_loss = 0.6706059575080872\n",
      "epoch n°173 : train_loss = 1.9239001274108887, val_loss = 0.669433057308197\n",
      "epoch n°174 : train_loss = 1.939967393875122, val_loss = 0.6662608981132507\n",
      "epoch n°175 : train_loss = 1.9360125064849854, val_loss = 0.6707191467285156\n",
      "epoch n°176 : train_loss = 1.9221211671829224, val_loss = 0.6686896085739136\n",
      "epoch n°177 : train_loss = 1.9213106632232666, val_loss = 0.6619707942008972\n",
      "epoch n°178 : train_loss = 1.9298326969146729, val_loss = 0.6658630967140198\n",
      "epoch n°179 : train_loss = 1.9193137884140015, val_loss = 0.6699776649475098\n",
      "epoch n°180 : train_loss = 1.9350712299346924, val_loss = 0.6716054081916809\n",
      "epoch n°181 : train_loss = 1.9262707233428955, val_loss = 0.6673169732093811\n",
      "epoch n°182 : train_loss = 1.9121767282485962, val_loss = 0.6599358916282654\n",
      "epoch n°183 : train_loss = 1.9219155311584473, val_loss = 0.6674325466156006\n",
      "epoch n°184 : train_loss = 1.9337021112442017, val_loss = 0.6670941114425659\n",
      "epoch n°185 : train_loss = 1.9150720834732056, val_loss = 0.6700955033302307\n",
      "epoch n°186 : train_loss = 1.9182254076004028, val_loss = 0.6687098145484924\n",
      "epoch n°187 : train_loss = 1.9279884099960327, val_loss = 0.6701095104217529\n",
      "epoch n°188 : train_loss = 1.9216620922088623, val_loss = 0.667186975479126\n",
      "epoch n°189 : train_loss = 1.9225327968597412, val_loss = 0.6662625074386597\n",
      "epoch n°190 : train_loss = 1.92177414894104, val_loss = 0.6619397401809692\n",
      "epoch n°191 : train_loss = 1.9195072650909424, val_loss = 0.6629367470741272\n",
      "epoch n°192 : train_loss = 1.9188979864120483, val_loss = 0.6625789403915405\n",
      "epoch n°193 : train_loss = 1.9184819459915161, val_loss = 0.6624549031257629\n",
      "epoch n°194 : train_loss = 1.9170523881912231, val_loss = 0.6645514965057373\n",
      "epoch n°195 : train_loss = 1.9256973266601562, val_loss = 0.6676827073097229\n",
      "epoch n°196 : train_loss = 1.9107979536056519, val_loss = 0.6668177843093872\n",
      "epoch n°197 : train_loss = 1.906162142753601, val_loss = 0.6704369783401489\n",
      "epoch n°198 : train_loss = 1.9059802293777466, val_loss = 0.6645376682281494\n",
      "epoch n°199 : train_loss = 1.919201374053955, val_loss = 0.6653695106506348\n",
      "epoch n°200 : train_loss = 1.9195536375045776, val_loss = 0.6697646379470825\n",
      "epoch n°201 : train_loss = 1.9005569219589233, val_loss = 0.6636870503425598\n",
      "epoch n°202 : train_loss = 1.917545199394226, val_loss = 0.6608659029006958\n",
      "epoch n°203 : train_loss = 1.9087642431259155, val_loss = 0.6738184094429016\n",
      "epoch n°204 : train_loss = 1.903814435005188, val_loss = 0.6743233799934387\n",
      "epoch n°205 : train_loss = 1.9136465787887573, val_loss = 0.6617223024368286\n",
      "epoch n°206 : train_loss = 1.902115821838379, val_loss = 0.6674014925956726\n",
      "epoch n°207 : train_loss = 1.9115763902664185, val_loss = 0.6681157946586609\n",
      "epoch n°208 : train_loss = 1.916540503501892, val_loss = 0.6605924367904663\n",
      "epoch n°209 : train_loss = 1.9049649238586426, val_loss = 0.6732703447341919\n",
      "epoch n°210 : train_loss = 1.9055873155593872, val_loss = 0.6708562970161438\n",
      "epoch n°211 : train_loss = 1.9213786125183105, val_loss = 0.6650934815406799\n",
      "epoch n°212 : train_loss = 1.913915991783142, val_loss = 0.6627817153930664\n",
      "epoch n°213 : train_loss = 1.9123238325119019, val_loss = 0.6657426357269287\n",
      "epoch n°214 : train_loss = 1.9109382629394531, val_loss = 0.6661874651908875\n",
      "epoch n°215 : train_loss = 1.9136998653411865, val_loss = 0.6585699319839478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°216 : train_loss = 1.910143256187439, val_loss = 0.6607254147529602\n",
      "epoch n°217 : train_loss = 1.9154164791107178, val_loss = 0.6659353971481323\n",
      "epoch n°218 : train_loss = 1.8971574306488037, val_loss = 0.6695629358291626\n",
      "epoch n°219 : train_loss = 1.899552345275879, val_loss = 0.6654301881790161\n",
      "epoch n°220 : train_loss = 1.9019097089767456, val_loss = 0.6689672470092773\n",
      "epoch n°221 : train_loss = 1.9076564311981201, val_loss = 0.6640021800994873\n",
      "epoch n°222 : train_loss = 1.896068811416626, val_loss = 0.6673497557640076\n",
      "epoch n°223 : train_loss = 1.9193994998931885, val_loss = 0.6696968078613281\n",
      "epoch n°224 : train_loss = 1.910822868347168, val_loss = 0.6671245098114014\n",
      "epoch n°225 : train_loss = 1.9031072854995728, val_loss = 0.665939211845398\n",
      "epoch n°226 : train_loss = 1.9150052070617676, val_loss = 0.6586798429489136\n",
      "epoch n°227 : train_loss = 1.9086921215057373, val_loss = 0.6668168306350708\n",
      "epoch n°228 : train_loss = 1.9119137525558472, val_loss = 0.6613078117370605\n",
      "epoch n°229 : train_loss = 1.9083577394485474, val_loss = 0.6627269983291626\n",
      "epoch n°230 : train_loss = 1.8971625566482544, val_loss = 0.6651512384414673\n",
      "epoch n°231 : train_loss = 1.9183721542358398, val_loss = 0.666556179523468\n",
      "epoch n°232 : train_loss = 1.912455677986145, val_loss = 0.6613929271697998\n",
      "epoch n°233 : train_loss = 1.912865400314331, val_loss = 0.6608555912971497\n",
      "epoch n°234 : train_loss = 1.9055707454681396, val_loss = 0.6682740449905396\n",
      "epoch n°235 : train_loss = 1.9097435474395752, val_loss = 0.6607391834259033\n",
      "epoch n°236 : train_loss = 1.9168936014175415, val_loss = 0.6758913993835449\n",
      "epoch n°237 : train_loss = 1.911963701248169, val_loss = 0.666762113571167\n",
      "epoch n°238 : train_loss = 1.9113929271697998, val_loss = 0.6696478128433228\n",
      "epoch n°239 : train_loss = 1.8968884944915771, val_loss = 0.665973424911499\n",
      "epoch n°240 : train_loss = 1.9361076354980469, val_loss = 0.6757513284683228\n",
      "epoch n°241 : train_loss = 1.9337602853775024, val_loss = 0.6654443144798279\n",
      "epoch n°242 : train_loss = 1.9307130575180054, val_loss = 0.6768795847892761\n",
      "epoch n°243 : train_loss = 1.9259741306304932, val_loss = 0.6738836169242859\n",
      "epoch n°244 : train_loss = 1.9416497945785522, val_loss = 0.6690102219581604\n",
      "epoch n°245 : train_loss = 1.9236310720443726, val_loss = 0.6659907102584839\n",
      "epoch n°246 : train_loss = 1.9324142932891846, val_loss = 0.6709321737289429\n",
      "epoch n°247 : train_loss = 1.9354499578475952, val_loss = 0.6663476824760437\n",
      "epoch n°248 : train_loss = 1.9335092306137085, val_loss = 0.6760783791542053\n",
      "epoch n°249 : train_loss = 1.9284824132919312, val_loss = 0.6749435067176819\n",
      "epoch n°250 : train_loss = 1.9387080669403076, val_loss = 0.6756506562232971\n",
      "epoch n°251 : train_loss = 1.9320652484893799, val_loss = 0.6686908006668091\n",
      "epoch n°252 : train_loss = 1.9286469221115112, val_loss = 0.6705071926116943\n",
      "epoch n°253 : train_loss = 1.9287537336349487, val_loss = 0.6730712652206421\n",
      "epoch n°254 : train_loss = 1.932215690612793, val_loss = 0.6655758023262024\n",
      "epoch n°255 : train_loss = 1.9139710664749146, val_loss = 0.6724787354469299\n",
      "epoch n°256 : train_loss = 1.9275637865066528, val_loss = 0.6677074432373047\n",
      "epoch n°257 : train_loss = 1.9427298307418823, val_loss = 0.6750626564025879\n",
      "epoch n°258 : train_loss = 1.9363915920257568, val_loss = 0.6759673357009888\n",
      "epoch n°259 : train_loss = 1.9425264596939087, val_loss = 0.6683966517448425\n",
      "epoch n°260 : train_loss = 1.9279870986938477, val_loss = 0.6665799021720886\n",
      "epoch n°261 : train_loss = 1.930139183998108, val_loss = 0.6711334586143494\n",
      "epoch n°262 : train_loss = 1.9193533658981323, val_loss = 0.669183075428009\n",
      "epoch n°263 : train_loss = 1.9454150199890137, val_loss = 0.6666250228881836\n",
      "epoch n°264 : train_loss = 1.9294790029525757, val_loss = 0.6655004620552063\n",
      "epoch n°265 : train_loss = 1.9358339309692383, val_loss = 0.6731577515602112\n",
      "epoch n°266 : train_loss = 1.9309074878692627, val_loss = 0.6702769994735718\n",
      "epoch n°267 : train_loss = 1.9350014925003052, val_loss = 0.6750730276107788\n",
      "epoch n°268 : train_loss = 1.9202548265457153, val_loss = 0.6676746010780334\n",
      "epoch n°269 : train_loss = 1.9178714752197266, val_loss = 0.6718067526817322\n",
      "epoch n°270 : train_loss = 1.9105982780456543, val_loss = 0.6742101311683655\n",
      "epoch n°271 : train_loss = 1.9261237382888794, val_loss = 0.6623369455337524\n",
      "epoch n°272 : train_loss = 1.9216359853744507, val_loss = 0.6647892594337463\n",
      "epoch n°273 : train_loss = 1.927777886390686, val_loss = 0.6625419855117798\n",
      "epoch n°274 : train_loss = 1.9153244495391846, val_loss = 0.6762393116950989\n",
      "epoch n°275 : train_loss = 1.9172102212905884, val_loss = 0.6743718385696411\n",
      "epoch n°276 : train_loss = 1.9241260290145874, val_loss = 0.6755111217498779\n",
      "epoch n°277 : train_loss = 1.9232664108276367, val_loss = 0.6724130511283875\n",
      "epoch n°278 : train_loss = 1.9285732507705688, val_loss = 0.6718273758888245\n",
      "epoch n°279 : train_loss = 1.9184134006500244, val_loss = 0.6680147647857666\n",
      "epoch n°280 : train_loss = 1.9261126518249512, val_loss = 0.6663336753845215\n",
      "epoch n°281 : train_loss = 1.9143662452697754, val_loss = 0.6619219183921814\n",
      "epoch n°282 : train_loss = 1.928007960319519, val_loss = 0.6709485650062561\n",
      "epoch n°283 : train_loss = 1.9197717905044556, val_loss = 0.6640019416809082\n",
      "epoch n°284 : train_loss = 1.9178186655044556, val_loss = 0.6636549234390259\n",
      "epoch n°285 : train_loss = 1.9231313467025757, val_loss = 0.6678752303123474\n",
      "epoch n°286 : train_loss = 1.9153070449829102, val_loss = 0.6726603507995605\n",
      "epoch n°287 : train_loss = 1.9128248691558838, val_loss = 0.6641855239868164\n",
      "epoch n°288 : train_loss = 1.921452283859253, val_loss = 0.6768292784690857\n",
      "epoch n°289 : train_loss = 1.9301908016204834, val_loss = 0.6730341911315918\n",
      "epoch n°290 : train_loss = 1.9193612337112427, val_loss = 0.6593288779258728\n",
      "epoch n°291 : train_loss = 1.9170010089874268, val_loss = 0.6703665256500244\n",
      "epoch n°292 : train_loss = 1.9112690687179565, val_loss = 0.6634462475776672\n",
      "epoch n°293 : train_loss = 1.9128406047821045, val_loss = 0.6720522046089172\n",
      "epoch n°294 : train_loss = 1.9172656536102295, val_loss = 0.6740272641181946\n",
      "epoch n°295 : train_loss = 1.9095066785812378, val_loss = 0.671213686466217\n",
      "epoch n°296 : train_loss = 1.916443109512329, val_loss = 0.6724103093147278\n",
      "epoch n°297 : train_loss = 1.9232475757598877, val_loss = 0.6751763224601746\n",
      "epoch n°298 : train_loss = 1.9210389852523804, val_loss = 0.6677960753440857\n",
      "epoch n°299 : train_loss = 1.9164024591445923, val_loss = 0.6763125658035278\n",
      "epoch n°300 : train_loss = 1.9085252285003662, val_loss = 0.6634864211082458\n",
      "epoch n°301 : train_loss = 1.9192332029342651, val_loss = 0.6685482859611511\n",
      "epoch n°302 : train_loss = 1.912869930267334, val_loss = 0.6622622013092041\n",
      "epoch n°303 : train_loss = 1.9172601699829102, val_loss = 0.6670362949371338\n",
      "epoch n°304 : train_loss = 1.9111249446868896, val_loss = 0.6654061675071716\n",
      "epoch n°305 : train_loss = 1.9195749759674072, val_loss = 0.6589207053184509\n",
      "epoch n°306 : train_loss = 1.9106699228286743, val_loss = 0.670375645160675\n",
      "epoch n°307 : train_loss = 1.9154444932937622, val_loss = 0.6661831140518188\n",
      "epoch n°308 : train_loss = 1.9064728021621704, val_loss = 0.6677247881889343\n",
      "epoch n°309 : train_loss = 1.9089157581329346, val_loss = 0.6692638993263245\n",
      "epoch n°310 : train_loss = 1.9204362630844116, val_loss = 0.66749507188797\n",
      "epoch n°311 : train_loss = 1.9166364669799805, val_loss = 0.6692275404930115\n",
      "epoch n°312 : train_loss = 1.904356598854065, val_loss = 0.6717133522033691\n",
      "epoch n°313 : train_loss = 1.9023698568344116, val_loss = 0.6695131063461304\n",
      "epoch n°314 : train_loss = 1.904486060142517, val_loss = 0.6685249209403992\n",
      "epoch n°315 : train_loss = 1.9064326286315918, val_loss = 0.6643093824386597\n",
      "epoch n°316 : train_loss = 1.9110369682312012, val_loss = 0.6711595058441162\n",
      "epoch n°317 : train_loss = 1.9116566181182861, val_loss = 0.6614190340042114\n",
      "epoch n°318 : train_loss = 1.9096742868423462, val_loss = 0.6636099815368652\n",
      "epoch n°319 : train_loss = 1.9106059074401855, val_loss = 0.6665624976158142\n",
      "epoch n°320 : train_loss = 1.9032206535339355, val_loss = 0.6734573245048523\n",
      "epoch n°321 : train_loss = 1.9067912101745605, val_loss = 0.6685623526573181\n",
      "epoch n°322 : train_loss = 1.9080575704574585, val_loss = 0.6639087200164795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°323 : train_loss = 1.904062271118164, val_loss = 0.6732215285301208\n",
      "epoch n°324 : train_loss = 1.91096031665802, val_loss = 0.6604369282722473\n",
      "epoch n°325 : train_loss = 1.9112727642059326, val_loss = 0.6682208180427551\n",
      "epoch n°326 : train_loss = 1.9087170362472534, val_loss = 0.6621803045272827\n",
      "epoch n°327 : train_loss = 1.9108225107192993, val_loss = 0.6674312353134155\n",
      "epoch n°328 : train_loss = 1.9030710458755493, val_loss = 0.6630890369415283\n",
      "epoch n°329 : train_loss = 1.90312659740448, val_loss = 0.6639883518218994\n",
      "epoch n°330 : train_loss = 1.90785551071167, val_loss = 0.6604573130607605\n",
      "epoch n°331 : train_loss = 1.9121031761169434, val_loss = 0.6658099889755249\n",
      "epoch n°332 : train_loss = 1.901501178741455, val_loss = 0.6583817005157471\n",
      "epoch n°333 : train_loss = 1.9106526374816895, val_loss = 0.6653616428375244\n",
      "epoch n°334 : train_loss = 1.8919577598571777, val_loss = 0.664421021938324\n",
      "epoch n°335 : train_loss = 1.9122226238250732, val_loss = 0.6709938049316406\n",
      "epoch n°336 : train_loss = 1.9063615798950195, val_loss = 0.672179639339447\n",
      "epoch n°337 : train_loss = 1.9084752798080444, val_loss = 0.661461353302002\n",
      "epoch n°338 : train_loss = 1.8982425928115845, val_loss = 0.6693962216377258\n",
      "epoch n°339 : train_loss = 1.904850721359253, val_loss = 0.6626847982406616\n",
      "epoch n°340 : train_loss = 1.9081553220748901, val_loss = 0.6652087569236755\n",
      "epoch n°341 : train_loss = 1.904535174369812, val_loss = 0.6637042760848999\n",
      "epoch n°342 : train_loss = 1.8899182081222534, val_loss = 0.6582913398742676\n",
      "epoch n°343 : train_loss = 1.8884974718093872, val_loss = 0.6636179685592651\n",
      "epoch n°344 : train_loss = 1.9018511772155762, val_loss = 0.6649132370948792\n",
      "epoch n°345 : train_loss = 1.9004186391830444, val_loss = 0.6662328243255615\n",
      "epoch n°346 : train_loss = 1.9043492078781128, val_loss = 0.6595531105995178\n",
      "epoch n°347 : train_loss = 1.9015883207321167, val_loss = 0.6612145304679871\n",
      "epoch n°348 : train_loss = 1.892599105834961, val_loss = 0.6652504205703735\n",
      "epoch n°349 : train_loss = 1.9037959575653076, val_loss = 0.6603018641471863\n",
      "epoch n°350 : train_loss = 1.9058961868286133, val_loss = 0.661985456943512\n",
      "epoch n°351 : train_loss = 1.9011249542236328, val_loss = 0.6619324684143066\n",
      "epoch n°352 : train_loss = 1.8986926078796387, val_loss = 0.6664800047874451\n",
      "epoch n°353 : train_loss = 1.8857439756393433, val_loss = 0.6665830612182617\n",
      "epoch n°354 : train_loss = 1.8897769451141357, val_loss = 0.6624908447265625\n",
      "epoch n°355 : train_loss = 1.9026156663894653, val_loss = 0.6582366228103638\n",
      "epoch n°356 : train_loss = 1.8928254842758179, val_loss = 0.6616783142089844\n",
      "epoch n°357 : train_loss = 1.9003322124481201, val_loss = 0.6643155813217163\n",
      "epoch n°358 : train_loss = 1.8939536809921265, val_loss = 0.6685258746147156\n",
      "epoch n°359 : train_loss = 1.8833582401275635, val_loss = 0.6621922254562378\n",
      "epoch n°360 : train_loss = 1.895390272140503, val_loss = 0.6653469800949097\n",
      "epoch n°361 : train_loss = 1.9035561084747314, val_loss = 0.6631141901016235\n",
      "epoch n°362 : train_loss = 1.892162799835205, val_loss = 0.6614353060722351\n",
      "epoch n°363 : train_loss = 1.894130825996399, val_loss = 0.6714304089546204\n",
      "epoch n°364 : train_loss = 1.9023711681365967, val_loss = 0.67220538854599\n",
      "epoch n°365 : train_loss = 1.8932565450668335, val_loss = 0.6628891229629517\n",
      "epoch n°366 : train_loss = 1.893132209777832, val_loss = 0.6656213402748108\n",
      "epoch n°367 : train_loss = 1.8879828453063965, val_loss = 0.6579266786575317\n",
      "epoch n°368 : train_loss = 1.8942720890045166, val_loss = 0.6526980400085449\n",
      "epoch n°369 : train_loss = 1.90123450756073, val_loss = 0.6649845838546753\n",
      "epoch n°370 : train_loss = 1.8915680646896362, val_loss = 0.6568555235862732\n",
      "epoch n°371 : train_loss = 1.8906750679016113, val_loss = 0.6665522456169128\n",
      "epoch n°372 : train_loss = 1.897940993309021, val_loss = 0.6664968729019165\n",
      "epoch n°373 : train_loss = 1.8839844465255737, val_loss = 0.6565229296684265\n",
      "epoch n°374 : train_loss = 1.8924411535263062, val_loss = 0.6647432446479797\n",
      "epoch n°375 : train_loss = 1.9058794975280762, val_loss = 0.6644493937492371\n",
      "epoch n°376 : train_loss = 1.902695655822754, val_loss = 0.6684136986732483\n",
      "epoch n°377 : train_loss = 1.8795709609985352, val_loss = 0.6716142296791077\n",
      "epoch n°378 : train_loss = 1.8952161073684692, val_loss = 0.664745032787323\n",
      "epoch n°379 : train_loss = 1.884651780128479, val_loss = 0.6598881483078003\n",
      "epoch n°380 : train_loss = 1.9009987115859985, val_loss = 0.662782609462738\n",
      "epoch n°381 : train_loss = 1.889466643333435, val_loss = 0.6581735610961914\n",
      "epoch n°382 : train_loss = 1.880329966545105, val_loss = 0.6593955755233765\n",
      "epoch n°383 : train_loss = 1.8834197521209717, val_loss = 0.6619013547897339\n",
      "epoch n°384 : train_loss = 1.8926044702529907, val_loss = 0.6643479466438293\n",
      "epoch n°385 : train_loss = 1.8850284814834595, val_loss = 0.6638362407684326\n",
      "epoch n°386 : train_loss = 1.8923850059509277, val_loss = 0.666925311088562\n",
      "epoch n°387 : train_loss = 1.8836613893508911, val_loss = 0.6600480079650879\n",
      "epoch n°388 : train_loss = 1.8947118520736694, val_loss = 0.6643458008766174\n",
      "epoch n°389 : train_loss = 1.8856737613677979, val_loss = 0.6581395864486694\n",
      "epoch n°390 : train_loss = 1.8905969858169556, val_loss = 0.6630042791366577\n",
      "epoch n°391 : train_loss = 1.8941670656204224, val_loss = 0.6569132208824158\n",
      "epoch n°392 : train_loss = 1.8894122838974, val_loss = 0.6630674600601196\n",
      "epoch n°393 : train_loss = 1.8958169221878052, val_loss = 0.6592236757278442\n",
      "epoch n°394 : train_loss = 1.894761085510254, val_loss = 0.6581612229347229\n",
      "epoch n°395 : train_loss = 1.8784383535385132, val_loss = 0.6571124792098999\n",
      "epoch n°396 : train_loss = 1.878829002380371, val_loss = 0.6570892930030823\n",
      "epoch n°397 : train_loss = 1.8897374868392944, val_loss = 0.6551808714866638\n",
      "epoch n°398 : train_loss = 1.8833585977554321, val_loss = 0.6601602435112\n",
      "epoch n°399 : train_loss = 1.886978268623352, val_loss = 0.6646202802658081\n",
      "epoch n°400 : train_loss = 1.8785998821258545, val_loss = 0.6623006463050842\n",
      "epoch n°401 : train_loss = 1.883128046989441, val_loss = 0.6580618619918823\n",
      "epoch n°402 : train_loss = 1.8920848369598389, val_loss = 0.6600511074066162\n",
      "epoch n°403 : train_loss = 1.9002469778060913, val_loss = 0.6627620458602905\n",
      "epoch n°404 : train_loss = 1.8843731880187988, val_loss = 0.6672282814979553\n",
      "epoch n°405 : train_loss = 1.8764071464538574, val_loss = 0.6647182703018188\n",
      "epoch n°406 : train_loss = 1.8739478588104248, val_loss = 0.665841817855835\n",
      "epoch n°407 : train_loss = 1.8835195302963257, val_loss = 0.6618003845214844\n",
      "epoch n°408 : train_loss = 1.8879295587539673, val_loss = 0.6627355217933655\n",
      "epoch n°409 : train_loss = 1.875333309173584, val_loss = 0.6601881384849548\n",
      "epoch n°410 : train_loss = 1.8867195844650269, val_loss = 0.6615365147590637\n",
      "epoch n°411 : train_loss = 1.8884083032608032, val_loss = 0.6639758348464966\n",
      "epoch n°412 : train_loss = 1.8756877183914185, val_loss = 0.6582530736923218\n",
      "epoch n°413 : train_loss = 1.8941960334777832, val_loss = 0.6595723628997803\n",
      "epoch n°414 : train_loss = 1.8875188827514648, val_loss = 0.6628866791725159\n",
      "epoch n°415 : train_loss = 1.8750184774398804, val_loss = 0.6564494371414185\n",
      "epoch n°416 : train_loss = 1.8885568380355835, val_loss = 0.6577509045600891\n",
      "epoch n°417 : train_loss = 1.8859918117523193, val_loss = 0.6577574610710144\n",
      "epoch n°418 : train_loss = 1.888881802558899, val_loss = 0.6567732095718384\n",
      "epoch n°419 : train_loss = 1.8882083892822266, val_loss = 0.6652425527572632\n",
      "epoch n°420 : train_loss = 1.8836917877197266, val_loss = 0.6648695468902588\n",
      "epoch n°421 : train_loss = 1.8819646835327148, val_loss = 0.664328396320343\n",
      "epoch n°422 : train_loss = 1.8842267990112305, val_loss = 0.6581095457077026\n",
      "epoch n°423 : train_loss = 1.8786048889160156, val_loss = 0.6631734371185303\n",
      "epoch n°424 : train_loss = 1.8775012493133545, val_loss = 0.6630815267562866\n",
      "epoch n°425 : train_loss = 1.888934850692749, val_loss = 0.6507447361946106\n",
      "epoch n°426 : train_loss = 1.8850998878479004, val_loss = 0.6637567281723022\n",
      "epoch n°427 : train_loss = 1.8873302936553955, val_loss = 0.6590778231620789\n",
      "epoch n°428 : train_loss = 1.8772194385528564, val_loss = 0.6532149314880371\n",
      "epoch n°429 : train_loss = 1.8818825483322144, val_loss = 0.6609138250350952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°430 : train_loss = 1.880090594291687, val_loss = 0.6594673991203308\n",
      "epoch n°431 : train_loss = 1.885167121887207, val_loss = 0.6624901294708252\n",
      "epoch n°432 : train_loss = 1.8756722211837769, val_loss = 0.662251353263855\n",
      "epoch n°433 : train_loss = 1.8881113529205322, val_loss = 0.6570694446563721\n",
      "epoch n°434 : train_loss = 1.878479242324829, val_loss = 0.6581265926361084\n",
      "epoch n°435 : train_loss = 1.8745136260986328, val_loss = 0.6690044403076172\n",
      "epoch n°436 : train_loss = 1.8838400840759277, val_loss = 0.6600982546806335\n",
      "epoch n°437 : train_loss = 1.8758097887039185, val_loss = 0.6663027405738831\n",
      "epoch n°438 : train_loss = 1.8789842128753662, val_loss = 0.6529269814491272\n",
      "epoch n°439 : train_loss = 1.8573768138885498, val_loss = 0.6632705330848694\n",
      "epoch n°440 : train_loss = 1.8791449069976807, val_loss = 0.6579681038856506\n",
      "epoch n°441 : train_loss = 1.876596450805664, val_loss = 0.6571252346038818\n",
      "epoch n°442 : train_loss = 1.8667120933532715, val_loss = 0.6644464135169983\n",
      "epoch n°443 : train_loss = 1.8769346475601196, val_loss = 0.6582447290420532\n",
      "epoch n°444 : train_loss = 1.8855260610580444, val_loss = 0.6546695232391357\n",
      "epoch n°445 : train_loss = 1.8810569047927856, val_loss = 0.6583988070487976\n",
      "epoch n°446 : train_loss = 1.8617876768112183, val_loss = 0.6645639538764954\n",
      "epoch n°447 : train_loss = 1.8764493465423584, val_loss = 0.6604184508323669\n",
      "epoch n°448 : train_loss = 1.8759510517120361, val_loss = 0.6593283414840698\n",
      "epoch n°449 : train_loss = 1.864831805229187, val_loss = 0.6616513133049011\n",
      "epoch n°450 : train_loss = 1.8870588541030884, val_loss = 0.6581040620803833\n",
      "epoch n°451 : train_loss = 1.8682280778884888, val_loss = 0.6644352078437805\n",
      "epoch n°452 : train_loss = 1.872793197631836, val_loss = 0.6648069024085999\n",
      "epoch n°453 : train_loss = 1.8817416429519653, val_loss = 0.6567907333374023\n",
      "epoch n°454 : train_loss = 1.875868797302246, val_loss = 0.664877712726593\n",
      "epoch n°455 : train_loss = 1.8796709775924683, val_loss = 0.6549922227859497\n",
      "epoch n°456 : train_loss = 1.8717076778411865, val_loss = 0.6609714031219482\n",
      "epoch n°457 : train_loss = 1.873840093612671, val_loss = 0.65496426820755\n",
      "epoch n°458 : train_loss = 1.8725789785385132, val_loss = 0.6649522185325623\n",
      "epoch n°459 : train_loss = 1.8818788528442383, val_loss = 0.6584802269935608\n",
      "epoch n°460 : train_loss = 1.8772807121276855, val_loss = 0.6609985828399658\n",
      "epoch n°461 : train_loss = 1.8767127990722656, val_loss = 0.6543605923652649\n",
      "epoch n°462 : train_loss = 1.888321042060852, val_loss = 0.6609325408935547\n",
      "epoch n°463 : train_loss = 1.8752954006195068, val_loss = 0.6568624973297119\n",
      "epoch n°464 : train_loss = 1.8713370561599731, val_loss = 0.6526755690574646\n",
      "epoch n°465 : train_loss = 1.8736464977264404, val_loss = 0.6632756590843201\n",
      "epoch n°466 : train_loss = 1.8751767873764038, val_loss = 0.6619500517845154\n",
      "epoch n°467 : train_loss = 1.8834048509597778, val_loss = 0.6689891219139099\n",
      "epoch n°468 : train_loss = 1.8745042085647583, val_loss = 0.6614875793457031\n",
      "epoch n°469 : train_loss = 1.8738341331481934, val_loss = 0.6649258136749268\n",
      "epoch n°470 : train_loss = 1.8737741708755493, val_loss = 0.6619935631752014\n",
      "epoch n°471 : train_loss = 1.8839709758758545, val_loss = 0.6568853259086609\n",
      "epoch n°472 : train_loss = 1.8646769523620605, val_loss = 0.6608206033706665\n",
      "epoch n°473 : train_loss = 1.87527334690094, val_loss = 0.6579304337501526\n",
      "epoch n°474 : train_loss = 1.865269422531128, val_loss = 0.6532010436058044\n",
      "epoch n°475 : train_loss = 1.8759865760803223, val_loss = 0.6592194437980652\n",
      "epoch n°476 : train_loss = 1.8795310258865356, val_loss = 0.6626266837120056\n",
      "epoch n°477 : train_loss = 1.864772915840149, val_loss = 0.6580067873001099\n",
      "epoch n°478 : train_loss = 1.869493842124939, val_loss = 0.659468948841095\n",
      "epoch n°479 : train_loss = 1.8851118087768555, val_loss = 0.6609436273574829\n",
      "epoch n°480 : train_loss = 1.8721227645874023, val_loss = 0.6596852540969849\n",
      "epoch n°481 : train_loss = 1.8690062761306763, val_loss = 0.6559606194496155\n",
      "epoch n°482 : train_loss = 1.8786842823028564, val_loss = 0.6554980874061584\n",
      "epoch n°483 : train_loss = 1.8790260553359985, val_loss = 0.6630334258079529\n",
      "epoch n°484 : train_loss = 1.8775670528411865, val_loss = 0.6629796028137207\n",
      "epoch n°485 : train_loss = 1.8787487745285034, val_loss = 0.6611611843109131\n",
      "epoch n°486 : train_loss = 1.8869853019714355, val_loss = 0.6632869243621826\n",
      "epoch n°487 : train_loss = 1.8709746599197388, val_loss = 0.6612759828567505\n",
      "epoch n°488 : train_loss = 1.8816890716552734, val_loss = 0.654956042766571\n",
      "epoch n°489 : train_loss = 1.8785731792449951, val_loss = 0.6728711128234863\n",
      "epoch n°490 : train_loss = 1.8699895143508911, val_loss = 0.6606094837188721\n",
      "epoch n°491 : train_loss = 1.8744841814041138, val_loss = 0.6627600789070129\n",
      "epoch n°492 : train_loss = 1.8798458576202393, val_loss = 0.6568873524665833\n",
      "epoch n°493 : train_loss = 1.8828989267349243, val_loss = 0.6612667441368103\n",
      "epoch n°494 : train_loss = 1.8746601343154907, val_loss = 0.6635842323303223\n",
      "epoch n°495 : train_loss = 1.87737238407135, val_loss = 0.6577290296554565\n",
      "epoch n°496 : train_loss = 1.89278244972229, val_loss = 0.6690170764923096\n",
      "epoch n°497 : train_loss = 1.9002004861831665, val_loss = 0.6661005616188049\n",
      "epoch n°498 : train_loss = 1.8986722230911255, val_loss = 0.6666067242622375\n",
      "epoch n°499 : train_loss = 1.899342656135559, val_loss = 0.6628393530845642\n",
      "epoch n°500 : train_loss = 1.8967303037643433, val_loss = 0.6607264280319214\n",
      "epoch n°501 : train_loss = 1.9053899049758911, val_loss = 0.6671857833862305\n",
      "epoch n°502 : train_loss = 1.9016247987747192, val_loss = 0.6643726229667664\n",
      "epoch n°503 : train_loss = 1.8877006769180298, val_loss = 0.6681989431381226\n",
      "epoch n°504 : train_loss = 1.895691156387329, val_loss = 0.6663405299186707\n",
      "epoch n°505 : train_loss = 1.8941056728363037, val_loss = 0.666181206703186\n",
      "epoch n°506 : train_loss = 1.897024393081665, val_loss = 0.6691867709159851\n",
      "epoch n°507 : train_loss = 1.895140290260315, val_loss = 0.6670508980751038\n",
      "epoch n°508 : train_loss = 1.9096858501434326, val_loss = 0.6619072556495667\n",
      "epoch n°509 : train_loss = 1.8942787647247314, val_loss = 0.6661821007728577\n",
      "epoch n°510 : train_loss = 1.8872475624084473, val_loss = 0.6697167158126831\n",
      "epoch n°511 : train_loss = 1.8968816995620728, val_loss = 0.67093825340271\n",
      "epoch n°512 : train_loss = 1.9129596948623657, val_loss = 0.667687177658081\n",
      "epoch n°513 : train_loss = 1.8964067697525024, val_loss = 0.6642742156982422\n",
      "epoch n°514 : train_loss = 1.89299476146698, val_loss = 0.6689339876174927\n",
      "epoch n°515 : train_loss = 1.8958653211593628, val_loss = 0.665697455406189\n",
      "epoch n°516 : train_loss = 1.8897703886032104, val_loss = 0.6656559705734253\n",
      "epoch n°517 : train_loss = 1.8986095190048218, val_loss = 0.6654478311538696\n",
      "epoch n°518 : train_loss = 1.890046238899231, val_loss = 0.6610941886901855\n",
      "epoch n°519 : train_loss = 1.8921995162963867, val_loss = 0.6626987457275391\n",
      "epoch n°520 : train_loss = 1.909312129020691, val_loss = 0.669104278087616\n",
      "epoch n°521 : train_loss = 1.9048830270767212, val_loss = 0.6666976809501648\n",
      "epoch n°522 : train_loss = 1.8895546197891235, val_loss = 0.6634843945503235\n",
      "epoch n°523 : train_loss = 1.885381817817688, val_loss = 0.6676741242408752\n",
      "epoch n°524 : train_loss = 1.9022951126098633, val_loss = 0.6698299646377563\n",
      "epoch n°525 : train_loss = 1.906079888343811, val_loss = 0.6596547365188599\n",
      "epoch n°526 : train_loss = 1.8982417583465576, val_loss = 0.6605064868927002\n",
      "epoch n°527 : train_loss = 1.8933550119400024, val_loss = 0.670204758644104\n",
      "epoch n°528 : train_loss = 1.8932631015777588, val_loss = 0.6644904613494873\n",
      "epoch n°529 : train_loss = 1.895221471786499, val_loss = 0.668361485004425\n",
      "epoch n°530 : train_loss = 1.9105994701385498, val_loss = 0.6639642715454102\n",
      "epoch n°531 : train_loss = 1.8924086093902588, val_loss = 0.664374053478241\n",
      "epoch n°532 : train_loss = 1.8874733448028564, val_loss = 0.6602598428726196\n",
      "epoch n°533 : train_loss = 1.8873634338378906, val_loss = 0.6650345325469971\n",
      "epoch n°534 : train_loss = 1.8873003721237183, val_loss = 0.6555885672569275\n",
      "epoch n°535 : train_loss = 1.8842443227767944, val_loss = 0.6653339266777039\n",
      "epoch n°536 : train_loss = 1.9056967496871948, val_loss = 0.6651486158370972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°537 : train_loss = 1.8919638395309448, val_loss = 0.6634392738342285\n",
      "epoch n°538 : train_loss = 1.8901268243789673, val_loss = 0.664462149143219\n",
      "epoch n°539 : train_loss = 1.8991936445236206, val_loss = 0.6645680665969849\n",
      "epoch n°540 : train_loss = 1.9017229080200195, val_loss = 0.662503182888031\n",
      "epoch n°541 : train_loss = 1.8817461729049683, val_loss = 0.6618682146072388\n",
      "epoch n°542 : train_loss = 1.8966917991638184, val_loss = 0.6693107485771179\n",
      "epoch n°543 : train_loss = 1.8984575271606445, val_loss = 0.6688109040260315\n",
      "epoch n°544 : train_loss = 1.8880298137664795, val_loss = 0.6690384149551392\n",
      "epoch n°545 : train_loss = 1.894222378730774, val_loss = 0.6663424372673035\n",
      "epoch n°546 : train_loss = 1.898322343826294, val_loss = 0.6698495745658875\n",
      "epoch n°547 : train_loss = 1.885811448097229, val_loss = 0.6587182879447937\n",
      "epoch n°548 : train_loss = 1.8925820589065552, val_loss = 0.662445604801178\n",
      "epoch n°549 : train_loss = 1.8902274370193481, val_loss = 0.66599041223526\n",
      "epoch n°550 : train_loss = 1.900879144668579, val_loss = 0.6664677858352661\n",
      "epoch n°551 : train_loss = 1.9045519828796387, val_loss = 0.660971462726593\n",
      "epoch n°552 : train_loss = 1.8897016048431396, val_loss = 0.6629096269607544\n",
      "epoch n°553 : train_loss = 1.8853205442428589, val_loss = 0.6626905798912048\n",
      "epoch n°554 : train_loss = 1.8952151536941528, val_loss = 0.6673060059547424\n",
      "epoch n°555 : train_loss = 1.890727162361145, val_loss = 0.6614238619804382\n",
      "epoch n°556 : train_loss = 1.8982292413711548, val_loss = 0.6650744080543518\n",
      "epoch n°557 : train_loss = 1.895081877708435, val_loss = 0.6636114120483398\n",
      "epoch n°558 : train_loss = 1.8953479528427124, val_loss = 0.6615309119224548\n",
      "epoch n°559 : train_loss = 1.8984276056289673, val_loss = 0.6613147258758545\n",
      "epoch n°560 : train_loss = 1.8823541402816772, val_loss = 0.6632722616195679\n",
      "epoch n°561 : train_loss = 1.8919817209243774, val_loss = 0.6636673808097839\n",
      "epoch n°562 : train_loss = 1.8907310962677002, val_loss = 0.6631472110748291\n",
      "epoch n°563 : train_loss = 1.8894281387329102, val_loss = 0.6660891771316528\n",
      "epoch n°564 : train_loss = 1.8932807445526123, val_loss = 0.6599262356758118\n",
      "epoch n°565 : train_loss = 1.894404411315918, val_loss = 0.6590741276741028\n",
      "epoch n°566 : train_loss = 1.8802419900894165, val_loss = 0.6653547286987305\n",
      "epoch n°567 : train_loss = 1.888150691986084, val_loss = 0.6656361818313599\n",
      "epoch n°568 : train_loss = 1.885070562362671, val_loss = 0.6605111360549927\n",
      "epoch n°569 : train_loss = 1.8913040161132812, val_loss = 0.6612746715545654\n",
      "epoch n°570 : train_loss = 1.8888393640518188, val_loss = 0.6646090745925903\n",
      "epoch n°571 : train_loss = 1.8919700384140015, val_loss = 0.6635620594024658\n",
      "epoch n°572 : train_loss = 1.8886969089508057, val_loss = 0.6647748947143555\n",
      "epoch n°573 : train_loss = 1.874140977859497, val_loss = 0.67148357629776\n",
      "epoch n°574 : train_loss = 1.8988196849822998, val_loss = 0.6630168557167053\n",
      "epoch n°575 : train_loss = 1.8819302320480347, val_loss = 0.6648339033126831\n",
      "epoch n°576 : train_loss = 1.8856279850006104, val_loss = 0.6635163426399231\n",
      "epoch n°577 : train_loss = 1.8851958513259888, val_loss = 0.6642582416534424\n",
      "epoch n°578 : train_loss = 1.8752143383026123, val_loss = 0.6638279557228088\n",
      "epoch n°579 : train_loss = 1.8865760564804077, val_loss = 0.6579035520553589\n",
      "epoch n°580 : train_loss = 1.8871370553970337, val_loss = 0.6557793617248535\n",
      "epoch n°581 : train_loss = 1.888658046722412, val_loss = 0.6631591320037842\n",
      "epoch n°582 : train_loss = 1.8792592287063599, val_loss = 0.6663181185722351\n",
      "epoch n°583 : train_loss = 1.8873498439788818, val_loss = 0.6556304097175598\n",
      "epoch n°584 : train_loss = 1.8919854164123535, val_loss = 0.6592776775360107\n",
      "epoch n°585 : train_loss = 1.8893494606018066, val_loss = 0.6659060716629028\n",
      "epoch n°586 : train_loss = 1.890660047531128, val_loss = 0.6585776805877686\n",
      "epoch n°587 : train_loss = 1.8983440399169922, val_loss = 0.6666449308395386\n",
      "epoch n°588 : train_loss = 1.8883849382400513, val_loss = 0.6621354222297668\n",
      "epoch n°589 : train_loss = 1.8889482021331787, val_loss = 0.6607085466384888\n",
      "epoch n°590 : train_loss = 1.8822470903396606, val_loss = 0.664070725440979\n",
      "epoch n°591 : train_loss = 1.8715914487838745, val_loss = 0.6676225662231445\n",
      "epoch n°592 : train_loss = 1.8857152462005615, val_loss = 0.663902759552002\n",
      "epoch n°593 : train_loss = 1.8866145610809326, val_loss = 0.6663223505020142\n",
      "epoch n°594 : train_loss = 1.8824371099472046, val_loss = 0.6542109251022339\n",
      "epoch n°595 : train_loss = 1.8825619220733643, val_loss = 0.661597728729248\n",
      "epoch n°596 : train_loss = 1.8832523822784424, val_loss = 0.6595594882965088\n",
      "epoch n°597 : train_loss = 1.8794792890548706, val_loss = 0.6628074049949646\n",
      "epoch n°598 : train_loss = 1.884078025817871, val_loss = 0.6639745235443115\n",
      "epoch n°599 : train_loss = 1.8880892992019653, val_loss = 0.6637107729911804\n",
      "epoch n°600 : train_loss = 1.8837159872055054, val_loss = 0.6665624380111694\n",
      "epoch n°601 : train_loss = 1.8894599676132202, val_loss = 0.6646652817726135\n",
      "epoch n°602 : train_loss = 1.8734866380691528, val_loss = 0.6589999198913574\n",
      "epoch n°603 : train_loss = 1.8938097953796387, val_loss = 0.6598260998725891\n",
      "epoch n°604 : train_loss = 1.8893855810165405, val_loss = 0.6649441123008728\n",
      "epoch n°605 : train_loss = 1.8831496238708496, val_loss = 0.6676435470581055\n",
      "epoch n°606 : train_loss = 1.8694514036178589, val_loss = 0.6615740656852722\n",
      "epoch n°607 : train_loss = 1.8935422897338867, val_loss = 0.6672611236572266\n",
      "epoch n°608 : train_loss = 1.8792730569839478, val_loss = 0.661884069442749\n",
      "epoch n°609 : train_loss = 1.8764731884002686, val_loss = 0.666469395160675\n",
      "epoch n°610 : train_loss = 1.8785395622253418, val_loss = 0.6643600463867188\n",
      "epoch n°611 : train_loss = 1.8913193941116333, val_loss = 0.666285514831543\n",
      "epoch n°612 : train_loss = 1.8904893398284912, val_loss = 0.6604166030883789\n",
      "epoch n°613 : train_loss = 1.8929678201675415, val_loss = 0.6661087274551392\n",
      "epoch n°614 : train_loss = 1.8837617635726929, val_loss = 0.66786128282547\n",
      "epoch n°615 : train_loss = 1.8820139169692993, val_loss = 0.6615968942642212\n",
      "epoch n°616 : train_loss = 1.8931598663330078, val_loss = 0.6617462635040283\n",
      "epoch n°617 : train_loss = 1.8731733560562134, val_loss = 0.6574552655220032\n",
      "epoch n°618 : train_loss = 1.8852077722549438, val_loss = 0.6573154926300049\n",
      "epoch n°619 : train_loss = 1.889210820198059, val_loss = 0.6618182063102722\n",
      "epoch n°620 : train_loss = 1.8744077682495117, val_loss = 0.6549053192138672\n",
      "epoch n°621 : train_loss = 1.8853620290756226, val_loss = 0.6599446535110474\n",
      "epoch n°622 : train_loss = 1.876932144165039, val_loss = 0.6599928140640259\n",
      "epoch n°623 : train_loss = 1.868584394454956, val_loss = 0.6610795855522156\n",
      "epoch n°624 : train_loss = 1.8846853971481323, val_loss = 0.6585835814476013\n",
      "epoch n°625 : train_loss = 1.884490966796875, val_loss = 0.6574034690856934\n",
      "epoch n°626 : train_loss = 1.8798136711120605, val_loss = 0.6636953353881836\n",
      "epoch n°627 : train_loss = 1.883097529411316, val_loss = 0.6637895107269287\n",
      "epoch n°628 : train_loss = 1.8744899034500122, val_loss = 0.6596720218658447\n",
      "epoch n°629 : train_loss = 1.8890831470489502, val_loss = 0.6577967405319214\n",
      "epoch n°630 : train_loss = 1.8927634954452515, val_loss = 0.6592919826507568\n",
      "epoch n°631 : train_loss = 1.875010371208191, val_loss = 0.6660801768302917\n",
      "epoch n°632 : train_loss = 1.8741416931152344, val_loss = 0.663419246673584\n",
      "epoch n°633 : train_loss = 1.8851107358932495, val_loss = 0.6633108854293823\n",
      "epoch n°634 : train_loss = 1.8856494426727295, val_loss = 0.6601147055625916\n",
      "epoch n°635 : train_loss = 1.881056785583496, val_loss = 0.6658620834350586\n",
      "epoch n°636 : train_loss = 1.882723331451416, val_loss = 0.6620215773582458\n",
      "epoch n°637 : train_loss = 1.8897243738174438, val_loss = 0.6591364741325378\n",
      "epoch n°638 : train_loss = 1.8758891820907593, val_loss = 0.660959005355835\n",
      "epoch n°639 : train_loss = 1.8751513957977295, val_loss = 0.6595484614372253\n",
      "epoch n°640 : train_loss = 1.8741235733032227, val_loss = 0.6635422110557556\n",
      "epoch n°641 : train_loss = 1.8823739290237427, val_loss = 0.6665967702865601\n",
      "epoch n°642 : train_loss = 1.8803341388702393, val_loss = 0.6604087352752686\n",
      "epoch n°643 : train_loss = 1.8860437870025635, val_loss = 0.6641683578491211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°644 : train_loss = 1.8822911977767944, val_loss = 0.6630779504776001\n",
      "epoch n°645 : train_loss = 1.889127254486084, val_loss = 0.6646910905838013\n",
      "epoch n°646 : train_loss = 1.8824368715286255, val_loss = 0.6563927531242371\n",
      "epoch n°647 : train_loss = 1.8719773292541504, val_loss = 0.6668244004249573\n",
      "epoch n°648 : train_loss = 1.8833107948303223, val_loss = 0.6637996435165405\n",
      "epoch n°649 : train_loss = 1.8846889734268188, val_loss = 0.6661915183067322\n",
      "epoch n°650 : train_loss = 1.8777706623077393, val_loss = 0.6574773788452148\n",
      "epoch n°651 : train_loss = 1.8738172054290771, val_loss = 0.658268392086029\n",
      "epoch n°652 : train_loss = 1.8876622915267944, val_loss = 0.6599984765052795\n",
      "epoch n°653 : train_loss = 1.8851488828659058, val_loss = 0.661758542060852\n",
      "epoch n°654 : train_loss = 1.885277509689331, val_loss = 0.6591383814811707\n",
      "epoch n°655 : train_loss = 1.8661068677902222, val_loss = 0.6618812084197998\n",
      "epoch n°656 : train_loss = 1.8784706592559814, val_loss = 0.6612038016319275\n",
      "epoch n°657 : train_loss = 1.8774067163467407, val_loss = 0.6605296730995178\n",
      "epoch n°658 : train_loss = 1.8746258020401, val_loss = 0.6618602275848389\n",
      "epoch n°659 : train_loss = 1.8738901615142822, val_loss = 0.659936249256134\n",
      "epoch n°660 : train_loss = 1.873480200767517, val_loss = 0.6607444882392883\n",
      "epoch n°661 : train_loss = 1.8816043138504028, val_loss = 0.6675791144371033\n",
      "epoch n°662 : train_loss = 1.8635696172714233, val_loss = 0.6592587828636169\n",
      "epoch n°663 : train_loss = 1.872188687324524, val_loss = 0.6628823280334473\n",
      "epoch n°664 : train_loss = 1.869532585144043, val_loss = 0.6605551838874817\n",
      "epoch n°665 : train_loss = 1.8742445707321167, val_loss = 0.6638489365577698\n",
      "epoch n°666 : train_loss = 1.8743336200714111, val_loss = 0.6585919857025146\n",
      "epoch n°667 : train_loss = 1.8817756175994873, val_loss = 0.666947066783905\n",
      "epoch n°668 : train_loss = 1.8758981227874756, val_loss = 0.6558483242988586\n",
      "epoch n°669 : train_loss = 1.8726377487182617, val_loss = 0.6611211895942688\n",
      "epoch n°670 : train_loss = 1.8758739233016968, val_loss = 0.6622377634048462\n",
      "epoch n°671 : train_loss = 1.879709005355835, val_loss = 0.6623378992080688\n",
      "epoch n°672 : train_loss = 1.8780957460403442, val_loss = 0.6648987531661987\n",
      "epoch n°673 : train_loss = 1.8719027042388916, val_loss = 0.6612513661384583\n",
      "epoch n°674 : train_loss = 1.8806662559509277, val_loss = 0.6581892967224121\n",
      "epoch n°675 : train_loss = 1.8836861848831177, val_loss = 0.6530373692512512\n",
      "epoch n°676 : train_loss = 1.869861125946045, val_loss = 0.6641191840171814\n",
      "epoch n°677 : train_loss = 1.8758858442306519, val_loss = 0.6624535918235779\n",
      "epoch n°678 : train_loss = 1.8834185600280762, val_loss = 0.662562370300293\n",
      "epoch n°679 : train_loss = 1.8710259199142456, val_loss = 0.6643875241279602\n",
      "epoch n°680 : train_loss = 1.8763775825500488, val_loss = 0.6597142815589905\n",
      "epoch n°681 : train_loss = 1.8758615255355835, val_loss = 0.6682775020599365\n",
      "epoch n°682 : train_loss = 1.8774025440216064, val_loss = 0.6635485291481018\n",
      "epoch n°683 : train_loss = 1.8745436668395996, val_loss = 0.6610715389251709\n",
      "epoch n°684 : train_loss = 1.8803269863128662, val_loss = 0.6624017357826233\n",
      "epoch n°685 : train_loss = 1.8845294713974, val_loss = 0.6578516960144043\n",
      "epoch n°686 : train_loss = 1.8787423372268677, val_loss = 0.6671020984649658\n",
      "epoch n°687 : train_loss = 1.8723838329315186, val_loss = 0.66314297914505\n",
      "epoch n°688 : train_loss = 1.8674677610397339, val_loss = 0.6571617722511292\n",
      "epoch n°689 : train_loss = 1.8723150491714478, val_loss = 0.6630876660346985\n",
      "epoch n°690 : train_loss = 1.8777016401290894, val_loss = 0.6638481616973877\n",
      "epoch n°691 : train_loss = 1.873091697692871, val_loss = 0.6556911468505859\n",
      "epoch n°692 : train_loss = 1.8660163879394531, val_loss = 0.6659886837005615\n",
      "epoch n°693 : train_loss = 1.8670523166656494, val_loss = 0.663381814956665\n",
      "epoch n°694 : train_loss = 1.8850899934768677, val_loss = 0.6562308073043823\n",
      "epoch n°695 : train_loss = 1.8701118230819702, val_loss = 0.6625090837478638\n",
      "epoch n°696 : train_loss = 1.870833396911621, val_loss = 0.6641960740089417\n",
      "epoch n°697 : train_loss = 1.869635820388794, val_loss = 0.6598637700080872\n",
      "epoch n°698 : train_loss = 1.8786053657531738, val_loss = 0.6618008613586426\n",
      "epoch n°699 : train_loss = 1.863109827041626, val_loss = 0.6574575304985046\n",
      "epoch n°700 : train_loss = 1.8716946840286255, val_loss = 0.6567621827125549\n",
      "epoch n°701 : train_loss = 1.8830716609954834, val_loss = 0.6652705669403076\n",
      "epoch n°702 : train_loss = 1.865291714668274, val_loss = 0.6597130298614502\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/state_6L_RowAtt_no_Cross_ATT.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 #moving average\n",
    "smothed_Eval = np.convolve(Eval, np.ones(N)/N, mode='valid')\n",
    "plt.plot(np.arange(len(smothed_Eval)),smothed_Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/state_6L_RowAtt_no_Cross_ATT.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/state_6L_RowAtt_no_Cross_ATT.pth\" \n",
    "SCORES = eval_model_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/state_6L_RowAtt_no_Cross_ATT.pth\" \n",
    "SCORES = eval_model_acc_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958d518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
