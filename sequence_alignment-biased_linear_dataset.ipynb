{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') #to avoid issues in the dataloading\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1378df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20891a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 4\n",
    "rep = torch.tensor([4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0, 0, 0, 0, 0, 0, 0,0, 0])\n",
    "#max 8 \n",
    "rep = torch.tensor([8, 8, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 5, 7, 8, 8, 8, 8, 8])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0.2, 0.5, 0.3, 0.9, 0.8, 0.5, 0.9, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=1400000,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by precomputing a bunch of data on the sequence families\n",
    "        \"\"\"\n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto',rep=rep,rand=rand): \n",
    "        \"\"\"\n",
    "        input idx of the family of the sample\n",
    "        return a Tensor containing several samples from the family corresponding to the index\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        bin_n = len(rep) #for biasing the sampling\n",
    "        \n",
    "        precomputed_pos = [] #positions of the amino-acids\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        #sampling more for big families and long sequences\n",
    "        if type(sample_size) != int:\n",
    "            sample_s = min(n * l,25_000)\n",
    "            coef = round((sample_s)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1): #computing PID and removing aligned '-' ##might need to compute the actual column num\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_i != '-' and aa_j != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0 #case 0/0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = [] #possible position to take\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "            \n",
    "            # biasing for more diverse PID  \n",
    "            bin_idx = int(PID_ij//(1/bin_n))\n",
    "            rep_number = rep[bin_idx].clone()\n",
    "            if torch.rand(1) < rand[bin_idx]:\n",
    "                rep_number+=1\n",
    "            \n",
    "            for _ in range(rep_number):\n",
    "                try:   \n",
    "                    k = np.random.choice(possible_k)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #adding to the output\n",
    "                lengths.append(align_l)\n",
    "                pos_ij = (k + precomputed_pos)\n",
    "                pos.append(pos_ij)\n",
    "                \n",
    "                #computing the windows\n",
    "                window_i = ''\n",
    "                window_j = ''\n",
    "                for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                    if w < 0 or w >= align_l: #case of the edges\n",
    "                        window_i += 'Z'\n",
    "                        window_j += 'Z'\n",
    "                    else:\n",
    "                        window_i += seq_i[w]\n",
    "                        window_j += seq_j[w]\n",
    "\n",
    "                y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "                X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "\n",
    "                X.append(X_i)\n",
    "                y.append(y_j)\n",
    "                PIDs.append(PID_ij)\n",
    "                #computing the local PID\n",
    "                local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "\n",
    "                loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "                try:\n",
    "                    tmp = local_PID_ij/loc_comp  \n",
    "                except:\n",
    "                    tmp = 0 #case 0/0\n",
    "\n",
    "                local_PIDs.append(tmp)\n",
    "                pfreqs.append(self.aa_freqs[idx])\n",
    "                p_i_freqs = self.p_aa_freqs[idx][i]\n",
    "                p_j_freqs = self.p_aa_freqs[idx][j]\n",
    "\n",
    "                local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "\n",
    "                assert y_j < 20\n",
    "                assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache() #clearing the cache\n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        out = X,y.long(),PIDs,local_PIDs,pfreqs,local_pfreqs,pos,lengths\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run This for new data\n",
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "#To load datasets with all features computed\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_dataset.cont_size = CONT_SIZE\n",
    "test_dataset.cont_size = CONT_SIZE\n",
    "val_dataset.cont_size = CONT_SIZE\n",
    "\n",
    "train_dataset.div = 2000\n",
    "test_dataset.div = 2000\n",
    "val_dataset.div = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Transforms a list of tensors to a batch tensor\n",
    "    \"\"\"\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[4] for item in batch],dim=0)\n",
    "    lpfreqs = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[6] for item in batch],dim=0)\n",
    "    length = torch.cat([item[7] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID,pfreqs,lpfreqs, pos, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(from the timm library)\n",
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb41b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier Head of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.reshape((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-like neural net\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,seq_len=4*CONT_SIZE+6,clf_dims=[256,64],cont_size=CONT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        blocks = []\n",
    "        for n_h, h_d,w,d in zip(num_heads,head_dims,wide_factors,drops):\n",
    "            blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.clf = Classifier_Head(in_features,clf_dims,out_size=out_size,seq_len=seq_len)\n",
    "        \n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "    \n",
    "    def to_input(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1)\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "        \n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        X = torch.cat([seq1,seq2],dim=1)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        lpf = F.pad(lpfreqs,pad=pad, mode='constant', value=0)\n",
    "\n",
    "        X_input = torch.cat([X,pf,lpf,pid],dim=1)\n",
    "        \n",
    "        return X_input\n",
    "    \n",
    "    def forward(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_input = self.to_input(x,PID,pfreqs,lpfreqs,pos,length)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        out = self.clf(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    \"\"\"\n",
    "    Returns the initialization parameters of the Transformer\n",
    "    \"\"\"\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b97a85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AttNet                                        [1, 20]                   400\n",
       "├─Sequential: 1-1                             [1, 32]                   --\n",
       "│    └─Linear: 2-1                            [1, 32]                   64\n",
       "│    └─Sigmoid: 2-2                           [1, 32]                   --\n",
       "├─Sequential: 1-2                             [1, 54, 32]               --\n",
       "│    └─Block: 2-3                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-1                     [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-2                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-3                    [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-4                   [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-5                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-6                    [1, 54, 32]               64\n",
       "│    └─Block: 2-4                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-7                     [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-8                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-9                    [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-10                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-11                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-12                   [1, 54, 32]               64\n",
       "│    └─Block: 2-5                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-13                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-14                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-15                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-16                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-17                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-18                   [1, 54, 32]               64\n",
       "│    └─Block: 2-6                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-19                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-20                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-21                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-22                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-23                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-24                   [1, 54, 32]               64\n",
       "│    └─Block: 2-7                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-25                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-26                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-27                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-28                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-29                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-30                   [1, 54, 32]               64\n",
       "│    └─Block: 2-8                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-31                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-32                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-33                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-34                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-35                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-36                   [1, 54, 32]               64\n",
       "├─Classifier_Head: 1-3                        [1, 20]                   --\n",
       "│    └─Sequential: 2-9                        [1, 20]                   --\n",
       "│    │    └─Linear: 3-37                      [1, 1024]                 1,770,496\n",
       "│    │    └─GELU: 3-38                        [1, 1024]                 --\n",
       "│    │    └─Dropout: 3-39                     [1, 1024]                 --\n",
       "│    │    └─Linear: 3-40                      [1, 256]                  262,400\n",
       "│    │    └─GELU: 3-41                        [1, 256]                  --\n",
       "│    │    └─Dropout: 3-42                     [1, 256]                  --\n",
       "│    │    └─Linear: 3-43                      [1, 64]                   16,448\n",
       "│    │    └─GELU: 3-44                        [1, 64]                   --\n",
       "│    │    └─Dropout: 3-45                     [1, 64]                   --\n",
       "│    │    └─Linear: 3-46                      [1, 20]                   1,300\n",
       "===============================================================================================\n",
       "Total params: 3,877,796\n",
       "Trainable params: 3,877,796\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.30\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 2.67\n",
       "Params size (MB): 9.19\n",
       "Estimated Total Size (MB): 11.86\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model,[(1,49,21),(1,),(1,20),(1,2,20),(1,49),(1,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Used for checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos, length in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                lPID = lPID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0982994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4d60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_acc_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "                \n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True,cont_size=CONT_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    \"\"\"\n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    LossMut = nn.BCELoss(reduction='sum')\n",
    "    EvalLoss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            lpfreqs = lpfreqs.to(device)\n",
    "            length = length.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "            \n",
    "\n",
    "            if use_mut:\n",
    "                X_idx = torch.argmax(X[:,cont_size],dim=1)\n",
    "                y_true = (X_idx == y).float().unsqueeze(1)  #0 if a mutation happens else 1 \n",
    "                y_pred = F.softmax(y_hat,dim=1)\n",
    "                y_pred = y_pred.gather(1,X_idx.view(-1,1)) #the Xth component of y_hat should be predicting ^\n",
    "                l = (Loss(y_hat,y) + LossMut(y_pred,y_true))/440 \n",
    "            else:\n",
    "                l = Loss(y_hat,y)/440 \n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.7078588008880615, val_loss = 0.8323175311088562\n",
      "epoch n°1 : train_loss = 2.255880117416382, val_loss = 0.7608538866043091\n",
      "epoch n°2 : train_loss = 2.1833884716033936, val_loss = 0.7476219534873962\n",
      "epoch n°3 : train_loss = 2.145447254180908, val_loss = 0.7499938011169434\n",
      "epoch n°4 : train_loss = 2.1305720806121826, val_loss = 0.7456762194633484\n",
      "epoch n°5 : train_loss = 2.1098127365112305, val_loss = 0.7507891654968262\n",
      "epoch n°6 : train_loss = 2.118419885635376, val_loss = 0.7451786398887634\n",
      "epoch n°7 : train_loss = 2.0947751998901367, val_loss = 0.7464073300361633\n",
      "epoch n°8 : train_loss = 2.087336540222168, val_loss = 0.7500407099723816\n",
      "epoch n°9 : train_loss = 2.090449094772339, val_loss = 0.7475588917732239\n",
      "epoch n°10 : train_loss = 2.0792298316955566, val_loss = 0.7383466958999634\n",
      "epoch n°11 : train_loss = 2.0862302780151367, val_loss = 0.740938663482666\n",
      "epoch n°12 : train_loss = 2.084897994995117, val_loss = 0.7436497807502747\n",
      "epoch n°13 : train_loss = 2.0796310901641846, val_loss = 0.739556610584259\n",
      "epoch n°14 : train_loss = 2.062603712081909, val_loss = 0.7466696500778198\n",
      "epoch n°15 : train_loss = 2.0741374492645264, val_loss = 0.7383598685264587\n",
      "epoch n°16 : train_loss = 2.1010191440582275, val_loss = 0.7431247234344482\n",
      "epoch n°17 : train_loss = 2.0910098552703857, val_loss = 0.7423720955848694\n",
      "epoch n°18 : train_loss = 2.1060121059417725, val_loss = 0.7423264384269714\n",
      "epoch n°19 : train_loss = 2.095402717590332, val_loss = 0.7440365552902222\n",
      "epoch n°20 : train_loss = 2.0784716606140137, val_loss = 0.7452077865600586\n",
      "epoch n°21 : train_loss = 2.081087112426758, val_loss = 0.739569365978241\n",
      "epoch n°22 : train_loss = 2.0755629539489746, val_loss = 0.7424851059913635\n",
      "epoch n°23 : train_loss = 2.058950662612915, val_loss = 0.7361321449279785\n",
      "epoch n°24 : train_loss = 2.0629899501800537, val_loss = 0.7398276925086975\n",
      "epoch n°25 : train_loss = 2.0682120323181152, val_loss = 0.7480189204216003\n",
      "epoch n°26 : train_loss = 2.061415910720825, val_loss = 0.740049421787262\n",
      "epoch n°27 : train_loss = 2.0520074367523193, val_loss = 0.7407849431037903\n",
      "epoch n°28 : train_loss = 2.047792673110962, val_loss = 0.737342119216919\n",
      "epoch n°29 : train_loss = 2.0486502647399902, val_loss = 0.7389453649520874\n",
      "epoch n°30 : train_loss = 2.0428574085235596, val_loss = 0.743157148361206\n",
      "epoch n°31 : train_loss = 2.0444605350494385, val_loss = 0.7406463027000427\n",
      "epoch n°32 : train_loss = 2.058072090148926, val_loss = 0.7368501424789429\n",
      "epoch n°33 : train_loss = 2.038247585296631, val_loss = 0.7392164468765259\n",
      "epoch n°34 : train_loss = 2.039315700531006, val_loss = 0.7342831492424011\n",
      "epoch n°35 : train_loss = 2.032702684402466, val_loss = 0.7364310622215271\n",
      "epoch n°36 : train_loss = 2.032623291015625, val_loss = 0.7361570596694946\n",
      "epoch n°37 : train_loss = 2.0288586616516113, val_loss = 0.7415787577629089\n",
      "epoch n°38 : train_loss = 2.037546396255493, val_loss = 0.7346189022064209\n",
      "epoch n°39 : train_loss = 2.0177392959594727, val_loss = 0.7370137572288513\n",
      "epoch n°40 : train_loss = 2.0204906463623047, val_loss = 0.7346979975700378\n",
      "epoch n°41 : train_loss = 2.016232490539551, val_loss = 0.7381957769393921\n",
      "epoch n°42 : train_loss = 2.0215818881988525, val_loss = 0.734325647354126\n",
      "epoch n°43 : train_loss = 2.0236756801605225, val_loss = 0.730863630771637\n",
      "epoch n°44 : train_loss = 2.015105962753296, val_loss = 0.731451153755188\n",
      "epoch n°45 : train_loss = 2.0199737548828125, val_loss = 0.73201584815979\n",
      "epoch n°46 : train_loss = 2.027567148208618, val_loss = 0.7340981364250183\n",
      "epoch n°47 : train_loss = 2.0116586685180664, val_loss = 0.7315365076065063\n",
      "epoch n°48 : train_loss = 2.04533314704895, val_loss = 0.7351788282394409\n",
      "epoch n°49 : train_loss = 2.034226179122925, val_loss = 0.7346731424331665\n",
      "epoch n°50 : train_loss = 2.051867723464966, val_loss = 0.740657389163971\n",
      "epoch n°51 : train_loss = 2.0233497619628906, val_loss = 0.7379770278930664\n",
      "epoch n°52 : train_loss = 2.0348188877105713, val_loss = 0.7348526120185852\n",
      "epoch n°53 : train_loss = 2.0397698879241943, val_loss = 0.7382081151008606\n",
      "epoch n°54 : train_loss = 2.032442569732666, val_loss = 0.7427071332931519\n",
      "epoch n°55 : train_loss = 2.0329666137695312, val_loss = 0.7392077445983887\n",
      "epoch n°56 : train_loss = 2.0356616973876953, val_loss = 0.7338465452194214\n",
      "epoch n°57 : train_loss = 2.033273696899414, val_loss = 0.7345101237297058\n",
      "epoch n°58 : train_loss = 2.0311648845672607, val_loss = 0.7368559837341309\n",
      "epoch n°59 : train_loss = 2.031989097595215, val_loss = 0.7305474281311035\n",
      "epoch n°60 : train_loss = 2.0159218311309814, val_loss = 0.7331220507621765\n",
      "epoch n°61 : train_loss = 2.0164685249328613, val_loss = 0.7318980097770691\n",
      "epoch n°62 : train_loss = 2.041027307510376, val_loss = 0.7387858629226685\n",
      "epoch n°63 : train_loss = 2.015683650970459, val_loss = 0.7394298911094666\n",
      "epoch n°64 : train_loss = 2.0288314819335938, val_loss = 0.7297936677932739\n",
      "epoch n°65 : train_loss = 2.0357730388641357, val_loss = 0.7332558631896973\n",
      "epoch n°66 : train_loss = 2.024219274520874, val_loss = 0.7384133338928223\n",
      "epoch n°67 : train_loss = 2.026099681854248, val_loss = 0.7338805794715881\n",
      "epoch n°68 : train_loss = 2.0366928577423096, val_loss = 0.7348796129226685\n",
      "epoch n°69 : train_loss = 2.0099642276763916, val_loss = 0.7364528179168701\n",
      "epoch n°70 : train_loss = 2.0135016441345215, val_loss = 0.7328992486000061\n",
      "epoch n°71 : train_loss = 2.006889820098877, val_loss = 0.7341403961181641\n",
      "epoch n°72 : train_loss = 2.0201194286346436, val_loss = 0.7380486726760864\n",
      "epoch n°73 : train_loss = 2.019965648651123, val_loss = 0.7323510646820068\n",
      "epoch n°74 : train_loss = 2.01312518119812, val_loss = 0.7332122921943665\n",
      "epoch n°75 : train_loss = 2.018911123275757, val_loss = 0.7351807951927185\n",
      "epoch n°76 : train_loss = 2.006075620651245, val_loss = 0.731311559677124\n",
      "epoch n°77 : train_loss = 2.0104970932006836, val_loss = 0.7300093173980713\n",
      "epoch n°78 : train_loss = 2.0127153396606445, val_loss = 0.7351434230804443\n",
      "epoch n°79 : train_loss = 1.9933654069900513, val_loss = 0.7340551614761353\n",
      "epoch n°80 : train_loss = 2.0011661052703857, val_loss = 0.7323158979415894\n",
      "epoch n°81 : train_loss = 1.9977660179138184, val_loss = 0.7356647849082947\n",
      "epoch n°82 : train_loss = 1.9956623315811157, val_loss = 0.7318819761276245\n",
      "epoch n°83 : train_loss = 1.9957942962646484, val_loss = 0.7311955094337463\n",
      "epoch n°84 : train_loss = 2.002824306488037, val_loss = 0.7322789430618286\n",
      "epoch n°85 : train_loss = 1.9965367317199707, val_loss = 0.7339008450508118\n",
      "epoch n°86 : train_loss = 1.9904593229293823, val_loss = 0.7259196043014526\n",
      "epoch n°87 : train_loss = 1.9870202541351318, val_loss = 0.7310476303100586\n",
      "epoch n°88 : train_loss = 1.9962396621704102, val_loss = 0.7259839177131653\n",
      "epoch n°89 : train_loss = 1.9920045137405396, val_loss = 0.7298808097839355\n",
      "epoch n°90 : train_loss = 1.9889017343521118, val_loss = 0.7295523285865784\n",
      "epoch n°91 : train_loss = 1.99339759349823, val_loss = 0.7307631373405457\n",
      "epoch n°92 : train_loss = 1.98912513256073, val_loss = 0.7275831699371338\n",
      "epoch n°93 : train_loss = 1.981501579284668, val_loss = 0.734948456287384\n",
      "epoch n°94 : train_loss = 1.9931416511535645, val_loss = 0.7322996258735657\n",
      "epoch n°95 : train_loss = 1.9841777086257935, val_loss = 0.7286194562911987\n",
      "epoch n°96 : train_loss = 1.987252950668335, val_loss = 0.7302200198173523\n",
      "epoch n°97 : train_loss = 1.9957737922668457, val_loss = 0.7272306680679321\n",
      "epoch n°98 : train_loss = 1.978444218635559, val_loss = 0.730586588382721\n",
      "epoch n°99 : train_loss = 1.986222743988037, val_loss = 0.725022554397583\n",
      "epoch n°100 : train_loss = 1.9910390377044678, val_loss = 0.7282423377037048\n",
      "epoch n°101 : train_loss = 1.9839603900909424, val_loss = 0.7248582243919373\n",
      "epoch n°102 : train_loss = 1.9721449613571167, val_loss = 0.7316873669624329\n",
      "epoch n°103 : train_loss = 1.9841371774673462, val_loss = 0.733915388584137\n",
      "epoch n°104 : train_loss = 1.9810811281204224, val_loss = 0.7320826053619385\n",
      "epoch n°105 : train_loss = 1.9780648946762085, val_loss = 0.7287593483924866\n",
      "epoch n°106 : train_loss = 1.9880425930023193, val_loss = 0.7298141121864319\n",
      "epoch n°107 : train_loss = 1.9851042032241821, val_loss = 0.724096417427063\n",
      "epoch n°108 : train_loss = 1.9931635856628418, val_loss = 0.7286596298217773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 1.9840521812438965, val_loss = 0.7279499769210815\n",
      "epoch n°110 : train_loss = 1.9725931882858276, val_loss = 0.7236574292182922\n",
      "epoch n°111 : train_loss = 1.9734901189804077, val_loss = 0.726706326007843\n",
      "epoch n°112 : train_loss = 2.007477045059204, val_loss = 0.7333267331123352\n",
      "epoch n°113 : train_loss = 2.001431465148926, val_loss = 0.7292020320892334\n",
      "epoch n°114 : train_loss = 2.001767158508301, val_loss = 0.729005753993988\n",
      "epoch n°115 : train_loss = 1.9929035902023315, val_loss = 0.7324731349945068\n",
      "epoch n°116 : train_loss = 2.0025761127471924, val_loss = 0.7357401847839355\n",
      "epoch n°117 : train_loss = 2.000272035598755, val_loss = 0.7363021969795227\n",
      "epoch n°118 : train_loss = 2.0034749507904053, val_loss = 0.7313282489776611\n",
      "epoch n°119 : train_loss = 2.003446102142334, val_loss = 0.7346231937408447\n",
      "epoch n°120 : train_loss = 1.9984562397003174, val_loss = 0.7332544922828674\n",
      "epoch n°121 : train_loss = 1.9982668161392212, val_loss = 0.7358217835426331\n",
      "epoch n°122 : train_loss = 1.99986732006073, val_loss = 0.7310740947723389\n",
      "epoch n°123 : train_loss = 1.9851115942001343, val_loss = 0.7323151230812073\n",
      "epoch n°124 : train_loss = 2.001115083694458, val_loss = 0.7303400635719299\n",
      "epoch n°125 : train_loss = 1.990473985671997, val_loss = 0.7333955764770508\n",
      "epoch n°126 : train_loss = 1.9994301795959473, val_loss = 0.7350506782531738\n",
      "epoch n°127 : train_loss = 1.9920718669891357, val_loss = 0.7316909432411194\n",
      "epoch n°128 : train_loss = 2.003751039505005, val_loss = 0.730305552482605\n",
      "epoch n°129 : train_loss = 1.9931564331054688, val_loss = 0.7370175123214722\n",
      "epoch n°130 : train_loss = 1.9962053298950195, val_loss = 0.7303014397621155\n",
      "epoch n°131 : train_loss = 2.007025718688965, val_loss = 0.7350597977638245\n",
      "epoch n°132 : train_loss = 1.9959087371826172, val_loss = 0.7314902544021606\n",
      "epoch n°133 : train_loss = 2.0006308555603027, val_loss = 0.7320907711982727\n",
      "epoch n°134 : train_loss = 1.9838734865188599, val_loss = 0.731018602848053\n",
      "epoch n°135 : train_loss = 1.9868817329406738, val_loss = 0.7350133657455444\n",
      "epoch n°136 : train_loss = 1.9889225959777832, val_loss = 0.73170405626297\n",
      "epoch n°137 : train_loss = 1.999089002609253, val_loss = 0.7297422289848328\n",
      "epoch n°138 : train_loss = 1.99350106716156, val_loss = 0.735200047492981\n",
      "epoch n°139 : train_loss = 1.9896425008773804, val_loss = 0.7345513105392456\n",
      "epoch n°140 : train_loss = 1.9931902885437012, val_loss = 0.7347815036773682\n",
      "epoch n°141 : train_loss = 2.0036087036132812, val_loss = 0.7331980466842651\n",
      "epoch n°142 : train_loss = 1.997225284576416, val_loss = 0.7305378913879395\n",
      "epoch n°143 : train_loss = 1.9922945499420166, val_loss = 0.7305656671524048\n",
      "epoch n°144 : train_loss = 1.97408127784729, val_loss = 0.7356153726577759\n",
      "epoch n°145 : train_loss = 1.9895310401916504, val_loss = 0.728705883026123\n",
      "epoch n°146 : train_loss = 1.9769785404205322, val_loss = 0.7259891033172607\n",
      "epoch n°147 : train_loss = 1.9972361326217651, val_loss = 0.7295514345169067\n",
      "epoch n°148 : train_loss = 1.9781502485275269, val_loss = 0.7340786457061768\n",
      "epoch n°149 : train_loss = 1.9857410192489624, val_loss = 0.7297239303588867\n",
      "epoch n°150 : train_loss = 1.989546298980713, val_loss = 0.7344815731048584\n",
      "epoch n°151 : train_loss = 1.965880274772644, val_loss = 0.7311257719993591\n",
      "epoch n°152 : train_loss = 1.9734430313110352, val_loss = 0.7299772500991821\n",
      "epoch n°153 : train_loss = 1.977689266204834, val_loss = 0.7305024266242981\n",
      "epoch n°154 : train_loss = 1.9878566265106201, val_loss = 0.7314921617507935\n",
      "epoch n°155 : train_loss = 1.9771722555160522, val_loss = 0.7269608378410339\n",
      "epoch n°156 : train_loss = 1.9786475896835327, val_loss = 0.7266343832015991\n",
      "epoch n°157 : train_loss = 1.9719599485397339, val_loss = 0.7284714579582214\n",
      "epoch n°158 : train_loss = 1.9771672487258911, val_loss = 0.7264928221702576\n",
      "epoch n°159 : train_loss = 1.9757790565490723, val_loss = 0.7315393686294556\n",
      "epoch n°160 : train_loss = 1.9697283506393433, val_loss = 0.7317769527435303\n",
      "epoch n°161 : train_loss = 1.98162043094635, val_loss = 0.7331709265708923\n",
      "epoch n°162 : train_loss = 1.970658302307129, val_loss = 0.7269775867462158\n",
      "epoch n°163 : train_loss = 1.974172592163086, val_loss = 0.7343509793281555\n",
      "epoch n°164 : train_loss = 1.9803829193115234, val_loss = 0.7277171611785889\n",
      "epoch n°165 : train_loss = 1.966618299484253, val_loss = 0.7264624834060669\n",
      "epoch n°166 : train_loss = 1.9608560800552368, val_loss = 0.7263049483299255\n",
      "epoch n°167 : train_loss = 1.973875880241394, val_loss = 0.730774462223053\n",
      "epoch n°168 : train_loss = 1.962303876876831, val_loss = 0.7312871217727661\n",
      "epoch n°169 : train_loss = 1.9799928665161133, val_loss = 0.7292040586471558\n",
      "epoch n°170 : train_loss = 1.9749220609664917, val_loss = 0.73049396276474\n",
      "epoch n°171 : train_loss = 1.959778904914856, val_loss = 0.7341294884681702\n",
      "epoch n°172 : train_loss = 1.9641083478927612, val_loss = 0.729789674282074\n",
      "epoch n°173 : train_loss = 1.968687653541565, val_loss = 0.7326778173446655\n",
      "epoch n°174 : train_loss = 1.957500696182251, val_loss = 0.7294600605964661\n",
      "epoch n°175 : train_loss = 1.9601414203643799, val_loss = 0.7343773245811462\n",
      "epoch n°176 : train_loss = 1.9605751037597656, val_loss = 0.7319836020469666\n",
      "epoch n°177 : train_loss = 1.9594718217849731, val_loss = 0.7280710339546204\n",
      "epoch n°178 : train_loss = 1.9605027437210083, val_loss = 0.7303889989852905\n",
      "epoch n°179 : train_loss = 1.9504038095474243, val_loss = 0.7227058410644531\n",
      "epoch n°180 : train_loss = 1.9641543626785278, val_loss = 0.7263402938842773\n",
      "epoch n°181 : train_loss = 1.9520514011383057, val_loss = 0.7341635823249817\n",
      "epoch n°182 : train_loss = 1.962138056755066, val_loss = 0.7243383526802063\n",
      "epoch n°183 : train_loss = 1.9653880596160889, val_loss = 0.7269458770751953\n",
      "epoch n°184 : train_loss = 1.9584711790084839, val_loss = 0.7311854958534241\n",
      "epoch n°185 : train_loss = 1.9398118257522583, val_loss = 0.7294513583183289\n",
      "epoch n°186 : train_loss = 1.9555963277816772, val_loss = 0.7260401248931885\n",
      "epoch n°187 : train_loss = 1.962534785270691, val_loss = 0.7271503210067749\n",
      "epoch n°188 : train_loss = 1.9510011672973633, val_loss = 0.726597011089325\n",
      "epoch n°189 : train_loss = 1.9479351043701172, val_loss = 0.7258272767066956\n",
      "epoch n°190 : train_loss = 1.9529463052749634, val_loss = 0.7228536009788513\n",
      "epoch n°191 : train_loss = 1.9485909938812256, val_loss = 0.7263790965080261\n",
      "epoch n°192 : train_loss = 1.9471558332443237, val_loss = 0.7257568836212158\n",
      "epoch n°193 : train_loss = 1.94636070728302, val_loss = 0.7236882448196411\n",
      "epoch n°194 : train_loss = 1.947174310684204, val_loss = 0.7249938249588013\n",
      "epoch n°195 : train_loss = 1.9444886445999146, val_loss = 0.7269560098648071\n",
      "epoch n°196 : train_loss = 1.9541761875152588, val_loss = 0.732517421245575\n",
      "epoch n°197 : train_loss = 1.9505372047424316, val_loss = 0.7265889644622803\n",
      "epoch n°198 : train_loss = 1.943412184715271, val_loss = 0.7271294593811035\n",
      "epoch n°199 : train_loss = 1.950134515762329, val_loss = 0.7276554107666016\n",
      "epoch n°200 : train_loss = 1.9550421237945557, val_loss = 0.7268325090408325\n",
      "epoch n°201 : train_loss = 1.937307596206665, val_loss = 0.7286218404769897\n",
      "epoch n°202 : train_loss = 1.9400802850723267, val_loss = 0.7260599136352539\n",
      "epoch n°203 : train_loss = 1.9463971853256226, val_loss = 0.7322566509246826\n",
      "epoch n°204 : train_loss = 1.9582440853118896, val_loss = 0.7281339168548584\n",
      "epoch n°205 : train_loss = 1.946946620941162, val_loss = 0.7294893860816956\n",
      "epoch n°206 : train_loss = 1.9378905296325684, val_loss = 0.727931797504425\n",
      "epoch n°207 : train_loss = 1.939457893371582, val_loss = 0.720118522644043\n",
      "epoch n°208 : train_loss = 1.9324917793273926, val_loss = 0.7245709896087646\n",
      "epoch n°209 : train_loss = 1.9390236139297485, val_loss = 0.7260686755180359\n",
      "epoch n°210 : train_loss = 1.9362022876739502, val_loss = 0.7231301069259644\n",
      "epoch n°211 : train_loss = 1.9295964241027832, val_loss = 0.7293135523796082\n",
      "epoch n°212 : train_loss = 1.9395676851272583, val_loss = 0.7234331369400024\n",
      "epoch n°213 : train_loss = 1.9446018934249878, val_loss = 0.7237773537635803\n",
      "epoch n°214 : train_loss = 1.945451259613037, val_loss = 0.7278886437416077\n",
      "epoch n°215 : train_loss = 1.9509308338165283, val_loss = 0.723702609539032\n",
      "epoch n°216 : train_loss = 1.9444024562835693, val_loss = 0.7275902032852173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°217 : train_loss = 1.9441889524459839, val_loss = 0.7248989343643188\n",
      "epoch n°218 : train_loss = 1.95054292678833, val_loss = 0.7246847152709961\n",
      "epoch n°219 : train_loss = 1.9434691667556763, val_loss = 0.7268033027648926\n",
      "epoch n°220 : train_loss = 1.9372066259384155, val_loss = 0.729185163974762\n",
      "epoch n°221 : train_loss = 1.938293218612671, val_loss = 0.7301806807518005\n",
      "epoch n°222 : train_loss = 1.9396922588348389, val_loss = 0.7287731766700745\n",
      "epoch n°223 : train_loss = 1.9374659061431885, val_loss = 0.7261753678321838\n",
      "epoch n°224 : train_loss = 1.9425712823867798, val_loss = 0.7231400012969971\n",
      "epoch n°225 : train_loss = 1.9292234182357788, val_loss = 0.7240047454833984\n",
      "epoch n°226 : train_loss = 1.9448230266571045, val_loss = 0.7239157557487488\n",
      "epoch n°227 : train_loss = 1.931715488433838, val_loss = 0.7260242104530334\n",
      "epoch n°228 : train_loss = 1.939355731010437, val_loss = 0.7244805693626404\n",
      "epoch n°229 : train_loss = 1.9430782794952393, val_loss = 0.7276484966278076\n",
      "epoch n°230 : train_loss = 1.9517017602920532, val_loss = 0.7267925143241882\n",
      "epoch n°231 : train_loss = 1.9336148500442505, val_loss = 0.7232842445373535\n",
      "epoch n°232 : train_loss = 1.9302904605865479, val_loss = 0.7205081582069397\n",
      "epoch n°233 : train_loss = 1.9354888200759888, val_loss = 0.727234959602356\n",
      "epoch n°234 : train_loss = 1.9377028942108154, val_loss = 0.7243183255195618\n",
      "epoch n°235 : train_loss = 1.9419081211090088, val_loss = 0.7247084975242615\n",
      "epoch n°236 : train_loss = 1.94504714012146, val_loss = 0.7243036031723022\n",
      "epoch n°237 : train_loss = 1.9324678182601929, val_loss = 0.726606547832489\n",
      "epoch n°238 : train_loss = 1.9381574392318726, val_loss = 0.7260587811470032\n",
      "epoch n°239 : train_loss = 1.939283013343811, val_loss = 0.7296543717384338\n",
      "epoch n°240 : train_loss = 1.941845417022705, val_loss = 0.7318497896194458\n",
      "epoch n°241 : train_loss = 1.9665417671203613, val_loss = 0.7317185401916504\n",
      "epoch n°242 : train_loss = 1.9480621814727783, val_loss = 0.7293215990066528\n",
      "epoch n°243 : train_loss = 1.9653432369232178, val_loss = 0.7290397882461548\n",
      "epoch n°244 : train_loss = 1.9612668752670288, val_loss = 0.7322230935096741\n",
      "epoch n°245 : train_loss = 1.9600000381469727, val_loss = 0.7316649556159973\n",
      "epoch n°246 : train_loss = 1.9726617336273193, val_loss = 0.7293094396591187\n",
      "epoch n°247 : train_loss = 1.9644391536712646, val_loss = 0.7382767200469971\n",
      "epoch n°248 : train_loss = 1.9731780290603638, val_loss = 0.7316350936889648\n",
      "epoch n°249 : train_loss = 1.9582629203796387, val_loss = 0.7285096645355225\n",
      "epoch n°250 : train_loss = 1.9561198949813843, val_loss = 0.7299529314041138\n",
      "epoch n°251 : train_loss = 1.9725909233093262, val_loss = 0.7286822199821472\n",
      "epoch n°252 : train_loss = 1.9663046598434448, val_loss = 0.7311269640922546\n",
      "epoch n°253 : train_loss = 1.964318037033081, val_loss = 0.7252461314201355\n",
      "epoch n°254 : train_loss = 1.9673137664794922, val_loss = 0.732230544090271\n",
      "epoch n°255 : train_loss = 1.9605976343154907, val_loss = 0.727193295955658\n",
      "epoch n°256 : train_loss = 1.966686487197876, val_loss = 0.7300940752029419\n",
      "epoch n°257 : train_loss = 1.9553800821304321, val_loss = 0.7361946105957031\n",
      "epoch n°258 : train_loss = 1.9796792268753052, val_loss = 0.7346791625022888\n",
      "epoch n°259 : train_loss = 1.9534612894058228, val_loss = 0.730344831943512\n",
      "epoch n°260 : train_loss = 1.946513056755066, val_loss = 0.7328235507011414\n",
      "epoch n°261 : train_loss = 1.9571479558944702, val_loss = 0.7305107116699219\n",
      "epoch n°262 : train_loss = 1.9675648212432861, val_loss = 0.7262647747993469\n",
      "epoch n°263 : train_loss = 1.9549814462661743, val_loss = 0.7297496795654297\n",
      "epoch n°264 : train_loss = 1.9727861881256104, val_loss = 0.7314860820770264\n",
      "epoch n°265 : train_loss = 1.9665637016296387, val_loss = 0.729151725769043\n",
      "epoch n°266 : train_loss = 1.9419573545455933, val_loss = 0.7316136956214905\n",
      "epoch n°267 : train_loss = 1.9509069919586182, val_loss = 0.7277002930641174\n",
      "epoch n°268 : train_loss = 1.9560943841934204, val_loss = 0.7277534604072571\n",
      "epoch n°269 : train_loss = 1.9499143362045288, val_loss = 0.7325217127799988\n",
      "epoch n°270 : train_loss = 1.956899642944336, val_loss = 0.7258710861206055\n",
      "epoch n°271 : train_loss = 1.9472582340240479, val_loss = 0.7305850386619568\n",
      "epoch n°272 : train_loss = 1.9471181631088257, val_loss = 0.7322814464569092\n",
      "epoch n°273 : train_loss = 1.9600837230682373, val_loss = 0.7325146198272705\n",
      "epoch n°274 : train_loss = 1.9478381872177124, val_loss = 0.7344933748245239\n",
      "epoch n°275 : train_loss = 1.9524726867675781, val_loss = 0.7256802320480347\n",
      "epoch n°276 : train_loss = 1.9610289335250854, val_loss = 0.7315846681594849\n",
      "epoch n°277 : train_loss = 1.948569655418396, val_loss = 0.7325862646102905\n",
      "epoch n°278 : train_loss = 1.9596740007400513, val_loss = 0.7315418124198914\n",
      "epoch n°279 : train_loss = 1.9544415473937988, val_loss = 0.7243768572807312\n",
      "epoch n°280 : train_loss = 1.9587657451629639, val_loss = 0.7331138849258423\n",
      "epoch n°281 : train_loss = 1.9601634740829468, val_loss = 0.7290093302726746\n",
      "epoch n°282 : train_loss = 1.9542871713638306, val_loss = 0.7272498607635498\n",
      "epoch n°283 : train_loss = 1.963468074798584, val_loss = 0.7302106022834778\n",
      "epoch n°284 : train_loss = 1.9425750970840454, val_loss = 0.7305665016174316\n",
      "epoch n°285 : train_loss = 1.948706865310669, val_loss = 0.7269644737243652\n",
      "epoch n°286 : train_loss = 1.9519541263580322, val_loss = 0.7310737371444702\n",
      "epoch n°287 : train_loss = 1.9539395570755005, val_loss = 0.7339076995849609\n",
      "epoch n°288 : train_loss = 1.9499343633651733, val_loss = 0.7265311479568481\n",
      "epoch n°289 : train_loss = 1.9600430727005005, val_loss = 0.732049822807312\n",
      "epoch n°290 : train_loss = 1.9496088027954102, val_loss = 0.7275800108909607\n",
      "epoch n°291 : train_loss = 1.946025013923645, val_loss = 0.7295148968696594\n",
      "epoch n°292 : train_loss = 1.948984980583191, val_loss = 0.7288655042648315\n",
      "epoch n°293 : train_loss = 1.945417881011963, val_loss = 0.7257041931152344\n",
      "epoch n°294 : train_loss = 1.9534800052642822, val_loss = 0.7288985848426819\n",
      "epoch n°295 : train_loss = 1.9469159841537476, val_loss = 0.7304449081420898\n",
      "epoch n°296 : train_loss = 1.9415253400802612, val_loss = 0.729606032371521\n",
      "epoch n°297 : train_loss = 1.9359931945800781, val_loss = 0.7314769625663757\n",
      "epoch n°298 : train_loss = 1.9468445777893066, val_loss = 0.7270894050598145\n",
      "epoch n°299 : train_loss = 1.933582067489624, val_loss = 0.730776846408844\n",
      "epoch n°300 : train_loss = 1.934748888015747, val_loss = 0.7273528575897217\n",
      "epoch n°301 : train_loss = 1.9467484951019287, val_loss = 0.730283260345459\n",
      "epoch n°302 : train_loss = 1.9447433948516846, val_loss = 0.724851131439209\n",
      "epoch n°303 : train_loss = 1.9393728971481323, val_loss = 0.7286028861999512\n",
      "epoch n°304 : train_loss = 1.9488259553909302, val_loss = 0.72538822889328\n",
      "epoch n°305 : train_loss = 1.9504413604736328, val_loss = 0.7321034073829651\n",
      "epoch n°306 : train_loss = 1.9408127069473267, val_loss = 0.7285349369049072\n",
      "epoch n°307 : train_loss = 1.938317060470581, val_loss = 0.7253667116165161\n",
      "epoch n°308 : train_loss = 1.9365519285202026, val_loss = 0.7290861010551453\n",
      "epoch n°309 : train_loss = 1.9419238567352295, val_loss = 0.7316994071006775\n",
      "epoch n°310 : train_loss = 1.9306011199951172, val_loss = 0.7330124974250793\n",
      "epoch n°311 : train_loss = 1.9476393461227417, val_loss = 0.7245675921440125\n",
      "epoch n°312 : train_loss = 1.9537657499313354, val_loss = 0.7296363115310669\n",
      "epoch n°313 : train_loss = 1.9295531511306763, val_loss = 0.7307754158973694\n",
      "epoch n°314 : train_loss = 1.9476722478866577, val_loss = 0.727838397026062\n",
      "epoch n°315 : train_loss = 1.9501336812973022, val_loss = 0.7291256785392761\n",
      "epoch n°316 : train_loss = 1.9327484369277954, val_loss = 0.7310925126075745\n",
      "epoch n°317 : train_loss = 1.9405382871627808, val_loss = 0.7285155057907104\n",
      "epoch n°318 : train_loss = 1.9286537170410156, val_loss = 0.7255687713623047\n",
      "epoch n°319 : train_loss = 1.9324352741241455, val_loss = 0.7276653051376343\n",
      "epoch n°320 : train_loss = 1.9354504346847534, val_loss = 0.7306310534477234\n",
      "epoch n°321 : train_loss = 1.9285696744918823, val_loss = 0.7234432697296143\n",
      "epoch n°322 : train_loss = 1.9330382347106934, val_loss = 0.7272803783416748\n",
      "epoch n°323 : train_loss = 1.9351881742477417, val_loss = 0.7294692993164062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°324 : train_loss = 1.9371881484985352, val_loss = 0.7279180288314819\n",
      "epoch n°325 : train_loss = 1.9293138980865479, val_loss = 0.7245843410491943\n",
      "epoch n°326 : train_loss = 1.9295066595077515, val_loss = 0.7291541695594788\n",
      "epoch n°327 : train_loss = 1.9269031286239624, val_loss = 0.7290233969688416\n",
      "epoch n°328 : train_loss = 1.9316941499710083, val_loss = 0.7250645160675049\n",
      "epoch n°329 : train_loss = 1.9333678483963013, val_loss = 0.7332742810249329\n",
      "epoch n°330 : train_loss = 1.9271864891052246, val_loss = 0.7303193211555481\n",
      "epoch n°331 : train_loss = 1.9298357963562012, val_loss = 0.7306008338928223\n",
      "epoch n°332 : train_loss = 1.9413584470748901, val_loss = 0.7327774167060852\n",
      "epoch n°333 : train_loss = 1.9311381578445435, val_loss = 0.7293131351470947\n",
      "epoch n°334 : train_loss = 1.9267569780349731, val_loss = 0.7280483841896057\n",
      "epoch n°335 : train_loss = 1.9254437685012817, val_loss = 0.7278575301170349\n",
      "epoch n°336 : train_loss = 1.9158735275268555, val_loss = 0.7254029512405396\n",
      "epoch n°337 : train_loss = 1.92946195602417, val_loss = 0.7282419204711914\n",
      "epoch n°338 : train_loss = 1.9203492403030396, val_loss = 0.7268552184104919\n",
      "epoch n°339 : train_loss = 1.9250259399414062, val_loss = 0.7276274561882019\n",
      "epoch n°340 : train_loss = 1.9153159856796265, val_loss = 0.730812668800354\n",
      "epoch n°341 : train_loss = 1.9240810871124268, val_loss = 0.7329012155532837\n",
      "epoch n°342 : train_loss = 1.9241868257522583, val_loss = 0.7218886613845825\n",
      "epoch n°343 : train_loss = 1.931560754776001, val_loss = 0.7284068465232849\n",
      "epoch n°344 : train_loss = 1.9319788217544556, val_loss = 0.7252042293548584\n",
      "epoch n°345 : train_loss = 1.92823326587677, val_loss = 0.7225866913795471\n",
      "epoch n°346 : train_loss = 1.9230914115905762, val_loss = 0.7279930114746094\n",
      "epoch n°347 : train_loss = 1.9300774335861206, val_loss = 0.7285640835762024\n",
      "epoch n°348 : train_loss = 1.9354885816574097, val_loss = 0.7242445349693298\n",
      "epoch n°349 : train_loss = 1.9223600625991821, val_loss = 0.7244387865066528\n",
      "epoch n°350 : train_loss = 1.9230539798736572, val_loss = 0.7292546629905701\n",
      "epoch n°351 : train_loss = 1.9235726594924927, val_loss = 0.7327016592025757\n",
      "epoch n°352 : train_loss = 1.9308900833129883, val_loss = 0.7288282513618469\n",
      "epoch n°353 : train_loss = 1.906076192855835, val_loss = 0.7313061952590942\n",
      "epoch n°354 : train_loss = 1.91450035572052, val_loss = 0.7233704328536987\n",
      "epoch n°355 : train_loss = 1.924189567565918, val_loss = 0.7260530591011047\n",
      "epoch n°356 : train_loss = 1.921828269958496, val_loss = 0.7302823066711426\n",
      "epoch n°357 : train_loss = 1.9172395467758179, val_loss = 0.7301714420318604\n",
      "epoch n°358 : train_loss = 1.9184595346450806, val_loss = 0.7301740050315857\n",
      "epoch n°359 : train_loss = 1.9120874404907227, val_loss = 0.7230525016784668\n",
      "epoch n°360 : train_loss = 1.9224498271942139, val_loss = 0.7266143560409546\n",
      "epoch n°361 : train_loss = 1.914793610572815, val_loss = 0.7239416837692261\n",
      "epoch n°362 : train_loss = 1.9228583574295044, val_loss = 0.7253984212875366\n",
      "epoch n°363 : train_loss = 1.919411063194275, val_loss = 0.7243171334266663\n",
      "epoch n°364 : train_loss = 1.9179093837738037, val_loss = 0.7269884347915649\n",
      "epoch n°365 : train_loss = 1.9188698530197144, val_loss = 0.7229008078575134\n",
      "epoch n°366 : train_loss = 1.9155179262161255, val_loss = 0.7239319682121277\n",
      "epoch n°367 : train_loss = 1.9184588193893433, val_loss = 0.7265047430992126\n",
      "epoch n°368 : train_loss = 1.9094123840332031, val_loss = 0.7250782251358032\n",
      "epoch n°369 : train_loss = 1.9127304553985596, val_loss = 0.7266971468925476\n",
      "epoch n°370 : train_loss = 1.9162230491638184, val_loss = 0.7264124155044556\n",
      "epoch n°371 : train_loss = 1.8928771018981934, val_loss = 0.7236635088920593\n",
      "epoch n°372 : train_loss = 1.8958834409713745, val_loss = 0.7292735576629639\n",
      "epoch n°373 : train_loss = 1.9106084108352661, val_loss = 0.7278360724449158\n",
      "epoch n°374 : train_loss = 1.9072304964065552, val_loss = 0.7270247340202332\n",
      "epoch n°375 : train_loss = 1.9185044765472412, val_loss = 0.7270190119743347\n",
      "epoch n°376 : train_loss = 1.9130960702896118, val_loss = 0.728277325630188\n",
      "epoch n°377 : train_loss = 1.9188083410263062, val_loss = 0.727910041809082\n",
      "epoch n°378 : train_loss = 1.9082565307617188, val_loss = 0.7255646586418152\n",
      "epoch n°379 : train_loss = 1.9139728546142578, val_loss = 0.721798837184906\n",
      "epoch n°380 : train_loss = 1.912829875946045, val_loss = 0.7268731594085693\n",
      "epoch n°381 : train_loss = 1.9232189655303955, val_loss = 0.7238603234291077\n",
      "epoch n°382 : train_loss = 1.9150322675704956, val_loss = 0.7283161282539368\n",
      "epoch n°383 : train_loss = 1.9118033647537231, val_loss = 0.7287020683288574\n",
      "epoch n°384 : train_loss = 1.9082558155059814, val_loss = 0.7276514768600464\n",
      "epoch n°385 : train_loss = 1.901484727859497, val_loss = 0.7268890738487244\n",
      "epoch n°386 : train_loss = 1.907999873161316, val_loss = 0.7256439924240112\n",
      "epoch n°387 : train_loss = 1.8915996551513672, val_loss = 0.7299643158912659\n",
      "epoch n°388 : train_loss = 1.8986858129501343, val_loss = 0.7264021635055542\n",
      "epoch n°389 : train_loss = 1.9032870531082153, val_loss = 0.7266850471496582\n",
      "epoch n°390 : train_loss = 1.9055126905441284, val_loss = 0.7275077104568481\n",
      "epoch n°391 : train_loss = 1.9078749418258667, val_loss = 0.7256998419761658\n",
      "epoch n°392 : train_loss = 1.8986653089523315, val_loss = 0.7257227897644043\n",
      "epoch n°393 : train_loss = 1.9172849655151367, val_loss = 0.7278292775154114\n",
      "epoch n°394 : train_loss = 1.9055653810501099, val_loss = 0.72632896900177\n",
      "epoch n°395 : train_loss = 1.9010441303253174, val_loss = 0.7185544967651367\n",
      "epoch n°396 : train_loss = 1.9026761054992676, val_loss = 0.7274481654167175\n",
      "epoch n°397 : train_loss = 1.9099204540252686, val_loss = 0.7274879217147827\n",
      "epoch n°398 : train_loss = 1.91585373878479, val_loss = 0.7285239100456238\n",
      "epoch n°399 : train_loss = 1.9067119359970093, val_loss = 0.7240889072418213\n",
      "epoch n°400 : train_loss = 1.9006530046463013, val_loss = 0.7277576923370361\n",
      "epoch n°401 : train_loss = 1.8916696310043335, val_loss = 0.7243505120277405\n",
      "epoch n°402 : train_loss = 1.8972821235656738, val_loss = 0.7249568700790405\n",
      "epoch n°403 : train_loss = 1.8990179300308228, val_loss = 0.7261390089988708\n",
      "epoch n°404 : train_loss = 1.8991432189941406, val_loss = 0.7259793877601624\n",
      "epoch n°405 : train_loss = 1.8843340873718262, val_loss = 0.7216736078262329\n",
      "epoch n°406 : train_loss = 1.8971887826919556, val_loss = 0.7272120118141174\n",
      "epoch n°407 : train_loss = 1.9137979745864868, val_loss = 0.7252945303916931\n",
      "epoch n°408 : train_loss = 1.908774733543396, val_loss = 0.7252107858657837\n",
      "epoch n°409 : train_loss = 1.906976580619812, val_loss = 0.7293948531150818\n",
      "epoch n°410 : train_loss = 1.9016201496124268, val_loss = 0.7257798910140991\n",
      "epoch n°411 : train_loss = 1.904608964920044, val_loss = 0.7191947102546692\n",
      "epoch n°412 : train_loss = 1.889978051185608, val_loss = 0.7290765047073364\n",
      "epoch n°413 : train_loss = 1.9054332971572876, val_loss = 0.731198251247406\n",
      "epoch n°414 : train_loss = 1.9020402431488037, val_loss = 0.7255937457084656\n",
      "epoch n°415 : train_loss = 1.9084515571594238, val_loss = 0.7256483435630798\n",
      "epoch n°416 : train_loss = 1.8999117612838745, val_loss = 0.7229805588722229\n",
      "epoch n°417 : train_loss = 1.8975818157196045, val_loss = 0.7261108160018921\n",
      "epoch n°418 : train_loss = 1.8996691703796387, val_loss = 0.7257384061813354\n",
      "epoch n°419 : train_loss = 1.9031137228012085, val_loss = 0.7253038883209229\n",
      "epoch n°420 : train_loss = 1.8954006433486938, val_loss = 0.7288447618484497\n",
      "epoch n°421 : train_loss = 1.8989489078521729, val_loss = 0.722356379032135\n",
      "epoch n°422 : train_loss = 1.8983324766159058, val_loss = 0.7225607633590698\n",
      "epoch n°423 : train_loss = 1.887338399887085, val_loss = 0.7301679253578186\n",
      "epoch n°424 : train_loss = 1.8883529901504517, val_loss = 0.7215975522994995\n",
      "epoch n°425 : train_loss = 1.8972411155700684, val_loss = 0.7210658192634583\n",
      "epoch n°426 : train_loss = 1.8898746967315674, val_loss = 0.7261953353881836\n",
      "epoch n°427 : train_loss = 1.903451919555664, val_loss = 0.7292273044586182\n",
      "epoch n°428 : train_loss = 1.8960168361663818, val_loss = 0.7299997806549072\n",
      "epoch n°429 : train_loss = 1.9005882740020752, val_loss = 0.7262105941772461\n",
      "epoch n°430 : train_loss = 1.8922033309936523, val_loss = 0.7216980457305908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°431 : train_loss = 1.8900516033172607, val_loss = 0.7274057865142822\n",
      "epoch n°432 : train_loss = 1.8967887163162231, val_loss = 0.7206237316131592\n",
      "epoch n°433 : train_loss = 1.8943637609481812, val_loss = 0.7211494445800781\n",
      "epoch n°434 : train_loss = 1.882729172706604, val_loss = 0.7221794128417969\n",
      "epoch n°435 : train_loss = 1.8988847732543945, val_loss = 0.7269898056983948\n",
      "epoch n°436 : train_loss = 1.8785099983215332, val_loss = 0.7241218090057373\n",
      "epoch n°437 : train_loss = 1.8921942710876465, val_loss = 0.7239080667495728\n",
      "epoch n°438 : train_loss = 1.9000368118286133, val_loss = 0.7227378487586975\n",
      "epoch n°439 : train_loss = 1.8972253799438477, val_loss = 0.725124716758728\n",
      "epoch n°440 : train_loss = 1.8989442586898804, val_loss = 0.7260505557060242\n",
      "epoch n°441 : train_loss = 1.885783076286316, val_loss = 0.7254689931869507\n",
      "epoch n°442 : train_loss = 1.9018280506134033, val_loss = 0.7283207774162292\n",
      "epoch n°443 : train_loss = 1.9041774272918701, val_loss = 0.7253906726837158\n",
      "epoch n°444 : train_loss = 1.8871004581451416, val_loss = 0.7268569469451904\n",
      "epoch n°445 : train_loss = 1.8811194896697998, val_loss = 0.7226126194000244\n",
      "epoch n°446 : train_loss = 1.9065065383911133, val_loss = 0.7253912091255188\n",
      "epoch n°447 : train_loss = 1.8875725269317627, val_loss = 0.7217013835906982\n",
      "epoch n°448 : train_loss = 1.8935891389846802, val_loss = 0.7191485166549683\n",
      "epoch n°449 : train_loss = 1.8874573707580566, val_loss = 0.7262285351753235\n",
      "epoch n°450 : train_loss = 1.89248788356781, val_loss = 0.7277748584747314\n",
      "epoch n°451 : train_loss = 1.9000356197357178, val_loss = 0.72512286901474\n",
      "epoch n°452 : train_loss = 1.9003888368606567, val_loss = 0.7286947965621948\n",
      "epoch n°453 : train_loss = 1.8859962224960327, val_loss = 0.7283486723899841\n",
      "epoch n°454 : train_loss = 1.9050813913345337, val_loss = 0.7258548140525818\n",
      "epoch n°455 : train_loss = 1.8944159746170044, val_loss = 0.7271251678466797\n",
      "epoch n°456 : train_loss = 1.8877732753753662, val_loss = 0.7289427518844604\n",
      "epoch n°457 : train_loss = 1.890651822090149, val_loss = 0.7250202298164368\n",
      "epoch n°458 : train_loss = 1.8868283033370972, val_loss = 0.7173980474472046\n",
      "epoch n°459 : train_loss = 1.8790730237960815, val_loss = 0.7236304879188538\n",
      "epoch n°460 : train_loss = 1.884574294090271, val_loss = 0.7269417643547058\n",
      "epoch n°461 : train_loss = 1.8839738368988037, val_loss = 0.7238503098487854\n",
      "epoch n°462 : train_loss = 1.8882986307144165, val_loss = 0.7233901023864746\n",
      "epoch n°463 : train_loss = 1.8815630674362183, val_loss = 0.7251845002174377\n",
      "epoch n°464 : train_loss = 1.9021097421646118, val_loss = 0.7223469614982605\n",
      "epoch n°465 : train_loss = 1.8897966146469116, val_loss = 0.7225636839866638\n",
      "epoch n°466 : train_loss = 1.8842469453811646, val_loss = 0.7242969870567322\n",
      "epoch n°467 : train_loss = 1.8934892416000366, val_loss = 0.7261561155319214\n",
      "epoch n°468 : train_loss = 1.8878577947616577, val_loss = 0.7263855934143066\n",
      "epoch n°469 : train_loss = 1.8873227834701538, val_loss = 0.7287541031837463\n",
      "epoch n°470 : train_loss = 1.8834787607192993, val_loss = 0.7263758778572083\n",
      "epoch n°471 : train_loss = 1.8800116777420044, val_loss = 0.7241125106811523\n",
      "epoch n°472 : train_loss = 1.8886070251464844, val_loss = 0.7274080514907837\n",
      "epoch n°473 : train_loss = 1.8845263719558716, val_loss = 0.7267118096351624\n",
      "epoch n°474 : train_loss = 1.892756462097168, val_loss = 0.7239977717399597\n",
      "epoch n°475 : train_loss = 1.8963254690170288, val_loss = 0.7200340628623962\n",
      "epoch n°476 : train_loss = 1.8879077434539795, val_loss = 0.7306819558143616\n",
      "epoch n°477 : train_loss = 1.8944523334503174, val_loss = 0.7243633270263672\n",
      "epoch n°478 : train_loss = 1.8805276155471802, val_loss = 0.7249945402145386\n",
      "epoch n°479 : train_loss = 1.8901207447052002, val_loss = 0.7194546461105347\n",
      "epoch n°480 : train_loss = 1.9002610445022583, val_loss = 0.7280263304710388\n",
      "epoch n°481 : train_loss = 1.8892194032669067, val_loss = 0.7258905172348022\n",
      "epoch n°482 : train_loss = 1.8931525945663452, val_loss = 0.724050760269165\n",
      "epoch n°483 : train_loss = 1.887215256690979, val_loss = 0.7274594306945801\n",
      "epoch n°484 : train_loss = 1.8824158906936646, val_loss = 0.7282657027244568\n",
      "epoch n°485 : train_loss = 1.8814971446990967, val_loss = 0.725441038608551\n",
      "epoch n°486 : train_loss = 1.890487790107727, val_loss = 0.7246880531311035\n",
      "epoch n°487 : train_loss = 1.8785539865493774, val_loss = 0.7246736884117126\n",
      "epoch n°488 : train_loss = 1.8905452489852905, val_loss = 0.725440502166748\n",
      "epoch n°489 : train_loss = 1.8931164741516113, val_loss = 0.7218877673149109\n",
      "epoch n°490 : train_loss = 1.8918108940124512, val_loss = 0.7268995642662048\n",
      "epoch n°491 : train_loss = 1.8851505517959595, val_loss = 0.7244288325309753\n",
      "epoch n°492 : train_loss = 1.8863085508346558, val_loss = 0.7236686944961548\n",
      "epoch n°493 : train_loss = 1.894769549369812, val_loss = 0.7245078682899475\n",
      "epoch n°494 : train_loss = 1.8896257877349854, val_loss = 0.7230635285377502\n",
      "epoch n°495 : train_loss = 1.8964381217956543, val_loss = 0.7197867631912231\n",
      "epoch n°496 : train_loss = 1.9092686176300049, val_loss = 0.7316033840179443\n",
      "epoch n°497 : train_loss = 1.92328679561615, val_loss = 0.7282371520996094\n",
      "epoch n°498 : train_loss = 1.9263149499893188, val_loss = 0.7284709811210632\n",
      "epoch n°499 : train_loss = 1.9128942489624023, val_loss = 0.7269481420516968\n",
      "epoch n°500 : train_loss = 1.9040145874023438, val_loss = 0.7294820547103882\n",
      "epoch n°501 : train_loss = 1.9178030490875244, val_loss = 0.7264434695243835\n",
      "epoch n°502 : train_loss = 1.917770504951477, val_loss = 0.7316040992736816\n",
      "epoch n°503 : train_loss = 1.9242383241653442, val_loss = 0.7253100275993347\n",
      "epoch n°504 : train_loss = 1.9117794036865234, val_loss = 0.7318525314331055\n",
      "epoch n°505 : train_loss = 1.910475254058838, val_loss = 0.7267295718193054\n",
      "epoch n°506 : train_loss = 1.9197992086410522, val_loss = 0.7273702621459961\n",
      "epoch n°507 : train_loss = 1.9190362691879272, val_loss = 0.7301269173622131\n",
      "epoch n°508 : train_loss = 1.918089747428894, val_loss = 0.7280896306037903\n",
      "epoch n°509 : train_loss = 1.9112430810928345, val_loss = 0.7312736511230469\n",
      "epoch n°510 : train_loss = 1.9125806093215942, val_loss = 0.7329912185668945\n",
      "epoch n°511 : train_loss = 1.920669436454773, val_loss = 0.7302936911582947\n",
      "epoch n°512 : train_loss = 1.9147239923477173, val_loss = 0.7339248061180115\n",
      "epoch n°513 : train_loss = 1.9231117963790894, val_loss = 0.733634889125824\n",
      "epoch n°514 : train_loss = 1.934123158454895, val_loss = 0.7285917401313782\n",
      "epoch n°515 : train_loss = 1.9181832075119019, val_loss = 0.7266419529914856\n",
      "epoch n°516 : train_loss = 1.9221652746200562, val_loss = 0.7280222177505493\n",
      "epoch n°517 : train_loss = 1.9098267555236816, val_loss = 0.7240201234817505\n",
      "epoch n°518 : train_loss = 1.9203484058380127, val_loss = 0.7361857295036316\n",
      "epoch n°519 : train_loss = 1.9186713695526123, val_loss = 0.725050151348114\n",
      "epoch n°520 : train_loss = 1.9208897352218628, val_loss = 0.7304882407188416\n",
      "epoch n°521 : train_loss = 1.9227339029312134, val_loss = 0.7325349450111389\n",
      "epoch n°522 : train_loss = 1.9193177223205566, val_loss = 0.7277012467384338\n",
      "epoch n°523 : train_loss = 1.9184588193893433, val_loss = 0.7276033759117126\n",
      "epoch n°524 : train_loss = 1.9232823848724365, val_loss = 0.7244306802749634\n",
      "epoch n°525 : train_loss = 1.9181369543075562, val_loss = 0.726698100566864\n",
      "epoch n°526 : train_loss = 1.9085475206375122, val_loss = 0.7322048544883728\n",
      "epoch n°527 : train_loss = 1.9180264472961426, val_loss = 0.7291905283927917\n",
      "epoch n°528 : train_loss = 1.917942762374878, val_loss = 0.7279389500617981\n",
      "epoch n°529 : train_loss = 1.9147238731384277, val_loss = 0.7261911034584045\n",
      "epoch n°530 : train_loss = 1.9204256534576416, val_loss = 0.7291548848152161\n",
      "epoch n°531 : train_loss = 1.9282159805297852, val_loss = 0.7304852604866028\n",
      "epoch n°532 : train_loss = 1.9134607315063477, val_loss = 0.7273213267326355\n",
      "epoch n°533 : train_loss = 1.906198501586914, val_loss = 0.7311909794807434\n",
      "epoch n°534 : train_loss = 1.9218353033065796, val_loss = 0.7325830459594727\n",
      "epoch n°535 : train_loss = 1.9244753122329712, val_loss = 0.7232261896133423\n",
      "epoch n°536 : train_loss = 1.9084222316741943, val_loss = 0.7314463257789612\n",
      "epoch n°537 : train_loss = 1.9086096286773682, val_loss = 0.723944902420044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°538 : train_loss = 1.916497826576233, val_loss = 0.7283567786216736\n",
      "epoch n°539 : train_loss = 1.9253374338150024, val_loss = 0.7257511615753174\n",
      "epoch n°540 : train_loss = 1.92128586769104, val_loss = 0.7260490655899048\n",
      "epoch n°541 : train_loss = 1.9111379384994507, val_loss = 0.7288305759429932\n",
      "epoch n°542 : train_loss = 1.914551854133606, val_loss = 0.7308788895606995\n",
      "epoch n°543 : train_loss = 1.9099669456481934, val_loss = 0.7301961779594421\n",
      "epoch n°544 : train_loss = 1.919555902481079, val_loss = 0.7291899919509888\n",
      "epoch n°545 : train_loss = 1.9077311754226685, val_loss = 0.7325179576873779\n",
      "epoch n°546 : train_loss = 1.9138327836990356, val_loss = 0.7298210859298706\n",
      "epoch n°547 : train_loss = 1.9118400812149048, val_loss = 0.7385391592979431\n",
      "epoch n°548 : train_loss = 1.9201281070709229, val_loss = 0.7285159230232239\n",
      "epoch n°549 : train_loss = 1.9317315816879272, val_loss = 0.7277671098709106\n",
      "epoch n°550 : train_loss = 1.9182851314544678, val_loss = 0.7271064519882202\n",
      "epoch n°551 : train_loss = 1.9243841171264648, val_loss = 0.7294867634773254\n",
      "epoch n°552 : train_loss = 1.9125516414642334, val_loss = 0.7269960641860962\n",
      "epoch n°553 : train_loss = 1.9094072580337524, val_loss = 0.7331057786941528\n",
      "epoch n°554 : train_loss = 1.9153646230697632, val_loss = 0.7279789447784424\n",
      "epoch n°555 : train_loss = 1.9159588813781738, val_loss = 0.7271387577056885\n",
      "epoch n°556 : train_loss = 1.9135587215423584, val_loss = 0.7328327894210815\n",
      "epoch n°557 : train_loss = 1.918868899345398, val_loss = 0.7216283679008484\n",
      "epoch n°558 : train_loss = 1.922159194946289, val_loss = 0.72646564245224\n",
      "epoch n°559 : train_loss = 1.9037474393844604, val_loss = 0.7286741137504578\n",
      "epoch n°560 : train_loss = 1.9181597232818604, val_loss = 0.7283944487571716\n",
      "epoch n°561 : train_loss = 1.9163097143173218, val_loss = 0.7295258045196533\n",
      "epoch n°562 : train_loss = 1.9151231050491333, val_loss = 0.7211973667144775\n",
      "epoch n°563 : train_loss = 1.9097429513931274, val_loss = 0.7294027805328369\n",
      "epoch n°564 : train_loss = 1.913497805595398, val_loss = 0.7294519543647766\n",
      "epoch n°565 : train_loss = 1.9148869514465332, val_loss = 0.7249343395233154\n",
      "epoch n°566 : train_loss = 1.9141203165054321, val_loss = 0.7266449332237244\n",
      "epoch n°567 : train_loss = 1.9137593507766724, val_loss = 0.7254862785339355\n",
      "epoch n°568 : train_loss = 1.9118506908416748, val_loss = 0.7223416566848755\n",
      "epoch n°569 : train_loss = 1.9158966541290283, val_loss = 0.7285537719726562\n",
      "epoch n°570 : train_loss = 1.9126874208450317, val_loss = 0.7334067225456238\n",
      "epoch n°571 : train_loss = 1.91457998752594, val_loss = 0.7230285406112671\n",
      "epoch n°572 : train_loss = 1.9011471271514893, val_loss = 0.730254590511322\n",
      "epoch n°573 : train_loss = 1.9117108583450317, val_loss = 0.7310655117034912\n",
      "epoch n°574 : train_loss = 1.917312741279602, val_loss = 0.7270637154579163\n",
      "epoch n°575 : train_loss = 1.9204981327056885, val_loss = 0.7291145920753479\n",
      "epoch n°576 : train_loss = 1.9216731786727905, val_loss = 0.7228828072547913\n",
      "epoch n°577 : train_loss = 1.8983232975006104, val_loss = 0.7297057509422302\n",
      "epoch n°578 : train_loss = 1.9085193872451782, val_loss = 0.7253412008285522\n",
      "epoch n°579 : train_loss = 1.9235458374023438, val_loss = 0.7260844111442566\n",
      "epoch n°580 : train_loss = 1.9147733449935913, val_loss = 0.7260065674781799\n",
      "epoch n°581 : train_loss = 1.911846399307251, val_loss = 0.7278919219970703\n",
      "epoch n°582 : train_loss = 1.9171303510665894, val_loss = 0.7280861139297485\n",
      "epoch n°583 : train_loss = 1.9109920263290405, val_loss = 0.7269712090492249\n",
      "epoch n°584 : train_loss = 1.9085628986358643, val_loss = 0.7287892699241638\n",
      "epoch n°585 : train_loss = 1.9193698167800903, val_loss = 0.7290709614753723\n",
      "epoch n°586 : train_loss = 1.9071887731552124, val_loss = 0.728275716304779\n",
      "epoch n°587 : train_loss = 1.9169130325317383, val_loss = 0.7279126048088074\n",
      "epoch n°588 : train_loss = 1.9080156087875366, val_loss = 0.7212415337562561\n",
      "epoch n°589 : train_loss = 1.9102839231491089, val_loss = 0.7246643304824829\n",
      "epoch n°590 : train_loss = 1.9016480445861816, val_loss = 0.7231441140174866\n",
      "epoch n°591 : train_loss = 1.9098730087280273, val_loss = 0.73194420337677\n",
      "epoch n°592 : train_loss = 1.898684024810791, val_loss = 0.7283167243003845\n",
      "epoch n°593 : train_loss = 1.9007405042648315, val_loss = 0.7282299995422363\n",
      "epoch n°594 : train_loss = 1.9057917594909668, val_loss = 0.7253300547599792\n",
      "epoch n°595 : train_loss = 1.8999676704406738, val_loss = 0.7281712889671326\n",
      "epoch n°596 : train_loss = 1.9102818965911865, val_loss = 0.7291600704193115\n",
      "epoch n°597 : train_loss = 1.9152294397354126, val_loss = 0.7293140888214111\n",
      "epoch n°598 : train_loss = 1.903764247894287, val_loss = 0.7282239198684692\n",
      "epoch n°599 : train_loss = 1.9020379781723022, val_loss = 0.727242112159729\n",
      "epoch n°600 : train_loss = 1.9068703651428223, val_loss = 0.7328968644142151\n",
      "epoch n°601 : train_loss = 1.9182391166687012, val_loss = 0.7254199981689453\n",
      "epoch n°602 : train_loss = 1.9139710664749146, val_loss = 0.7231316566467285\n",
      "epoch n°603 : train_loss = 1.8931490182876587, val_loss = 0.7241566181182861\n",
      "epoch n°604 : train_loss = 1.9062464237213135, val_loss = 0.7273073196411133\n",
      "epoch n°605 : train_loss = 1.8967376947402954, val_loss = 0.7239375114440918\n",
      "epoch n°606 : train_loss = 1.9129674434661865, val_loss = 0.7312109470367432\n",
      "epoch n°607 : train_loss = 1.9087817668914795, val_loss = 0.7272306084632874\n",
      "epoch n°608 : train_loss = 1.9038455486297607, val_loss = 0.7283723950386047\n",
      "epoch n°609 : train_loss = 1.8914893865585327, val_loss = 0.724054753780365\n",
      "epoch n°610 : train_loss = 1.9040168523788452, val_loss = 0.7330276370048523\n",
      "epoch n°611 : train_loss = 1.8967583179473877, val_loss = 0.7238696217536926\n",
      "epoch n°612 : train_loss = 1.8961942195892334, val_loss = 0.7285934686660767\n",
      "epoch n°613 : train_loss = 1.9019132852554321, val_loss = 0.7296766042709351\n",
      "epoch n°614 : train_loss = 1.8986026048660278, val_loss = 0.730889081954956\n",
      "epoch n°615 : train_loss = 1.9052135944366455, val_loss = 0.7260228395462036\n",
      "epoch n°616 : train_loss = 1.901482343673706, val_loss = 0.7282307147979736\n",
      "epoch n°617 : train_loss = 1.8997784852981567, val_loss = 0.7294033169746399\n",
      "epoch n°618 : train_loss = 1.9050668478012085, val_loss = 0.7330262064933777\n",
      "epoch n°619 : train_loss = 1.906085729598999, val_loss = 0.7221141457557678\n",
      "epoch n°620 : train_loss = 1.8927760124206543, val_loss = 0.7263588309288025\n",
      "epoch n°621 : train_loss = 1.9128105640411377, val_loss = 0.7286633849143982\n",
      "epoch n°622 : train_loss = 1.9119212627410889, val_loss = 0.7227416038513184\n",
      "epoch n°623 : train_loss = 1.8960435390472412, val_loss = 0.7245503067970276\n",
      "epoch n°624 : train_loss = 1.8954933881759644, val_loss = 0.7270315289497375\n",
      "epoch n°625 : train_loss = 1.8978030681610107, val_loss = 0.7266672253608704\n",
      "epoch n°626 : train_loss = 1.8959031105041504, val_loss = 0.729935884475708\n",
      "epoch n°627 : train_loss = 1.8983970880508423, val_loss = 0.7297829985618591\n",
      "epoch n°628 : train_loss = 1.9010831117630005, val_loss = 0.7256289720535278\n",
      "epoch n°629 : train_loss = 1.9028446674346924, val_loss = 0.7289329171180725\n",
      "epoch n°630 : train_loss = 1.8900582790374756, val_loss = 0.7225163578987122\n",
      "epoch n°631 : train_loss = 1.892820954322815, val_loss = 0.724956214427948\n",
      "epoch n°632 : train_loss = 1.9046849012374878, val_loss = 0.7252417206764221\n",
      "epoch n°633 : train_loss = 1.9015953540802002, val_loss = 0.73149573802948\n",
      "epoch n°634 : train_loss = 1.9007270336151123, val_loss = 0.7278850078582764\n",
      "epoch n°635 : train_loss = 1.8938690423965454, val_loss = 0.7245106101036072\n",
      "epoch n°636 : train_loss = 1.9002933502197266, val_loss = 0.7270197868347168\n",
      "epoch n°637 : train_loss = 1.8949247598648071, val_loss = 0.7259407639503479\n",
      "epoch n°638 : train_loss = 1.88425612449646, val_loss = 0.7233738899230957\n",
      "epoch n°639 : train_loss = 1.8959152698516846, val_loss = 0.7279331088066101\n",
      "epoch n°640 : train_loss = 1.8944816589355469, val_loss = 0.7295572757720947\n",
      "epoch n°641 : train_loss = 1.8966612815856934, val_loss = 0.727067768573761\n",
      "epoch n°642 : train_loss = 1.892134189605713, val_loss = 0.7269478440284729\n",
      "epoch n°643 : train_loss = 1.8890267610549927, val_loss = 0.7222179174423218\n",
      "epoch n°644 : train_loss = 1.9034080505371094, val_loss = 0.7248401045799255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°645 : train_loss = 1.9001492261886597, val_loss = 0.7273584008216858\n",
      "epoch n°646 : train_loss = 1.8930867910385132, val_loss = 0.7265209555625916\n",
      "epoch n°647 : train_loss = 1.8970509767532349, val_loss = 0.7248619198799133\n",
      "epoch n°648 : train_loss = 1.8865424394607544, val_loss = 0.7253111600875854\n",
      "epoch n°649 : train_loss = 1.9050257205963135, val_loss = 0.7253919839859009\n",
      "epoch n°650 : train_loss = 1.9017170667648315, val_loss = 0.7273049354553223\n",
      "epoch n°651 : train_loss = 1.8910820484161377, val_loss = 0.730283796787262\n",
      "epoch n°652 : train_loss = 1.8957760334014893, val_loss = 0.7265708446502686\n",
      "epoch n°653 : train_loss = 1.8865418434143066, val_loss = 0.7302185297012329\n",
      "epoch n°654 : train_loss = 1.8945828676223755, val_loss = 0.7266501784324646\n",
      "epoch n°655 : train_loss = 1.9016592502593994, val_loss = 0.7327569127082825\n",
      "epoch n°656 : train_loss = 1.8979665040969849, val_loss = 0.7288306355476379\n",
      "epoch n°657 : train_loss = 1.893412470817566, val_loss = 0.725906252861023\n",
      "epoch n°658 : train_loss = 1.8912161588668823, val_loss = 0.7254818081855774\n",
      "epoch n°659 : train_loss = 1.900338888168335, val_loss = 0.7267880439758301\n",
      "epoch n°660 : train_loss = 1.890336513519287, val_loss = 0.7267357707023621\n",
      "epoch n°661 : train_loss = 1.8893437385559082, val_loss = 0.7265012264251709\n",
      "epoch n°662 : train_loss = 1.8894765377044678, val_loss = 0.722072184085846\n",
      "epoch n°663 : train_loss = 1.8988343477249146, val_loss = 0.7296905517578125\n",
      "epoch n°664 : train_loss = 1.8926068544387817, val_loss = 0.7295745611190796\n",
      "epoch n°665 : train_loss = 1.8927831649780273, val_loss = 0.7239275574684143\n",
      "epoch n°666 : train_loss = 1.8919003009796143, val_loss = 0.7278530597686768\n",
      "epoch n°667 : train_loss = 1.8925691843032837, val_loss = 0.7204760909080505\n",
      "epoch n°668 : train_loss = 1.8868924379348755, val_loss = 0.7295634746551514\n",
      "epoch n°669 : train_loss = 1.8966519832611084, val_loss = 0.7259519100189209\n",
      "epoch n°670 : train_loss = 1.8905229568481445, val_loss = 0.7258557677268982\n",
      "epoch n°671 : train_loss = 1.8924179077148438, val_loss = 0.7358620166778564\n",
      "epoch n°672 : train_loss = 1.8946079015731812, val_loss = 0.7282198667526245\n",
      "epoch n°673 : train_loss = 1.884623646736145, val_loss = 0.7266064882278442\n",
      "epoch n°674 : train_loss = 1.8979321718215942, val_loss = 0.7241333723068237\n",
      "epoch n°675 : train_loss = 1.8914644718170166, val_loss = 0.7267957329750061\n",
      "epoch n°676 : train_loss = 1.8771437406539917, val_loss = 0.7287527918815613\n",
      "epoch n°677 : train_loss = 1.8913568258285522, val_loss = 0.7305687665939331\n",
      "epoch n°678 : train_loss = 1.8906201124191284, val_loss = 0.7210460305213928\n",
      "epoch n°679 : train_loss = 1.8925679922103882, val_loss = 0.7299726009368896\n",
      "epoch n°680 : train_loss = 1.8853986263275146, val_loss = 0.7251532077789307\n",
      "epoch n°681 : train_loss = 1.890318751335144, val_loss = 0.7238460183143616\n",
      "epoch n°682 : train_loss = 1.8789652585983276, val_loss = 0.7252177000045776\n",
      "epoch n°683 : train_loss = 1.8822518587112427, val_loss = 0.7233523726463318\n",
      "epoch n°684 : train_loss = 1.8827745914459229, val_loss = 0.723164439201355\n",
      "epoch n°685 : train_loss = 1.8805502653121948, val_loss = 0.7246425747871399\n",
      "epoch n°686 : train_loss = 1.8968217372894287, val_loss = 0.7265637516975403\n",
      "epoch n°687 : train_loss = 1.8897875547409058, val_loss = 0.7223067879676819\n",
      "epoch n°688 : train_loss = 1.87840735912323, val_loss = 0.7207834124565125\n",
      "epoch n°689 : train_loss = 1.8848869800567627, val_loss = 0.727508544921875\n",
      "epoch n°690 : train_loss = 1.876466155052185, val_loss = 0.7251272201538086\n",
      "epoch n°691 : train_loss = 1.885561466217041, val_loss = 0.7290632128715515\n",
      "epoch n°692 : train_loss = 1.8761341571807861, val_loss = 0.7297658920288086\n",
      "epoch n°693 : train_loss = 1.8786414861679077, val_loss = 0.7238379120826721\n",
      "epoch n°694 : train_loss = 1.8928732872009277, val_loss = 0.7262231707572937\n",
      "epoch n°695 : train_loss = 1.8762344121932983, val_loss = 0.7314266562461853\n",
      "epoch n°696 : train_loss = 1.8877677917480469, val_loss = 0.7298402786254883\n",
      "epoch n°697 : train_loss = 1.8901101350784302, val_loss = 0.7249569892883301\n",
      "epoch n°698 : train_loss = 1.878616452217102, val_loss = 0.7273824214935303\n",
      "epoch n°699 : train_loss = 1.8785635232925415, val_loss = 0.7274929285049438\n",
      "epoch n°700 : train_loss = 1.8835450410842896, val_loss = 0.7263349294662476\n",
      "epoch n°701 : train_loss = 1.8830236196517944, val_loss = 0.7300496697425842\n",
      "epoch n°702 : train_loss = 1.8714784383773804, val_loss = 0.7286326289176941\n",
      "epoch n°703 : train_loss = 1.8872514963150024, val_loss = 0.7249552607536316\n",
      "epoch n°704 : train_loss = 1.8790017366409302, val_loss = 0.7218983769416809\n",
      "epoch n°705 : train_loss = 1.8767719268798828, val_loss = 0.7225354909896851\n",
      "epoch n°706 : train_loss = 1.8865679502487183, val_loss = 0.7288951873779297\n",
      "epoch n°707 : train_loss = 1.874760389328003, val_loss = 0.7290524244308472\n",
      "epoch n°708 : train_loss = 1.8853944540023804, val_loss = 0.7311932444572449\n",
      "epoch n°709 : train_loss = 1.8773044347763062, val_loss = 0.7217515110969543\n",
      "epoch n°710 : train_loss = 1.8815490007400513, val_loss = 0.7245175838470459\n",
      "epoch n°711 : train_loss = 1.8862149715423584, val_loss = 0.7273204922676086\n",
      "epoch n°712 : train_loss = 1.8818079233169556, val_loss = 0.7285508513450623\n",
      "epoch n°713 : train_loss = 1.87754225730896, val_loss = 0.7232186794281006\n",
      "epoch n°714 : train_loss = 1.873473882675171, val_loss = 0.724086582660675\n",
      "epoch n°715 : train_loss = 1.877792239189148, val_loss = 0.7300389409065247\n",
      "epoch n°716 : train_loss = 1.888453483581543, val_loss = 0.7270774245262146\n",
      "epoch n°717 : train_loss = 1.8854484558105469, val_loss = 0.7281837463378906\n",
      "epoch n°718 : train_loss = 1.8882216215133667, val_loss = 0.7236040830612183\n",
      "epoch n°719 : train_loss = 1.8775932788848877, val_loss = 0.7280037999153137\n",
      "epoch n°720 : train_loss = 1.8776466846466064, val_loss = 0.7249906063079834\n",
      "epoch n°721 : train_loss = 1.8780995607376099, val_loss = 0.726789653301239\n",
      "epoch n°722 : train_loss = 1.8860541582107544, val_loss = 0.7262377738952637\n",
      "epoch n°723 : train_loss = 1.8777021169662476, val_loss = 0.7288658618927002\n",
      "epoch n°724 : train_loss = 1.8649096488952637, val_loss = 0.7229630947113037\n",
      "epoch n°725 : train_loss = 1.8746315240859985, val_loss = 0.7233622670173645\n",
      "epoch n°726 : train_loss = 1.8678983449935913, val_loss = 0.7284817695617676\n",
      "epoch n°727 : train_loss = 1.8772497177124023, val_loss = 0.7281683683395386\n",
      "epoch n°728 : train_loss = 1.8747894763946533, val_loss = 0.7278276681900024\n",
      "epoch n°729 : train_loss = 1.873439073562622, val_loss = 0.7275879383087158\n",
      "epoch n°730 : train_loss = 1.8826860189437866, val_loss = 0.7233079075813293\n",
      "epoch n°731 : train_loss = 1.8873906135559082, val_loss = 0.727088212966919\n",
      "epoch n°732 : train_loss = 1.870082139968872, val_loss = 0.7233952283859253\n",
      "epoch n°733 : train_loss = 1.8923671245574951, val_loss = 0.725051760673523\n",
      "epoch n°734 : train_loss = 1.8788089752197266, val_loss = 0.7290045619010925\n",
      "epoch n°735 : train_loss = 1.8781229257583618, val_loss = 0.72260582447052\n",
      "epoch n°736 : train_loss = 1.882175326347351, val_loss = 0.7260497808456421\n",
      "epoch n°737 : train_loss = 1.8862284421920776, val_loss = 0.7261404395103455\n",
      "epoch n°738 : train_loss = 1.8759759664535522, val_loss = 0.7231876254081726\n",
      "epoch n°739 : train_loss = 1.865339756011963, val_loss = 0.7318581938743591\n",
      "epoch n°740 : train_loss = 1.8801281452178955, val_loss = 0.7209354043006897\n",
      "epoch n°741 : train_loss = 1.8669285774230957, val_loss = 0.7257679104804993\n",
      "epoch n°742 : train_loss = 1.8763123750686646, val_loss = 0.7276973128318787\n",
      "epoch n°743 : train_loss = 1.879457950592041, val_loss = 0.7273249626159668\n",
      "epoch n°744 : train_loss = 1.8670036792755127, val_loss = 0.726662814617157\n",
      "epoch n°745 : train_loss = 1.8657917976379395, val_loss = 0.7231471538543701\n",
      "epoch n°746 : train_loss = 1.873383641242981, val_loss = 0.7277247905731201\n",
      "epoch n°747 : train_loss = 1.8723373413085938, val_loss = 0.727674126625061\n",
      "epoch n°748 : train_loss = 1.874690294265747, val_loss = 0.7212895154953003\n",
      "epoch n°749 : train_loss = 1.8677978515625, val_loss = 0.7290393114089966\n",
      "epoch n°750 : train_loss = 1.8672454357147217, val_loss = 0.7295777797698975\n",
      "epoch n°751 : train_loss = 1.8668948411941528, val_loss = 0.7237512469291687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°752 : train_loss = 1.8727425336837769, val_loss = 0.7243048548698425\n",
      "epoch n°753 : train_loss = 1.872338891029358, val_loss = 0.7285764813423157\n",
      "epoch n°754 : train_loss = 1.8684622049331665, val_loss = 0.7273467183113098\n",
      "epoch n°755 : train_loss = 1.8647288084030151, val_loss = 0.7222586870193481\n",
      "epoch n°756 : train_loss = 1.8701198101043701, val_loss = 0.7222480773925781\n",
      "epoch n°757 : train_loss = 1.8798828125, val_loss = 0.7266771793365479\n",
      "epoch n°758 : train_loss = 1.8713895082473755, val_loss = 0.7251461148262024\n",
      "epoch n°759 : train_loss = 1.872825026512146, val_loss = 0.7220488786697388\n",
      "epoch n°760 : train_loss = 1.8699181079864502, val_loss = 0.7273523211479187\n",
      "epoch n°761 : train_loss = 1.8660383224487305, val_loss = 0.7243602871894836\n",
      "epoch n°762 : train_loss = 1.8553904294967651, val_loss = 0.7292673587799072\n",
      "epoch n°763 : train_loss = 1.8706218004226685, val_loss = 0.7254104018211365\n",
      "epoch n°764 : train_loss = 1.866614580154419, val_loss = 0.7291359305381775\n",
      "epoch n°765 : train_loss = 1.8756945133209229, val_loss = 0.7235351800918579\n",
      "epoch n°766 : train_loss = 1.8759872913360596, val_loss = 0.7243795394897461\n",
      "epoch n°767 : train_loss = 1.8652831315994263, val_loss = 0.7235689163208008\n",
      "epoch n°768 : train_loss = 1.8676429986953735, val_loss = 0.725299596786499\n",
      "epoch n°769 : train_loss = 1.8559825420379639, val_loss = 0.7225781679153442\n",
      "epoch n°770 : train_loss = 1.8664665222167969, val_loss = 0.7275639772415161\n",
      "epoch n°771 : train_loss = 1.8680168390274048, val_loss = 0.7250628471374512\n",
      "epoch n°772 : train_loss = 1.8647172451019287, val_loss = 0.7208552360534668\n",
      "epoch n°773 : train_loss = 1.8674461841583252, val_loss = 0.7269676923751831\n",
      "epoch n°774 : train_loss = 1.8706036806106567, val_loss = 0.7271348834037781\n",
      "epoch n°775 : train_loss = 1.8755300045013428, val_loss = 0.7302884459495544\n",
      "epoch n°776 : train_loss = 1.8623244762420654, val_loss = 0.7221693992614746\n",
      "epoch n°777 : train_loss = 1.8660472631454468, val_loss = 0.7274052500724792\n",
      "epoch n°778 : train_loss = 1.8784003257751465, val_loss = 0.7216901183128357\n",
      "epoch n°779 : train_loss = 1.869793176651001, val_loss = 0.7293763160705566\n",
      "epoch n°780 : train_loss = 1.8593804836273193, val_loss = 0.7207987904548645\n",
      "epoch n°781 : train_loss = 1.8658639192581177, val_loss = 0.7255488634109497\n",
      "epoch n°782 : train_loss = 1.8596317768096924, val_loss = 0.7216183543205261\n",
      "epoch n°783 : train_loss = 1.8671975135803223, val_loss = 0.7287200689315796\n",
      "epoch n°784 : train_loss = 1.857120394706726, val_loss = 0.729609489440918\n",
      "epoch n°785 : train_loss = 1.855878472328186, val_loss = 0.7240257859230042\n",
      "epoch n°786 : train_loss = 1.8645024299621582, val_loss = 0.7245244383811951\n",
      "epoch n°787 : train_loss = 1.858293056488037, val_loss = 0.7250558137893677\n",
      "epoch n°788 : train_loss = 1.863704800605774, val_loss = 0.7271118760108948\n",
      "epoch n°789 : train_loss = 1.859869122505188, val_loss = 0.7291116714477539\n",
      "epoch n°790 : train_loss = 1.8629449605941772, val_loss = 0.729141891002655\n",
      "epoch n°791 : train_loss = 1.8525947332382202, val_loss = 0.7242103815078735\n",
      "epoch n°792 : train_loss = 1.8647511005401611, val_loss = 0.7265551686286926\n",
      "epoch n°793 : train_loss = 1.8649029731750488, val_loss = 0.7325215935707092\n",
      "epoch n°794 : train_loss = 1.8732389211654663, val_loss = 0.723418116569519\n",
      "epoch n°795 : train_loss = 1.862901210784912, val_loss = 0.7263504266738892\n",
      "epoch n°796 : train_loss = 1.8588645458221436, val_loss = 0.7215707302093506\n",
      "epoch n°797 : train_loss = 1.8627387285232544, val_loss = 0.7289712429046631\n",
      "epoch n°798 : train_loss = 1.8622393608093262, val_loss = 0.7260338664054871\n",
      "epoch n°799 : train_loss = 1.8658697605133057, val_loss = 0.7229926586151123\n",
      "epoch n°800 : train_loss = 1.8714271783828735, val_loss = 0.7250156998634338\n",
      "epoch n°801 : train_loss = 1.855469822883606, val_loss = 0.7272512316703796\n",
      "epoch n°802 : train_loss = 1.8554325103759766, val_loss = 0.7210575938224792\n",
      "epoch n°803 : train_loss = 1.865454912185669, val_loss = 0.727527379989624\n",
      "epoch n°804 : train_loss = 1.8659433126449585, val_loss = 0.7243708372116089\n",
      "epoch n°805 : train_loss = 1.8636996746063232, val_loss = 0.7221890687942505\n",
      "epoch n°806 : train_loss = 1.8655462265014648, val_loss = 0.7248387932777405\n",
      "epoch n°807 : train_loss = 1.8608990907669067, val_loss = 0.7227302193641663\n",
      "epoch n°808 : train_loss = 1.858603835105896, val_loss = 0.7245576977729797\n",
      "epoch n°809 : train_loss = 1.8635209798812866, val_loss = 0.7249667644500732\n",
      "epoch n°810 : train_loss = 1.8662846088409424, val_loss = 0.7258964776992798\n",
      "epoch n°811 : train_loss = 1.8528664112091064, val_loss = 0.7248948216438293\n",
      "epoch n°812 : train_loss = 1.8590666055679321, val_loss = 0.7275069355964661\n",
      "epoch n°813 : train_loss = 1.8649029731750488, val_loss = 0.723348081111908\n",
      "epoch n°814 : train_loss = 1.8563084602355957, val_loss = 0.7232300639152527\n",
      "epoch n°815 : train_loss = 1.8629332780838013, val_loss = 0.7247243523597717\n",
      "epoch n°816 : train_loss = 1.8544459342956543, val_loss = 0.7294384241104126\n",
      "epoch n°817 : train_loss = 1.856473445892334, val_loss = 0.7216918468475342\n",
      "epoch n°818 : train_loss = 1.858268141746521, val_loss = 0.7258999943733215\n",
      "epoch n°819 : train_loss = 1.8658277988433838, val_loss = 0.7226176857948303\n",
      "epoch n°820 : train_loss = 1.8460325002670288, val_loss = 0.7207847237586975\n",
      "epoch n°821 : train_loss = 1.8574069738388062, val_loss = 0.7227479815483093\n",
      "epoch n°822 : train_loss = 1.8562952280044556, val_loss = 0.7262170910835266\n",
      "epoch n°823 : train_loss = 1.846782922744751, val_loss = 0.7281227707862854\n",
      "epoch n°824 : train_loss = 1.8534795045852661, val_loss = 0.7240236401557922\n",
      "epoch n°825 : train_loss = 1.8566839694976807, val_loss = 0.7232882380485535\n",
      "epoch n°826 : train_loss = 1.8576076030731201, val_loss = 0.7212119102478027\n",
      "epoch n°827 : train_loss = 1.8605320453643799, val_loss = 0.7284114956855774\n",
      "epoch n°828 : train_loss = 1.8548365831375122, val_loss = 0.7296586036682129\n",
      "epoch n°829 : train_loss = 1.859482765197754, val_loss = 0.7267937064170837\n",
      "epoch n°830 : train_loss = 1.854387640953064, val_loss = 0.7241745591163635\n",
      "epoch n°831 : train_loss = 1.8592661619186401, val_loss = 0.7297431230545044\n",
      "epoch n°832 : train_loss = 1.8485190868377686, val_loss = 0.7256055474281311\n",
      "epoch n°833 : train_loss = 1.854359745979309, val_loss = 0.7215111255645752\n",
      "epoch n°834 : train_loss = 1.8661797046661377, val_loss = 0.7236821055412292\n",
      "epoch n°835 : train_loss = 1.850484013557434, val_loss = 0.7295093536376953\n",
      "epoch n°836 : train_loss = 1.857331395149231, val_loss = 0.7207056283950806\n",
      "epoch n°837 : train_loss = 1.8551437854766846, val_loss = 0.7305639386177063\n",
      "epoch n°838 : train_loss = 1.8442126512527466, val_loss = 0.7282770276069641\n",
      "epoch n°839 : train_loss = 1.8518635034561157, val_loss = 0.7269450426101685\n",
      "epoch n°840 : train_loss = 1.8525912761688232, val_loss = 0.7217097878456116\n",
      "epoch n°841 : train_loss = 1.854746699333191, val_loss = 0.7226255536079407\n",
      "epoch n°842 : train_loss = 1.8562748432159424, val_loss = 0.7282779812812805\n",
      "epoch n°843 : train_loss = 1.8521652221679688, val_loss = 0.7280524969100952\n",
      "epoch n°844 : train_loss = 1.8477140665054321, val_loss = 0.7233512997627258\n",
      "epoch n°845 : train_loss = 1.8462156057357788, val_loss = 0.7236175537109375\n",
      "epoch n°846 : train_loss = 1.8574926853179932, val_loss = 0.7285835146903992\n",
      "epoch n°847 : train_loss = 1.8580515384674072, val_loss = 0.7237151861190796\n",
      "epoch n°848 : train_loss = 1.8534423112869263, val_loss = 0.7279236912727356\n",
      "epoch n°849 : train_loss = 1.8515247106552124, val_loss = 0.7218530774116516\n",
      "epoch n°850 : train_loss = 1.8558363914489746, val_loss = 0.7216381430625916\n",
      "epoch n°851 : train_loss = 1.8538844585418701, val_loss = 0.7278055548667908\n",
      "epoch n°852 : train_loss = 1.858970046043396, val_loss = 0.7227084636688232\n",
      "epoch n°853 : train_loss = 1.8551908731460571, val_loss = 0.7209182381629944\n",
      "epoch n°854 : train_loss = 1.8602724075317383, val_loss = 0.7290545701980591\n",
      "epoch n°855 : train_loss = 1.8351539373397827, val_loss = 0.7265751957893372\n",
      "epoch n°856 : train_loss = 1.853579044342041, val_loss = 0.7253278493881226\n",
      "epoch n°857 : train_loss = 1.8597123622894287, val_loss = 0.7204068303108215\n",
      "epoch n°858 : train_loss = 1.8671917915344238, val_loss = 0.7249453663825989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°859 : train_loss = 1.8528168201446533, val_loss = 0.7253769636154175\n",
      "epoch n°860 : train_loss = 1.8455631732940674, val_loss = 0.7217516303062439\n",
      "epoch n°861 : train_loss = 1.8412126302719116, val_loss = 0.7247738838195801\n",
      "epoch n°862 : train_loss = 1.850693702697754, val_loss = 0.7241203188896179\n",
      "epoch n°863 : train_loss = 1.8388913869857788, val_loss = 0.7211124300956726\n",
      "epoch n°864 : train_loss = 1.8468607664108276, val_loss = 0.7205847501754761\n",
      "epoch n°865 : train_loss = 1.8489896059036255, val_loss = 0.7269605398178101\n",
      "epoch n°866 : train_loss = 1.846357822418213, val_loss = 0.7205340266227722\n",
      "epoch n°867 : train_loss = 1.8608274459838867, val_loss = 0.7230575084686279\n",
      "epoch n°868 : train_loss = 1.8380436897277832, val_loss = 0.7223275303840637\n",
      "epoch n°869 : train_loss = 1.8602113723754883, val_loss = 0.7261210680007935\n",
      "epoch n°870 : train_loss = 1.8395957946777344, val_loss = 0.7210292816162109\n",
      "epoch n°871 : train_loss = 1.8423376083374023, val_loss = 0.7265687584877014\n",
      "epoch n°872 : train_loss = 1.8632009029388428, val_loss = 0.7249196171760559\n",
      "epoch n°873 : train_loss = 1.8533196449279785, val_loss = 0.7308921813964844\n",
      "epoch n°874 : train_loss = 1.8456698656082153, val_loss = 0.7259842753410339\n",
      "epoch n°875 : train_loss = 1.851737380027771, val_loss = 0.7197069525718689\n",
      "epoch n°876 : train_loss = 1.8503338098526, val_loss = 0.7171834707260132\n",
      "epoch n°877 : train_loss = 1.8534770011901855, val_loss = 0.7248058915138245\n",
      "epoch n°878 : train_loss = 1.844085454940796, val_loss = 0.7248502373695374\n",
      "epoch n°879 : train_loss = 1.8543933629989624, val_loss = 0.7228003740310669\n",
      "epoch n°880 : train_loss = 1.8473691940307617, val_loss = 0.7258636355400085\n",
      "epoch n°881 : train_loss = 1.850816249847412, val_loss = 0.7272619605064392\n",
      "epoch n°882 : train_loss = 1.8529300689697266, val_loss = 0.7264282703399658\n",
      "epoch n°883 : train_loss = 1.8419979810714722, val_loss = 0.7321797013282776\n",
      "epoch n°884 : train_loss = 1.847105860710144, val_loss = 0.7247507572174072\n",
      "epoch n°885 : train_loss = 1.8492168188095093, val_loss = 0.7241822481155396\n",
      "epoch n°886 : train_loss = 1.8412158489227295, val_loss = 0.723926842212677\n",
      "epoch n°887 : train_loss = 1.8442614078521729, val_loss = 0.7246792316436768\n",
      "epoch n°888 : train_loss = 1.8581194877624512, val_loss = 0.7213647365570068\n",
      "epoch n°889 : train_loss = 1.8465555906295776, val_loss = 0.722868800163269\n",
      "epoch n°890 : train_loss = 1.8528932332992554, val_loss = 0.724604070186615\n",
      "epoch n°891 : train_loss = 1.8460330963134766, val_loss = 0.7236747145652771\n",
      "epoch n°892 : train_loss = 1.8430914878845215, val_loss = 0.7255952954292297\n",
      "epoch n°893 : train_loss = 1.8537001609802246, val_loss = 0.7207083702087402\n",
      "epoch n°894 : train_loss = 1.856080174446106, val_loss = 0.7239068150520325\n",
      "epoch n°895 : train_loss = 1.8569570779800415, val_loss = 0.719394326210022\n",
      "epoch n°896 : train_loss = 1.8536349534988403, val_loss = 0.7220815420150757\n",
      "epoch n°897 : train_loss = 1.8418337106704712, val_loss = 0.7213158011436462\n",
      "epoch n°898 : train_loss = 1.8481335639953613, val_loss = 0.7247969508171082\n",
      "epoch n°899 : train_loss = 1.84531569480896, val_loss = 0.720832884311676\n",
      "epoch n°900 : train_loss = 1.847668170928955, val_loss = 0.7185960412025452\n",
      "epoch n°901 : train_loss = 1.846323013305664, val_loss = 0.7207302451133728\n",
      "epoch n°902 : train_loss = 1.8501313924789429, val_loss = 0.7220536470413208\n",
      "epoch n°903 : train_loss = 1.8437997102737427, val_loss = 0.7226637005805969\n",
      "epoch n°904 : train_loss = 1.8449827432632446, val_loss = 0.7227912545204163\n",
      "epoch n°905 : train_loss = 1.852707862854004, val_loss = 0.7242428064346313\n",
      "epoch n°906 : train_loss = 1.8500415086746216, val_loss = 0.7213811278343201\n",
      "epoch n°907 : train_loss = 1.8404840230941772, val_loss = 0.7220926284790039\n",
      "epoch n°908 : train_loss = 1.840335726737976, val_loss = 0.7234851121902466\n",
      "epoch n°909 : train_loss = 1.8599998950958252, val_loss = 0.720160722732544\n",
      "epoch n°910 : train_loss = 1.8509471416473389, val_loss = 0.7218191623687744\n",
      "epoch n°911 : train_loss = 1.844651699066162, val_loss = 0.7253420948982239\n",
      "epoch n°912 : train_loss = 1.8362526893615723, val_loss = 0.7246951460838318\n",
      "epoch n°913 : train_loss = 1.8459293842315674, val_loss = 0.7280862331390381\n",
      "epoch n°914 : train_loss = 1.851110816001892, val_loss = 0.7207741141319275\n",
      "epoch n°915 : train_loss = 1.8415838479995728, val_loss = 0.722377598285675\n",
      "epoch n°916 : train_loss = 1.8398773670196533, val_loss = 0.7246041893959045\n",
      "epoch n°917 : train_loss = 1.8462517261505127, val_loss = 0.7201394438743591\n",
      "epoch n°918 : train_loss = 1.8400719165802002, val_loss = 0.7270466089248657\n",
      "epoch n°919 : train_loss = 1.846021294593811, val_loss = 0.7234878540039062\n",
      "epoch n°920 : train_loss = 1.8408867120742798, val_loss = 0.7230531573295593\n",
      "epoch n°921 : train_loss = 1.8417521715164185, val_loss = 0.7217161059379578\n",
      "epoch n°922 : train_loss = 1.8425922393798828, val_loss = 0.7234832644462585\n",
      "epoch n°923 : train_loss = 1.8391728401184082, val_loss = 0.7215715646743774\n",
      "epoch n°924 : train_loss = 1.844934344291687, val_loss = 0.72447270154953\n",
      "epoch n°925 : train_loss = 1.8424334526062012, val_loss = 0.7206878066062927\n",
      "epoch n°926 : train_loss = 1.8412915468215942, val_loss = 0.7276800870895386\n",
      "epoch n°927 : train_loss = 1.8441356420516968, val_loss = 0.7238494157791138\n",
      "epoch n°928 : train_loss = 1.8442414999008179, val_loss = 0.7235289812088013\n",
      "epoch n°929 : train_loss = 1.8480902910232544, val_loss = 0.7239229679107666\n",
      "epoch n°930 : train_loss = 1.8493012189865112, val_loss = 0.7224869728088379\n",
      "epoch n°931 : train_loss = 1.844115138053894, val_loss = 0.725857675075531\n",
      "epoch n°932 : train_loss = 1.842420220375061, val_loss = 0.7202687859535217\n",
      "epoch n°933 : train_loss = 1.8474986553192139, val_loss = 0.72208172082901\n",
      "epoch n°934 : train_loss = 1.8524442911148071, val_loss = 0.7241076231002808\n",
      "epoch n°935 : train_loss = 1.8456028699874878, val_loss = 0.721225917339325\n",
      "epoch n°936 : train_loss = 1.8485844135284424, val_loss = 0.7202108502388\n",
      "epoch n°937 : train_loss = 1.8459917306900024, val_loss = 0.7244155406951904\n",
      "epoch n°938 : train_loss = 1.8403459787368774, val_loss = 0.7206541895866394\n",
      "epoch n°939 : train_loss = 1.8444924354553223, val_loss = 0.7255635857582092\n",
      "epoch n°940 : train_loss = 1.8377503156661987, val_loss = 0.7259231209754944\n",
      "epoch n°941 : train_loss = 1.8478012084960938, val_loss = 0.7298492193222046\n",
      "epoch n°942 : train_loss = 1.847719669342041, val_loss = 0.7198668122291565\n",
      "epoch n°943 : train_loss = 1.8418210744857788, val_loss = 0.7207531929016113\n",
      "epoch n°944 : train_loss = 1.853317379951477, val_loss = 0.7234708070755005\n",
      "epoch n°945 : train_loss = 1.8436603546142578, val_loss = 0.7205317616462708\n",
      "epoch n°946 : train_loss = 1.842244267463684, val_loss = 0.7259765863418579\n",
      "epoch n°947 : train_loss = 1.8458722829818726, val_loss = 0.7246944904327393\n",
      "epoch n°948 : train_loss = 1.8431650400161743, val_loss = 0.719893753528595\n",
      "epoch n°949 : train_loss = 1.843933343887329, val_loss = 0.7197364568710327\n",
      "epoch n°950 : train_loss = 1.8447993993759155, val_loss = 0.7218705415725708\n",
      "epoch n°951 : train_loss = 1.842511773109436, val_loss = 0.7248812913894653\n",
      "epoch n°952 : train_loss = 1.8291072845458984, val_loss = 0.7213453054428101\n",
      "epoch n°953 : train_loss = 1.8488088846206665, val_loss = 0.7283400297164917\n",
      "epoch n°954 : train_loss = 1.8311737775802612, val_loss = 0.72248774766922\n",
      "epoch n°955 : train_loss = 1.8533623218536377, val_loss = 0.7271880507469177\n",
      "epoch n°956 : train_loss = 1.8480985164642334, val_loss = 0.7222572565078735\n",
      "epoch n°957 : train_loss = 1.8420292139053345, val_loss = 0.7253881096839905\n",
      "epoch n°958 : train_loss = 1.8322722911834717, val_loss = 0.7262170314788818\n",
      "epoch n°959 : train_loss = 1.8349478244781494, val_loss = 0.722458004951477\n",
      "epoch n°960 : train_loss = 1.8501728773117065, val_loss = 0.720065176486969\n",
      "epoch n°961 : train_loss = 1.8473790884017944, val_loss = 0.7244080305099487\n",
      "epoch n°962 : train_loss = 1.8450417518615723, val_loss = 0.723324716091156\n",
      "epoch n°963 : train_loss = 1.8433784246444702, val_loss = 0.720823347568512\n",
      "epoch n°964 : train_loss = 1.853054165840149, val_loss = 0.7248623967170715\n",
      "epoch n°965 : train_loss = 1.8603193759918213, val_loss = 0.7232831716537476\n",
      "epoch n°966 : train_loss = 1.8350332975387573, val_loss = 0.7247166037559509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°967 : train_loss = 1.8465360403060913, val_loss = 0.7237498760223389\n",
      "epoch n°968 : train_loss = 1.8465023040771484, val_loss = 0.7276091575622559\n",
      "epoch n°969 : train_loss = 1.845519781112671, val_loss = 0.7221574783325195\n",
      "epoch n°970 : train_loss = 1.8424948453903198, val_loss = 0.7244138717651367\n",
      "epoch n°971 : train_loss = 1.8397303819656372, val_loss = 0.7207720875740051\n",
      "epoch n°972 : train_loss = 1.8483219146728516, val_loss = 0.7260168790817261\n",
      "epoch n°973 : train_loss = 1.8536959886550903, val_loss = 0.7256425619125366\n",
      "epoch n°974 : train_loss = 1.8364607095718384, val_loss = 0.722160279750824\n",
      "epoch n°975 : train_loss = 1.8411505222320557, val_loss = 0.7235482335090637\n",
      "epoch n°976 : train_loss = 1.842783808708191, val_loss = 0.7254703640937805\n",
      "epoch n°977 : train_loss = 1.849898338317871, val_loss = 0.723810613155365\n",
      "epoch n°978 : train_loss = 1.8428306579589844, val_loss = 0.7288322448730469\n",
      "epoch n°979 : train_loss = 1.8281071186065674, val_loss = 0.7241532206535339\n",
      "epoch n°980 : train_loss = 1.8449063301086426, val_loss = 0.7252719402313232\n",
      "epoch n°981 : train_loss = 1.8478916883468628, val_loss = 0.7194394469261169\n",
      "epoch n°982 : train_loss = 1.8407940864562988, val_loss = 0.7225314974784851\n",
      "epoch n°983 : train_loss = 1.8430818319320679, val_loss = 0.7219516634941101\n",
      "epoch n°984 : train_loss = 1.8505778312683105, val_loss = 0.7287480235099792\n",
      "epoch n°985 : train_loss = 1.8467280864715576, val_loss = 0.7275100946426392\n",
      "epoch n°986 : train_loss = 1.8434422016143799, val_loss = 0.7253270149230957\n",
      "epoch n°987 : train_loss = 1.8429951667785645, val_loss = 0.7228303551673889\n",
      "epoch n°988 : train_loss = 1.840259075164795, val_loss = 0.7199515104293823\n",
      "epoch n°989 : train_loss = 1.8432997465133667, val_loss = 0.720607340335846\n",
      "epoch n°990 : train_loss = 1.8484843969345093, val_loss = 0.7217129468917847\n",
      "epoch n°991 : train_loss = 1.8369090557098389, val_loss = 0.725346565246582\n",
      "epoch n°992 : train_loss = 1.8408122062683105, val_loss = 0.7192290425300598\n",
      "epoch n°993 : train_loss = 1.8540204763412476, val_loss = 0.7208928465843201\n",
      "epoch n°994 : train_loss = 1.8388454914093018, val_loss = 0.7260233759880066\n",
      "epoch n°995 : train_loss = 1.837111473083496, val_loss = 0.7273716330528259\n",
      "epoch n°996 : train_loss = 1.8448230028152466, val_loss = 0.7253888249397278\n",
      "epoch n°997 : train_loss = 1.8382971286773682, val_loss = 0.7203103303909302\n",
      "epoch n°998 : train_loss = 1.8407607078552246, val_loss = 0.7204550504684448\n",
      "epoch n°999 : train_loss = 1.8401730060577393, val_loss = 0.7247141003608704\n",
      "epoch n°1000 : train_loss = 1.8484632968902588, val_loss = 0.7236573696136475\n",
      "epoch n°1001 : train_loss = 1.8442418575286865, val_loss = 0.7223422527313232\n",
      "epoch n°1002 : train_loss = 1.8415403366088867, val_loss = 0.7231197357177734\n",
      "epoch n°1003 : train_loss = 1.844725489616394, val_loss = 0.7272740602493286\n",
      "epoch n°1004 : train_loss = 1.8269858360290527, val_loss = 0.7231639623641968\n",
      "epoch n°1005 : train_loss = 1.8516534566879272, val_loss = 0.7227274775505066\n",
      "epoch n°1006 : train_loss = 1.850790023803711, val_loss = 0.722541332244873\n",
      "epoch n°1007 : train_loss = 1.8479485511779785, val_loss = 0.7276971936225891\n",
      "epoch n°1008 : train_loss = 1.8743356466293335, val_loss = 0.7373661994934082\n",
      "epoch n°1009 : train_loss = 1.8732459545135498, val_loss = 0.7279019355773926\n",
      "epoch n°1010 : train_loss = 1.8720436096191406, val_loss = 0.7344838380813599\n",
      "epoch n°1011 : train_loss = 1.8899168968200684, val_loss = 0.722550630569458\n",
      "epoch n°1012 : train_loss = 1.8778574466705322, val_loss = 0.727439284324646\n",
      "epoch n°1013 : train_loss = 1.8664742708206177, val_loss = 0.7295712232589722\n",
      "epoch n°1014 : train_loss = 1.872206211090088, val_loss = 0.7269071340560913\n",
      "epoch n°1015 : train_loss = 1.8767369985580444, val_loss = 0.7281764149665833\n",
      "epoch n°1016 : train_loss = 1.8724652528762817, val_loss = 0.7302866578102112\n",
      "epoch n°1017 : train_loss = 1.882563591003418, val_loss = 0.7262084484100342\n",
      "epoch n°1018 : train_loss = 1.872979760169983, val_loss = 0.7283928990364075\n",
      "epoch n°1019 : train_loss = 1.8832303285598755, val_loss = 0.7272188067436218\n",
      "epoch n°1020 : train_loss = 1.8775215148925781, val_loss = 0.7314938902854919\n",
      "epoch n°1021 : train_loss = 1.8750228881835938, val_loss = 0.7325536012649536\n",
      "epoch n°1022 : train_loss = 1.8748300075531006, val_loss = 0.731255054473877\n",
      "epoch n°1023 : train_loss = 1.8820253610610962, val_loss = 0.7260557413101196\n",
      "epoch n°1024 : train_loss = 1.8671823740005493, val_loss = 0.7313980460166931\n",
      "epoch n°1025 : train_loss = 1.8816150426864624, val_loss = 0.7318416237831116\n",
      "epoch n°1026 : train_loss = 1.8669652938842773, val_loss = 0.7219691872596741\n",
      "epoch n°1027 : train_loss = 1.8699719905853271, val_loss = 0.7249906063079834\n",
      "epoch n°1028 : train_loss = 1.8828792572021484, val_loss = 0.7243894934654236\n",
      "epoch n°1029 : train_loss = 1.87904953956604, val_loss = 0.729366660118103\n",
      "epoch n°1030 : train_loss = 1.8768212795257568, val_loss = 0.72605299949646\n",
      "epoch n°1031 : train_loss = 1.8803738355636597, val_loss = 0.727834939956665\n",
      "epoch n°1032 : train_loss = 1.8787251710891724, val_loss = 0.7255567312240601\n",
      "epoch n°1033 : train_loss = 1.8775174617767334, val_loss = 0.72891765832901\n",
      "epoch n°1034 : train_loss = 1.8707449436187744, val_loss = 0.7298088073730469\n",
      "epoch n°1035 : train_loss = 1.8759846687316895, val_loss = 0.7253351807594299\n",
      "epoch n°1036 : train_loss = 1.8819061517715454, val_loss = 0.7283647656440735\n",
      "epoch n°1037 : train_loss = 1.8769512176513672, val_loss = 0.7235897779464722\n",
      "epoch n°1038 : train_loss = 1.8720194101333618, val_loss = 0.7315601110458374\n",
      "epoch n°1039 : train_loss = 1.8789666891098022, val_loss = 0.7247107028961182\n",
      "epoch n°1040 : train_loss = 1.8839935064315796, val_loss = 0.7261015772819519\n",
      "epoch n°1041 : train_loss = 1.8830926418304443, val_loss = 0.7272682189941406\n",
      "epoch n°1042 : train_loss = 1.871996283531189, val_loss = 0.7259666323661804\n",
      "epoch n°1043 : train_loss = 1.8774913549423218, val_loss = 0.7303037643432617\n",
      "epoch n°1044 : train_loss = 1.8793913125991821, val_loss = 0.7279924154281616\n",
      "epoch n°1045 : train_loss = 1.8864262104034424, val_loss = 0.723871648311615\n",
      "epoch n°1046 : train_loss = 1.8741869926452637, val_loss = 0.7253835201263428\n",
      "epoch n°1047 : train_loss = 1.8811233043670654, val_loss = 0.7304729223251343\n",
      "epoch n°1048 : train_loss = 1.883126974105835, val_loss = 0.7259429693222046\n",
      "epoch n°1049 : train_loss = 1.8831861019134521, val_loss = 0.731840968132019\n",
      "epoch n°1050 : train_loss = 1.8745585680007935, val_loss = 0.7223974466323853\n",
      "epoch n°1051 : train_loss = 1.8753925561904907, val_loss = 0.7271151542663574\n",
      "epoch n°1052 : train_loss = 1.8728057146072388, val_loss = 0.7238425016403198\n",
      "epoch n°1053 : train_loss = 1.8782693147659302, val_loss = 0.7265735268592834\n",
      "epoch n°1054 : train_loss = 1.8748606443405151, val_loss = 0.7304074168205261\n",
      "epoch n°1055 : train_loss = 1.8795469999313354, val_loss = 0.7270231246948242\n",
      "epoch n°1056 : train_loss = 1.8762445449829102, val_loss = 0.7286799550056458\n",
      "epoch n°1057 : train_loss = 1.874517798423767, val_loss = 0.7268983721733093\n",
      "epoch n°1058 : train_loss = 1.8823561668395996, val_loss = 0.7304734587669373\n",
      "epoch n°1059 : train_loss = 1.8928498029708862, val_loss = 0.7306916117668152\n",
      "epoch n°1060 : train_loss = 1.872380256652832, val_loss = 0.7265233397483826\n",
      "epoch n°1061 : train_loss = 1.879521131515503, val_loss = 0.7266468405723572\n",
      "epoch n°1062 : train_loss = 1.8745203018188477, val_loss = 0.7244914770126343\n",
      "epoch n°1063 : train_loss = 1.871067762374878, val_loss = 0.7263376712799072\n",
      "epoch n°1064 : train_loss = 1.8783390522003174, val_loss = 0.7217941284179688\n",
      "epoch n°1065 : train_loss = 1.8731402158737183, val_loss = 0.7259048223495483\n",
      "epoch n°1066 : train_loss = 1.87629234790802, val_loss = 0.7264898419380188\n",
      "epoch n°1067 : train_loss = 1.8847213983535767, val_loss = 0.7268498539924622\n",
      "epoch n°1068 : train_loss = 1.8787180185317993, val_loss = 0.7264434099197388\n",
      "epoch n°1069 : train_loss = 1.8845300674438477, val_loss = 0.7278027534484863\n",
      "epoch n°1070 : train_loss = 1.871896505355835, val_loss = 0.7260021567344666\n",
      "epoch n°1071 : train_loss = 1.8773081302642822, val_loss = 0.7277276515960693\n",
      "epoch n°1072 : train_loss = 1.8833743333816528, val_loss = 0.7214064598083496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1073 : train_loss = 1.8782309293746948, val_loss = 0.7261980772018433\n",
      "epoch n°1074 : train_loss = 1.8916677236557007, val_loss = 0.7276089787483215\n",
      "epoch n°1075 : train_loss = 1.87882661819458, val_loss = 0.7304039597511292\n",
      "epoch n°1076 : train_loss = 1.8887851238250732, val_loss = 0.7284289002418518\n",
      "epoch n°1077 : train_loss = 1.8767462968826294, val_loss = 0.7297254800796509\n",
      "epoch n°1078 : train_loss = 1.871459722518921, val_loss = 0.7314000725746155\n",
      "epoch n°1079 : train_loss = 1.8870158195495605, val_loss = 0.726231575012207\n",
      "epoch n°1080 : train_loss = 1.8767006397247314, val_loss = 0.7274027466773987\n",
      "epoch n°1081 : train_loss = 1.8721745014190674, val_loss = 0.7259824872016907\n",
      "epoch n°1082 : train_loss = 1.8844789266586304, val_loss = 0.7274802923202515\n",
      "epoch n°1083 : train_loss = 1.8715949058532715, val_loss = 0.7240707278251648\n",
      "epoch n°1084 : train_loss = 1.8663355112075806, val_loss = 0.7261125445365906\n",
      "epoch n°1085 : train_loss = 1.8915212154388428, val_loss = 0.7308651804924011\n",
      "epoch n°1086 : train_loss = 1.8719470500946045, val_loss = 0.7319563031196594\n",
      "epoch n°1087 : train_loss = 1.8719263076782227, val_loss = 0.7245524525642395\n",
      "epoch n°1088 : train_loss = 1.8801319599151611, val_loss = 0.7259476184844971\n",
      "epoch n°1089 : train_loss = 1.8830273151397705, val_loss = 0.7269030809402466\n",
      "epoch n°1090 : train_loss = 1.8972411155700684, val_loss = 0.7291848659515381\n",
      "epoch n°1091 : train_loss = 1.8732891082763672, val_loss = 0.7265196442604065\n",
      "epoch n°1092 : train_loss = 1.877961277961731, val_loss = 0.7231524586677551\n",
      "epoch n°1093 : train_loss = 1.8867791891098022, val_loss = 0.7300589680671692\n",
      "epoch n°1094 : train_loss = 1.8737906217575073, val_loss = 0.7287233471870422\n",
      "epoch n°1095 : train_loss = 1.8806064128875732, val_loss = 0.7246701717376709\n",
      "epoch n°1096 : train_loss = 1.8756133317947388, val_loss = 0.7275863885879517\n",
      "epoch n°1097 : train_loss = 1.8735991716384888, val_loss = 0.7265447974205017\n",
      "epoch n°1098 : train_loss = 1.8747180700302124, val_loss = 0.7274940609931946\n",
      "epoch n°1099 : train_loss = 1.8789311647415161, val_loss = 0.7238114476203918\n",
      "epoch n°1100 : train_loss = 1.8757915496826172, val_loss = 0.7275480628013611\n",
      "epoch n°1101 : train_loss = 1.878704309463501, val_loss = 0.7252135276794434\n",
      "epoch n°1102 : train_loss = 1.8847585916519165, val_loss = 0.7328152060508728\n",
      "epoch n°1103 : train_loss = 1.8833662271499634, val_loss = 0.7265520095825195\n",
      "epoch n°1104 : train_loss = 1.8813855648040771, val_loss = 0.7304279208183289\n",
      "epoch n°1105 : train_loss = 1.8708767890930176, val_loss = 0.7274559736251831\n",
      "epoch n°1106 : train_loss = 1.872211217880249, val_loss = 0.7312845587730408\n",
      "epoch n°1107 : train_loss = 1.8753725290298462, val_loss = 0.725368082523346\n",
      "epoch n°1108 : train_loss = 1.886375069618225, val_loss = 0.7261546850204468\n",
      "epoch n°1109 : train_loss = 1.8779934644699097, val_loss = 0.7264049649238586\n",
      "epoch n°1110 : train_loss = 1.8839763402938843, val_loss = 0.7237127423286438\n",
      "epoch n°1111 : train_loss = 1.869145154953003, val_loss = 0.7232587337493896\n",
      "epoch n°1112 : train_loss = 1.8746538162231445, val_loss = 0.7294999361038208\n",
      "epoch n°1113 : train_loss = 1.870038628578186, val_loss = 0.7277095913887024\n",
      "epoch n°1114 : train_loss = 1.8896665573120117, val_loss = 0.7294369339942932\n",
      "epoch n°1115 : train_loss = 1.869521141052246, val_loss = 0.7306212782859802\n",
      "epoch n°1116 : train_loss = 1.8717114925384521, val_loss = 0.7229797840118408\n",
      "epoch n°1117 : train_loss = 1.8784246444702148, val_loss = 0.721834123134613\n",
      "epoch n°1118 : train_loss = 1.8785679340362549, val_loss = 0.7265161871910095\n",
      "epoch n°1119 : train_loss = 1.87607741355896, val_loss = 0.7262007594108582\n",
      "epoch n°1120 : train_loss = 1.880182147026062, val_loss = 0.7254506945610046\n",
      "epoch n°1121 : train_loss = 1.862870216369629, val_loss = 0.7267302870750427\n",
      "epoch n°1122 : train_loss = 1.8693690299987793, val_loss = 0.7235457301139832\n",
      "epoch n°1123 : train_loss = 1.8788572549819946, val_loss = 0.7241385579109192\n",
      "epoch n°1124 : train_loss = 1.87590491771698, val_loss = 0.7315464019775391\n",
      "epoch n°1125 : train_loss = 1.8660295009613037, val_loss = 0.7242799401283264\n",
      "epoch n°1126 : train_loss = 1.8661543130874634, val_loss = 0.7230086922645569\n",
      "epoch n°1127 : train_loss = 1.8719780445098877, val_loss = 0.7305542826652527\n",
      "epoch n°1128 : train_loss = 1.862011432647705, val_loss = 0.7320772409439087\n",
      "epoch n°1129 : train_loss = 1.8638490438461304, val_loss = 0.7265222072601318\n",
      "epoch n°1130 : train_loss = 1.8693528175354004, val_loss = 0.7280310988426208\n",
      "epoch n°1131 : train_loss = 1.8805266618728638, val_loss = 0.7242093086242676\n",
      "epoch n°1132 : train_loss = 1.8770947456359863, val_loss = 0.7276804447174072\n",
      "epoch n°1133 : train_loss = 1.8793507814407349, val_loss = 0.7311580181121826\n",
      "epoch n°1134 : train_loss = 1.8700183629989624, val_loss = 0.7273565530776978\n",
      "epoch n°1135 : train_loss = 1.8761248588562012, val_loss = 0.7274048924446106\n",
      "epoch n°1136 : train_loss = 1.87060546875, val_loss = 0.7276962399482727\n",
      "epoch n°1137 : train_loss = 1.8783283233642578, val_loss = 0.7261120080947876\n",
      "epoch n°1138 : train_loss = 1.8759745359420776, val_loss = 0.7283433675765991\n",
      "epoch n°1139 : train_loss = 1.8768532276153564, val_loss = 0.7270042896270752\n",
      "epoch n°1140 : train_loss = 1.8782880306243896, val_loss = 0.7264348864555359\n",
      "epoch n°1141 : train_loss = 1.8754218816757202, val_loss = 0.7233640551567078\n",
      "epoch n°1142 : train_loss = 1.8769584894180298, val_loss = 0.7318773865699768\n",
      "epoch n°1143 : train_loss = 1.8788443803787231, val_loss = 0.7237428426742554\n",
      "epoch n°1144 : train_loss = 1.857354760169983, val_loss = 0.7255301475524902\n",
      "epoch n°1145 : train_loss = 1.8800772428512573, val_loss = 0.724967896938324\n",
      "epoch n°1146 : train_loss = 1.8783349990844727, val_loss = 0.7283217906951904\n",
      "epoch n°1147 : train_loss = 1.8758972883224487, val_loss = 0.7235307693481445\n",
      "epoch n°1148 : train_loss = 1.8781962394714355, val_loss = 0.7372483611106873\n",
      "epoch n°1149 : train_loss = 1.869846224784851, val_loss = 0.7290159463882446\n",
      "epoch n°1150 : train_loss = 1.8795359134674072, val_loss = 0.7286102175712585\n",
      "epoch n°1151 : train_loss = 1.8750760555267334, val_loss = 0.723754346370697\n",
      "epoch n°1152 : train_loss = 1.8744302988052368, val_loss = 0.727841317653656\n",
      "epoch n°1153 : train_loss = 1.8697192668914795, val_loss = 0.7258777022361755\n",
      "epoch n°1154 : train_loss = 1.8735377788543701, val_loss = 0.7274019718170166\n",
      "epoch n°1155 : train_loss = 1.8832168579101562, val_loss = 0.7290136218070984\n",
      "epoch n°1156 : train_loss = 1.8760128021240234, val_loss = 0.7259769439697266\n",
      "epoch n°1157 : train_loss = 1.8739503622055054, val_loss = 0.7248804569244385\n",
      "epoch n°1158 : train_loss = 1.878710150718689, val_loss = 0.7349352836608887\n",
      "epoch n°1159 : train_loss = 1.8659124374389648, val_loss = 0.7314183115959167\n",
      "epoch n°1160 : train_loss = 1.8725717067718506, val_loss = 0.7329491376876831\n",
      "epoch n°1161 : train_loss = 1.8726873397827148, val_loss = 0.7247927784919739\n",
      "epoch n°1162 : train_loss = 1.8690698146820068, val_loss = 0.728240430355072\n",
      "epoch n°1163 : train_loss = 1.8743008375167847, val_loss = 0.7235152721405029\n",
      "epoch n°1164 : train_loss = 1.8616496324539185, val_loss = 0.7247354984283447\n",
      "epoch n°1165 : train_loss = 1.8728679418563843, val_loss = 0.7238268852233887\n",
      "epoch n°1166 : train_loss = 1.8731062412261963, val_loss = 0.7277848124504089\n",
      "epoch n°1167 : train_loss = 1.8684289455413818, val_loss = 0.7221863269805908\n",
      "epoch n°1168 : train_loss = 1.875700831413269, val_loss = 0.7291375398635864\n",
      "epoch n°1169 : train_loss = 1.8724427223205566, val_loss = 0.7252442240715027\n",
      "epoch n°1170 : train_loss = 1.8680944442749023, val_loss = 0.7253337502479553\n",
      "epoch n°1171 : train_loss = 1.8614376783370972, val_loss = 0.7250702977180481\n",
      "epoch n°1172 : train_loss = 1.8743972778320312, val_loss = 0.7309486269950867\n",
      "epoch n°1173 : train_loss = 1.867421269416809, val_loss = 0.7249572277069092\n",
      "epoch n°1174 : train_loss = 1.8727134466171265, val_loss = 0.7274637222290039\n",
      "epoch n°1175 : train_loss = 1.8692309856414795, val_loss = 0.7259709239006042\n",
      "epoch n°1176 : train_loss = 1.875058650970459, val_loss = 0.7262713313102722\n",
      "epoch n°1177 : train_loss = 1.8633573055267334, val_loss = 0.7240867018699646\n",
      "epoch n°1178 : train_loss = 1.8580678701400757, val_loss = 0.7246032357215881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1179 : train_loss = 1.8640742301940918, val_loss = 0.7227064371109009\n",
      "epoch n°1180 : train_loss = 1.8679219484329224, val_loss = 0.7235991954803467\n",
      "epoch n°1181 : train_loss = 1.8683110475540161, val_loss = 0.7194294333457947\n",
      "epoch n°1182 : train_loss = 1.8737720251083374, val_loss = 0.7232407927513123\n",
      "epoch n°1183 : train_loss = 1.8678665161132812, val_loss = 0.722780704498291\n",
      "epoch n°1184 : train_loss = 1.8585443496704102, val_loss = 0.7309541702270508\n",
      "epoch n°1185 : train_loss = 1.8610832691192627, val_loss = 0.7247521877288818\n",
      "epoch n°1186 : train_loss = 1.8687968254089355, val_loss = 0.7256034016609192\n",
      "epoch n°1187 : train_loss = 1.8707443475723267, val_loss = 0.73041170835495\n",
      "epoch n°1188 : train_loss = 1.8617581129074097, val_loss = 0.7241639494895935\n",
      "epoch n°1189 : train_loss = 1.8631714582443237, val_loss = 0.7264998555183411\n",
      "epoch n°1190 : train_loss = 1.8638625144958496, val_loss = 0.7288408875465393\n",
      "epoch n°1191 : train_loss = 1.8672285079956055, val_loss = 0.7281318306922913\n",
      "epoch n°1192 : train_loss = 1.8751295804977417, val_loss = 0.7223954200744629\n",
      "epoch n°1193 : train_loss = 1.8693203926086426, val_loss = 0.7221037149429321\n",
      "epoch n°1194 : train_loss = 1.8705538511276245, val_loss = 0.7275535464286804\n",
      "epoch n°1195 : train_loss = 1.8725318908691406, val_loss = 0.7259926795959473\n",
      "epoch n°1196 : train_loss = 1.85316002368927, val_loss = 0.729240894317627\n",
      "epoch n°1197 : train_loss = 1.862772822380066, val_loss = 0.7216045260429382\n",
      "epoch n°1198 : train_loss = 1.877349615097046, val_loss = 0.7312422394752502\n",
      "epoch n°1199 : train_loss = 1.8640663623809814, val_loss = 0.7321120500564575\n",
      "epoch n°1200 : train_loss = 1.8668941259384155, val_loss = 0.7254330515861511\n",
      "epoch n°1201 : train_loss = 1.865275263786316, val_loss = 0.7252845168113708\n",
      "epoch n°1202 : train_loss = 1.8681761026382446, val_loss = 0.727415919303894\n",
      "epoch n°1203 : train_loss = 1.8765709400177002, val_loss = 0.7296205163002014\n",
      "epoch n°1204 : train_loss = 1.8752192258834839, val_loss = 0.7263715267181396\n",
      "epoch n°1205 : train_loss = 1.866710901260376, val_loss = 0.7203966379165649\n",
      "epoch n°1206 : train_loss = 1.8638068437576294, val_loss = 0.7261009812355042\n",
      "epoch n°1207 : train_loss = 1.860190749168396, val_loss = 0.7264655232429504\n",
      "epoch n°1208 : train_loss = 1.860156774520874, val_loss = 0.7216520309448242\n",
      "epoch n°1209 : train_loss = 1.864051103591919, val_loss = 0.7285591959953308\n",
      "epoch n°1210 : train_loss = 1.8650009632110596, val_loss = 0.7288593649864197\n",
      "epoch n°1211 : train_loss = 1.8615384101867676, val_loss = 0.722403883934021\n",
      "epoch n°1212 : train_loss = 1.8794267177581787, val_loss = 0.7239583134651184\n",
      "epoch n°1213 : train_loss = 1.8645579814910889, val_loss = 0.7276296019554138\n",
      "epoch n°1214 : train_loss = 1.8672279119491577, val_loss = 0.7260611653327942\n",
      "epoch n°1215 : train_loss = 1.8607826232910156, val_loss = 0.7246794700622559\n",
      "epoch n°1216 : train_loss = 1.8673405647277832, val_loss = 0.7250505685806274\n",
      "epoch n°1217 : train_loss = 1.8616368770599365, val_loss = 0.7266606092453003\n",
      "epoch n°1218 : train_loss = 1.8712562322616577, val_loss = 0.7276309132575989\n",
      "epoch n°1219 : train_loss = 1.8662890195846558, val_loss = 0.72599196434021\n",
      "epoch n°1220 : train_loss = 1.8647964000701904, val_loss = 0.729541003704071\n",
      "epoch n°1221 : train_loss = 1.8655668497085571, val_loss = 0.725797712802887\n",
      "epoch n°1222 : train_loss = 1.8554259538650513, val_loss = 0.7302270531654358\n",
      "epoch n°1223 : train_loss = 1.880568027496338, val_loss = 0.7275938391685486\n",
      "epoch n°1224 : train_loss = 1.864552617073059, val_loss = 0.7303945422172546\n",
      "epoch n°1225 : train_loss = 1.873788833618164, val_loss = 0.7265546917915344\n",
      "epoch n°1226 : train_loss = 1.853129267692566, val_loss = 0.7241225242614746\n",
      "epoch n°1227 : train_loss = 1.8765435218811035, val_loss = 0.7238487005233765\n",
      "epoch n°1228 : train_loss = 1.867965817451477, val_loss = 0.7251367568969727\n",
      "epoch n°1229 : train_loss = 1.8543071746826172, val_loss = 0.7250902652740479\n",
      "epoch n°1230 : train_loss = 1.853089451789856, val_loss = 0.7260326147079468\n",
      "epoch n°1231 : train_loss = 1.8786749839782715, val_loss = 0.7288147807121277\n",
      "epoch n°1232 : train_loss = 1.8641146421432495, val_loss = 0.7274947166442871\n",
      "epoch n°1233 : train_loss = 1.8665533065795898, val_loss = 0.7261150479316711\n",
      "epoch n°1234 : train_loss = 1.8575165271759033, val_loss = 0.7238800525665283\n",
      "epoch n°1235 : train_loss = 1.8679004907608032, val_loss = 0.7246549725532532\n",
      "epoch n°1236 : train_loss = 1.8643243312835693, val_loss = 0.7252320647239685\n",
      "epoch n°1237 : train_loss = 1.8570473194122314, val_loss = 0.7233431935310364\n",
      "epoch n°1238 : train_loss = 1.86780846118927, val_loss = 0.7340448498725891\n",
      "epoch n°1239 : train_loss = 1.866039752960205, val_loss = 0.7298465371131897\n",
      "epoch n°1240 : train_loss = 1.8679758310317993, val_loss = 0.7227054238319397\n",
      "epoch n°1241 : train_loss = 1.8718440532684326, val_loss = 0.7229542136192322\n",
      "epoch n°1242 : train_loss = 1.863046646118164, val_loss = 0.7297070622444153\n",
      "epoch n°1243 : train_loss = 1.8634393215179443, val_loss = 0.7291173338890076\n",
      "epoch n°1244 : train_loss = 1.869964599609375, val_loss = 0.7247986197471619\n",
      "epoch n°1245 : train_loss = 1.867299199104309, val_loss = 0.7241963148117065\n",
      "epoch n°1246 : train_loss = 1.8497577905654907, val_loss = 0.7271164655685425\n",
      "epoch n°1247 : train_loss = 1.8746014833450317, val_loss = 0.7264740467071533\n",
      "epoch n°1248 : train_loss = 1.8666013479232788, val_loss = 0.7238503694534302\n",
      "epoch n°1249 : train_loss = 1.8610798120498657, val_loss = 0.7259845733642578\n",
      "epoch n°1250 : train_loss = 1.8529192209243774, val_loss = 0.7240299582481384\n",
      "epoch n°1251 : train_loss = 1.8592162132263184, val_loss = 0.7304221391677856\n",
      "epoch n°1252 : train_loss = 1.8674259185791016, val_loss = 0.7253952026367188\n",
      "epoch n°1253 : train_loss = 1.8583048582077026, val_loss = 0.7280139327049255\n",
      "epoch n°1254 : train_loss = 1.879925012588501, val_loss = 0.7221037745475769\n",
      "epoch n°1255 : train_loss = 1.8587173223495483, val_loss = 0.7256291508674622\n",
      "epoch n°1256 : train_loss = 1.866118311882019, val_loss = 0.7289612889289856\n",
      "epoch n°1257 : train_loss = 1.8602776527404785, val_loss = 0.729380190372467\n",
      "epoch n°1258 : train_loss = 1.8651541471481323, val_loss = 0.722690999507904\n",
      "epoch n°1259 : train_loss = 1.861835241317749, val_loss = 0.7284181118011475\n",
      "epoch n°1260 : train_loss = 1.8551599979400635, val_loss = 0.7234733700752258\n",
      "epoch n°1261 : train_loss = 1.8513643741607666, val_loss = 0.7316343784332275\n",
      "epoch n°1262 : train_loss = 1.858351230621338, val_loss = 0.7256815433502197\n",
      "epoch n°1263 : train_loss = 1.8572691679000854, val_loss = 0.7324987053871155\n",
      "epoch n°1264 : train_loss = 1.8674875497817993, val_loss = 0.7260029911994934\n",
      "epoch n°1265 : train_loss = 1.855189323425293, val_loss = 0.7288538813591003\n",
      "epoch n°1266 : train_loss = 1.8471940755844116, val_loss = 0.7282243371009827\n",
      "epoch n°1267 : train_loss = 1.8575507402420044, val_loss = 0.7254338264465332\n",
      "epoch n°1268 : train_loss = 1.853130578994751, val_loss = 0.7243630886077881\n",
      "epoch n°1269 : train_loss = 1.865543246269226, val_loss = 0.73087477684021\n",
      "epoch n°1270 : train_loss = 1.8663891553878784, val_loss = 0.7276368141174316\n",
      "epoch n°1271 : train_loss = 1.8576769828796387, val_loss = 0.722480833530426\n",
      "epoch n°1272 : train_loss = 1.8560863733291626, val_loss = 0.7275655269622803\n",
      "epoch n°1273 : train_loss = 1.8690428733825684, val_loss = 0.7287331819534302\n",
      "epoch n°1274 : train_loss = 1.8548181056976318, val_loss = 0.728350043296814\n",
      "epoch n°1275 : train_loss = 1.8666555881500244, val_loss = 0.7280406951904297\n",
      "epoch n°1276 : train_loss = 1.8531407117843628, val_loss = 0.7232944965362549\n",
      "epoch n°1277 : train_loss = 1.8523958921432495, val_loss = 0.7282263040542603\n",
      "epoch n°1278 : train_loss = 1.8579623699188232, val_loss = 0.7264548540115356\n",
      "epoch n°1279 : train_loss = 1.8603324890136719, val_loss = 0.7256740927696228\n",
      "epoch n°1280 : train_loss = 1.853154182434082, val_loss = 0.7254714965820312\n",
      "epoch n°1281 : train_loss = 1.852270483970642, val_loss = 0.7199045419692993\n",
      "epoch n°1282 : train_loss = 1.8542778491973877, val_loss = 0.7239307761192322\n",
      "epoch n°1283 : train_loss = 1.8544560670852661, val_loss = 0.7266384363174438\n",
      "epoch n°1284 : train_loss = 1.8583346605300903, val_loss = 0.7274617552757263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1285 : train_loss = 1.861235499382019, val_loss = 0.7229043841362\n",
      "epoch n°1286 : train_loss = 1.8537743091583252, val_loss = 0.726176917552948\n",
      "epoch n°1287 : train_loss = 1.8481507301330566, val_loss = 0.7271362543106079\n",
      "epoch n°1288 : train_loss = 1.8626757860183716, val_loss = 0.7280737161636353\n",
      "epoch n°1289 : train_loss = 1.8569797277450562, val_loss = 0.726346492767334\n",
      "epoch n°1290 : train_loss = 1.8559538125991821, val_loss = 0.7225862741470337\n",
      "epoch n°1291 : train_loss = 1.8384217023849487, val_loss = 0.7295297980308533\n",
      "epoch n°1292 : train_loss = 1.8484314680099487, val_loss = 0.7279568314552307\n",
      "epoch n°1293 : train_loss = 1.8637605905532837, val_loss = 0.7268769145011902\n",
      "epoch n°1294 : train_loss = 1.8641483783721924, val_loss = 0.7266526222229004\n",
      "epoch n°1295 : train_loss = 1.859699010848999, val_loss = 0.7256075739860535\n",
      "epoch n°1296 : train_loss = 1.860093116760254, val_loss = 0.7285817265510559\n",
      "epoch n°1297 : train_loss = 1.8630930185317993, val_loss = 0.7265347242355347\n",
      "epoch n°1298 : train_loss = 1.8628123998641968, val_loss = 0.7239354252815247\n",
      "epoch n°1299 : train_loss = 1.8516442775726318, val_loss = 0.7259897589683533\n",
      "epoch n°1300 : train_loss = 1.8583793640136719, val_loss = 0.7294301390647888\n",
      "epoch n°1301 : train_loss = 1.843103051185608, val_loss = 0.7247322797775269\n",
      "epoch n°1302 : train_loss = 1.861014723777771, val_loss = 0.7277891039848328\n",
      "epoch n°1303 : train_loss = 1.8467141389846802, val_loss = 0.7247057557106018\n",
      "epoch n°1304 : train_loss = 1.8499209880828857, val_loss = 0.7276604175567627\n",
      "epoch n°1305 : train_loss = 1.8562136888504028, val_loss = 0.727206826210022\n",
      "epoch n°1306 : train_loss = 1.849694848060608, val_loss = 0.7241575717926025\n",
      "epoch n°1307 : train_loss = 1.8487544059753418, val_loss = 0.7264565229415894\n",
      "epoch n°1308 : train_loss = 1.8582053184509277, val_loss = 0.7261983156204224\n",
      "epoch n°1309 : train_loss = 1.856785535812378, val_loss = 0.7312259674072266\n",
      "epoch n°1310 : train_loss = 1.8603594303131104, val_loss = 0.7269283533096313\n",
      "epoch n°1311 : train_loss = 1.8616993427276611, val_loss = 0.7325710654258728\n",
      "epoch n°1312 : train_loss = 1.844264268875122, val_loss = 0.7218235731124878\n",
      "epoch n°1313 : train_loss = 1.851622462272644, val_loss = 0.728730320930481\n",
      "epoch n°1314 : train_loss = 1.8533426523208618, val_loss = 0.7261579632759094\n",
      "epoch n°1315 : train_loss = 1.859940528869629, val_loss = 0.7208934426307678\n",
      "epoch n°1316 : train_loss = 1.8599940538406372, val_loss = 0.7278063297271729\n",
      "epoch n°1317 : train_loss = 1.8545730113983154, val_loss = 0.7239651083946228\n",
      "epoch n°1318 : train_loss = 1.8461915254592896, val_loss = 0.7270437479019165\n",
      "epoch n°1319 : train_loss = 1.868091344833374, val_loss = 0.7245780229568481\n",
      "epoch n°1320 : train_loss = 1.8550233840942383, val_loss = 0.7251784205436707\n",
      "epoch n°1321 : train_loss = 1.8453691005706787, val_loss = 0.7253940105438232\n",
      "epoch n°1322 : train_loss = 1.85590660572052, val_loss = 0.7237453460693359\n",
      "epoch n°1323 : train_loss = 1.8640387058258057, val_loss = 0.7315450310707092\n",
      "epoch n°1324 : train_loss = 1.8528302907943726, val_loss = 0.7260459065437317\n",
      "epoch n°1325 : train_loss = 1.8520042896270752, val_loss = 0.7264419794082642\n",
      "epoch n°1326 : train_loss = 1.8624550104141235, val_loss = 0.7252618074417114\n",
      "epoch n°1327 : train_loss = 1.857397437095642, val_loss = 0.7255602478981018\n",
      "epoch n°1328 : train_loss = 1.8577768802642822, val_loss = 0.7248775959014893\n",
      "epoch n°1329 : train_loss = 1.847983717918396, val_loss = 0.725289523601532\n",
      "epoch n°1330 : train_loss = 1.854689598083496, val_loss = 0.7290339469909668\n",
      "epoch n°1331 : train_loss = 1.8595259189605713, val_loss = 0.7221289277076721\n",
      "epoch n°1332 : train_loss = 1.8528484106063843, val_loss = 0.7259345054626465\n",
      "epoch n°1333 : train_loss = 1.856307029724121, val_loss = 0.7228530645370483\n",
      "epoch n°1334 : train_loss = 1.8556928634643555, val_loss = 0.7239248156547546\n",
      "epoch n°1335 : train_loss = 1.855159044265747, val_loss = 0.7242503762245178\n",
      "epoch n°1336 : train_loss = 1.8533685207366943, val_loss = 0.7233158349990845\n",
      "epoch n°1337 : train_loss = 1.848312258720398, val_loss = 0.7267073392868042\n",
      "epoch n°1338 : train_loss = 1.8487924337387085, val_loss = 0.7232049107551575\n",
      "epoch n°1339 : train_loss = 1.8548158407211304, val_loss = 0.725074827671051\n",
      "epoch n°1340 : train_loss = 1.862529993057251, val_loss = 0.7238215208053589\n",
      "epoch n°1341 : train_loss = 1.859210729598999, val_loss = 0.7267193794250488\n",
      "epoch n°1342 : train_loss = 1.8536665439605713, val_loss = 0.7253043055534363\n",
      "epoch n°1343 : train_loss = 1.8588536977767944, val_loss = 0.7263144850730896\n",
      "epoch n°1344 : train_loss = 1.8544996976852417, val_loss = 0.7243824005126953\n",
      "epoch n°1345 : train_loss = 1.8520869016647339, val_loss = 0.7270214557647705\n",
      "epoch n°1346 : train_loss = 1.85765540599823, val_loss = 0.7300963997840881\n",
      "epoch n°1347 : train_loss = 1.8457057476043701, val_loss = 0.7246485352516174\n",
      "epoch n°1348 : train_loss = 1.8509137630462646, val_loss = 0.7273598909378052\n",
      "epoch n°1349 : train_loss = 1.8515607118606567, val_loss = 0.722739577293396\n",
      "epoch n°1350 : train_loss = 1.8540102243423462, val_loss = 0.7247191071510315\n",
      "epoch n°1351 : train_loss = 1.8630919456481934, val_loss = 0.7212781310081482\n",
      "epoch n°1352 : train_loss = 1.8516309261322021, val_loss = 0.7205865979194641\n",
      "epoch n°1353 : train_loss = 1.8563146591186523, val_loss = 0.7233246564865112\n",
      "epoch n°1354 : train_loss = 1.8510613441467285, val_loss = 0.7288902401924133\n",
      "epoch n°1355 : train_loss = 1.8539817333221436, val_loss = 0.7255763411521912\n",
      "epoch n°1356 : train_loss = 1.8502061367034912, val_loss = 0.7264341115951538\n",
      "epoch n°1357 : train_loss = 1.85683012008667, val_loss = 0.7220239639282227\n",
      "epoch n°1358 : train_loss = 1.8458219766616821, val_loss = 0.7244388461112976\n",
      "epoch n°1359 : train_loss = 1.843767523765564, val_loss = 0.7239235043525696\n",
      "epoch n°1360 : train_loss = 1.8492263555526733, val_loss = 0.7257676124572754\n",
      "epoch n°1361 : train_loss = 1.8642826080322266, val_loss = 0.7275269627571106\n",
      "epoch n°1362 : train_loss = 1.8509639501571655, val_loss = 0.7237243056297302\n",
      "epoch n°1363 : train_loss = 1.8659313917160034, val_loss = 0.7248724102973938\n",
      "epoch n°1364 : train_loss = 1.845886468887329, val_loss = 0.7266637086868286\n",
      "epoch n°1365 : train_loss = 1.8365767002105713, val_loss = 0.7266465425491333\n",
      "epoch n°1366 : train_loss = 1.8436598777770996, val_loss = 0.7261505126953125\n",
      "epoch n°1367 : train_loss = 1.8387019634246826, val_loss = 0.7300961017608643\n",
      "epoch n°1368 : train_loss = 1.8548009395599365, val_loss = 0.725204348564148\n",
      "epoch n°1369 : train_loss = 1.8461992740631104, val_loss = 0.7274596691131592\n",
      "epoch n°1370 : train_loss = 1.8449071645736694, val_loss = 0.7319943904876709\n",
      "epoch n°1371 : train_loss = 1.8458579778671265, val_loss = 0.7280911803245544\n",
      "epoch n°1372 : train_loss = 1.8542366027832031, val_loss = 0.7287887334823608\n",
      "epoch n°1373 : train_loss = 1.8446972370147705, val_loss = 0.714216411113739\n",
      "epoch n°1374 : train_loss = 1.841962456703186, val_loss = 0.7227208018302917\n",
      "epoch n°1375 : train_loss = 1.8474899530410767, val_loss = 0.7246248126029968\n",
      "epoch n°1376 : train_loss = 1.8488024473190308, val_loss = 0.7280290722846985\n",
      "epoch n°1377 : train_loss = 1.8558640480041504, val_loss = 0.7261946201324463\n",
      "epoch n°1378 : train_loss = 1.8561233282089233, val_loss = 0.7239124178886414\n",
      "epoch n°1379 : train_loss = 1.838525414466858, val_loss = 0.7266213893890381\n",
      "epoch n°1380 : train_loss = 1.8537739515304565, val_loss = 0.7267540693283081\n",
      "epoch n°1381 : train_loss = 1.8474293947219849, val_loss = 0.7251610159873962\n",
      "epoch n°1382 : train_loss = 1.8432034254074097, val_loss = 0.7274215221405029\n",
      "epoch n°1383 : train_loss = 1.853311538696289, val_loss = 0.7267958521842957\n",
      "epoch n°1384 : train_loss = 1.8395271301269531, val_loss = 0.7239702343940735\n",
      "epoch n°1385 : train_loss = 1.8475725650787354, val_loss = 0.7303131818771362\n",
      "epoch n°1386 : train_loss = 1.8349982500076294, val_loss = 0.7252408862113953\n",
      "epoch n°1387 : train_loss = 1.8423515558242798, val_loss = 0.7243825197219849\n",
      "epoch n°1388 : train_loss = 1.8428417444229126, val_loss = 0.7226383090019226\n",
      "epoch n°1389 : train_loss = 1.837817907333374, val_loss = 0.7298084497451782\n",
      "epoch n°1390 : train_loss = 1.8434375524520874, val_loss = 0.7234177589416504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1391 : train_loss = 1.85044264793396, val_loss = 0.7225409150123596\n",
      "epoch n°1392 : train_loss = 1.84064781665802, val_loss = 0.7304993271827698\n",
      "epoch n°1393 : train_loss = 1.849853515625, val_loss = 0.7275994420051575\n",
      "epoch n°1394 : train_loss = 1.843638300895691, val_loss = 0.7249077558517456\n",
      "epoch n°1395 : train_loss = 1.8456090688705444, val_loss = 0.7292748093605042\n",
      "epoch n°1396 : train_loss = 1.847522497177124, val_loss = 0.7258881330490112\n",
      "epoch n°1397 : train_loss = 1.8341617584228516, val_loss = 0.7214956283569336\n",
      "epoch n°1398 : train_loss = 1.8467892408370972, val_loss = 0.7228364944458008\n",
      "epoch n°1399 : train_loss = 1.841019630432129, val_loss = 0.7289721369743347\n",
      "epoch n°1400 : train_loss = 1.848976492881775, val_loss = 0.7213743925094604\n",
      "epoch n°1401 : train_loss = 1.84578537940979, val_loss = 0.723275899887085\n",
      "epoch n°1402 : train_loss = 1.8532235622406006, val_loss = 0.7276908755302429\n",
      "epoch n°1403 : train_loss = 1.844670295715332, val_loss = 0.7261804342269897\n",
      "epoch n°1404 : train_loss = 1.8375824689865112, val_loss = 0.7280159592628479\n",
      "epoch n°1405 : train_loss = 1.8467168807983398, val_loss = 0.7245350480079651\n",
      "epoch n°1406 : train_loss = 1.843462347984314, val_loss = 0.7301409244537354\n",
      "epoch n°1407 : train_loss = 1.8636761903762817, val_loss = 0.7261964082717896\n",
      "epoch n°1408 : train_loss = 1.8514317274093628, val_loss = 0.7227528095245361\n",
      "epoch n°1409 : train_loss = 1.8571324348449707, val_loss = 0.7291983366012573\n",
      "epoch n°1410 : train_loss = 1.8434648513793945, val_loss = 0.7253613471984863\n",
      "epoch n°1411 : train_loss = 1.837423324584961, val_loss = 0.7236170172691345\n",
      "epoch n°1412 : train_loss = 1.8445380926132202, val_loss = 0.7284460067749023\n",
      "epoch n°1413 : train_loss = 1.8409740924835205, val_loss = 0.7275500893592834\n",
      "epoch n°1414 : train_loss = 1.83919095993042, val_loss = 0.7207528352737427\n",
      "epoch n°1415 : train_loss = 1.8550184965133667, val_loss = 0.7231231331825256\n",
      "epoch n°1416 : train_loss = 1.8451159000396729, val_loss = 0.7258970141410828\n",
      "epoch n°1417 : train_loss = 1.839746356010437, val_loss = 0.7280449867248535\n",
      "epoch n°1418 : train_loss = 1.8441883325576782, val_loss = 0.7204623222351074\n",
      "epoch n°1419 : train_loss = 1.8379615545272827, val_loss = 0.7247538566589355\n",
      "epoch n°1420 : train_loss = 1.841414451599121, val_loss = 0.7220870852470398\n",
      "epoch n°1421 : train_loss = 1.8357458114624023, val_loss = 0.7205153107643127\n",
      "epoch n°1422 : train_loss = 1.8454521894454956, val_loss = 0.7248765826225281\n",
      "epoch n°1423 : train_loss = 1.842624545097351, val_loss = 0.7233297228813171\n",
      "epoch n°1424 : train_loss = 1.8443372249603271, val_loss = 0.7261980772018433\n",
      "epoch n°1425 : train_loss = 1.8424956798553467, val_loss = 0.7228701710700989\n",
      "epoch n°1426 : train_loss = 1.8404209613800049, val_loss = 0.7298376560211182\n",
      "epoch n°1427 : train_loss = 1.8494136333465576, val_loss = 0.7206265926361084\n",
      "epoch n°1428 : train_loss = 1.844763159751892, val_loss = 0.7248244285583496\n",
      "epoch n°1429 : train_loss = 1.8448593616485596, val_loss = 0.7281591296195984\n",
      "epoch n°1430 : train_loss = 1.8477883338928223, val_loss = 0.7252167463302612\n",
      "epoch n°1431 : train_loss = 1.8316330909729004, val_loss = 0.7227017283439636\n",
      "epoch n°1432 : train_loss = 1.8388028144836426, val_loss = 0.7318992614746094\n",
      "epoch n°1433 : train_loss = 1.8466883897781372, val_loss = 0.7252917885780334\n",
      "epoch n°1434 : train_loss = 1.8374035358428955, val_loss = 0.7253631353378296\n",
      "epoch n°1435 : train_loss = 1.8425343036651611, val_loss = 0.720923125743866\n",
      "epoch n°1436 : train_loss = 1.8371233940124512, val_loss = 0.7261558771133423\n",
      "epoch n°1437 : train_loss = 1.8492214679718018, val_loss = 0.7242319583892822\n",
      "epoch n°1438 : train_loss = 1.8336163759231567, val_loss = 0.7232422232627869\n",
      "epoch n°1439 : train_loss = 1.8420389890670776, val_loss = 0.7228124737739563\n",
      "epoch n°1440 : train_loss = 1.8288615942001343, val_loss = 0.7253154516220093\n",
      "epoch n°1441 : train_loss = 1.8405457735061646, val_loss = 0.7256450653076172\n",
      "epoch n°1442 : train_loss = 1.8367741107940674, val_loss = 0.7220647931098938\n",
      "epoch n°1443 : train_loss = 1.8390498161315918, val_loss = 0.7201067209243774\n",
      "epoch n°1444 : train_loss = 1.841614842414856, val_loss = 0.7209420204162598\n",
      "epoch n°1445 : train_loss = 1.8350855112075806, val_loss = 0.7298421859741211\n",
      "epoch n°1446 : train_loss = 1.8402788639068604, val_loss = 0.7242414355278015\n",
      "epoch n°1447 : train_loss = 1.8410800695419312, val_loss = 0.7271304726600647\n",
      "epoch n°1448 : train_loss = 1.8454687595367432, val_loss = 0.725915789604187\n",
      "epoch n°1449 : train_loss = 1.8324006795883179, val_loss = 0.7247551679611206\n",
      "epoch n°1450 : train_loss = 1.8368011713027954, val_loss = 0.7282386422157288\n",
      "epoch n°1451 : train_loss = 1.8332302570343018, val_loss = 0.7247697710990906\n",
      "epoch n°1452 : train_loss = 1.842882513999939, val_loss = 0.7215320467948914\n",
      "epoch n°1453 : train_loss = 1.8402273654937744, val_loss = 0.72286456823349\n",
      "epoch n°1454 : train_loss = 1.8366796970367432, val_loss = 0.7266912460327148\n",
      "epoch n°1455 : train_loss = 1.8382666110992432, val_loss = 0.7255692481994629\n",
      "epoch n°1456 : train_loss = 1.847129225730896, val_loss = 0.7252001762390137\n",
      "epoch n°1457 : train_loss = 1.8322911262512207, val_loss = 0.7267163991928101\n",
      "epoch n°1458 : train_loss = 1.8466235399246216, val_loss = 0.7227348685264587\n",
      "epoch n°1459 : train_loss = 1.838938593864441, val_loss = 0.7239319086074829\n",
      "epoch n°1460 : train_loss = 1.8464841842651367, val_loss = 0.7258594036102295\n",
      "epoch n°1461 : train_loss = 1.8420578241348267, val_loss = 0.725603461265564\n",
      "epoch n°1462 : train_loss = 1.8324819803237915, val_loss = 0.7242487668991089\n",
      "epoch n°1463 : train_loss = 1.8389414548873901, val_loss = 0.7276178598403931\n",
      "epoch n°1464 : train_loss = 1.8470669984817505, val_loss = 0.7230475544929504\n",
      "epoch n°1465 : train_loss = 1.8420931100845337, val_loss = 0.7302852869033813\n",
      "epoch n°1466 : train_loss = 1.8410695791244507, val_loss = 0.7269207239151001\n",
      "epoch n°1467 : train_loss = 1.8485262393951416, val_loss = 0.7230238914489746\n",
      "epoch n°1468 : train_loss = 1.8323613405227661, val_loss = 0.7224925756454468\n",
      "epoch n°1469 : train_loss = 1.843978762626648, val_loss = 0.7272806167602539\n",
      "epoch n°1470 : train_loss = 1.8390228748321533, val_loss = 0.7200050354003906\n",
      "epoch n°1471 : train_loss = 1.8333959579467773, val_loss = 0.7233724594116211\n",
      "epoch n°1472 : train_loss = 1.826477289199829, val_loss = 0.7266790270805359\n",
      "epoch n°1473 : train_loss = 1.83633553981781, val_loss = 0.723612904548645\n",
      "epoch n°1474 : train_loss = 1.844741940498352, val_loss = 0.7265202403068542\n",
      "epoch n°1475 : train_loss = 1.8405711650848389, val_loss = 0.725537121295929\n",
      "epoch n°1476 : train_loss = 1.8311246633529663, val_loss = 0.7279796004295349\n",
      "epoch n°1477 : train_loss = 1.832363247871399, val_loss = 0.7275665402412415\n",
      "epoch n°1478 : train_loss = 1.8267815113067627, val_loss = 0.7277288436889648\n",
      "epoch n°1479 : train_loss = 1.8442339897155762, val_loss = 0.7273985743522644\n",
      "epoch n°1480 : train_loss = 1.8427764177322388, val_loss = 0.7237702012062073\n",
      "epoch n°1481 : train_loss = 1.8349575996398926, val_loss = 0.7191181182861328\n",
      "epoch n°1482 : train_loss = 1.8446044921875, val_loss = 0.7218009233474731\n",
      "epoch n°1483 : train_loss = 1.8418983221054077, val_loss = 0.7258117198944092\n",
      "epoch n°1484 : train_loss = 1.840924859046936, val_loss = 0.7260274291038513\n",
      "epoch n°1485 : train_loss = 1.836473822593689, val_loss = 0.7243466973304749\n",
      "epoch n°1486 : train_loss = 1.8329752683639526, val_loss = 0.7252605557441711\n",
      "epoch n°1487 : train_loss = 1.8423640727996826, val_loss = 0.723755955696106\n",
      "epoch n°1488 : train_loss = 1.8313441276550293, val_loss = 0.7252547144889832\n",
      "epoch n°1489 : train_loss = 1.8358466625213623, val_loss = 0.7225751876831055\n",
      "epoch n°1490 : train_loss = 1.8354458808898926, val_loss = 0.7292975783348083\n",
      "epoch n°1491 : train_loss = 1.8345940113067627, val_loss = 0.7232882380485535\n",
      "epoch n°1492 : train_loss = 1.8351160287857056, val_loss = 0.7247969508171082\n",
      "epoch n°1493 : train_loss = 1.836394190788269, val_loss = 0.7273587584495544\n",
      "epoch n°1494 : train_loss = 1.8304426670074463, val_loss = 0.7227641344070435\n",
      "epoch n°1495 : train_loss = 1.8280222415924072, val_loss = 0.7272467017173767\n",
      "epoch n°1496 : train_loss = 1.8447151184082031, val_loss = 0.7266128063201904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1497 : train_loss = 1.835961937904358, val_loss = 0.7221198081970215\n",
      "epoch n°1498 : train_loss = 1.841373324394226, val_loss = 0.7231627702713013\n",
      "epoch n°1499 : train_loss = 1.8235416412353516, val_loss = 0.7319398522377014\n",
      "epoch n°1500 : train_loss = 1.834427833557129, val_loss = 0.7195560932159424\n",
      "epoch n°1501 : train_loss = 1.8348416090011597, val_loss = 0.7279971837997437\n",
      "epoch n°1502 : train_loss = 1.8337154388427734, val_loss = 0.7262253165245056\n",
      "epoch n°1503 : train_loss = 1.8364770412445068, val_loss = 0.7227177023887634\n",
      "epoch n°1504 : train_loss = 1.8298263549804688, val_loss = 0.7200196385383606\n",
      "epoch n°1505 : train_loss = 1.841535210609436, val_loss = 0.7243656516075134\n",
      "epoch n°1506 : train_loss = 1.8464642763137817, val_loss = 0.7280246615409851\n",
      "epoch n°1507 : train_loss = 1.8271944522857666, val_loss = 0.7233352661132812\n",
      "epoch n°1508 : train_loss = 1.8452945947647095, val_loss = 0.7244486212730408\n",
      "epoch n°1509 : train_loss = 1.839898943901062, val_loss = 0.721145749092102\n",
      "epoch n°1510 : train_loss = 1.8208047151565552, val_loss = 0.7226282358169556\n",
      "epoch n°1511 : train_loss = 1.8318636417388916, val_loss = 0.7248396873474121\n",
      "epoch n°1512 : train_loss = 1.8227084875106812, val_loss = 0.7233037352561951\n",
      "epoch n°1513 : train_loss = 1.8353097438812256, val_loss = 0.723683774471283\n",
      "epoch n°1514 : train_loss = 1.834529161453247, val_loss = 0.7247023582458496\n",
      "epoch n°1515 : train_loss = 1.8362503051757812, val_loss = 0.7237917184829712\n",
      "epoch n°1516 : train_loss = 1.8392021656036377, val_loss = 0.7265768051147461\n",
      "epoch n°1517 : train_loss = 1.8359174728393555, val_loss = 0.7230761647224426\n",
      "epoch n°1518 : train_loss = 1.8294703960418701, val_loss = 0.723668098449707\n",
      "epoch n°1519 : train_loss = 1.8372589349746704, val_loss = 0.725017786026001\n",
      "epoch n°1520 : train_loss = 1.8372242450714111, val_loss = 0.7157655358314514\n",
      "epoch n°1521 : train_loss = 1.8306785821914673, val_loss = 0.7237346768379211\n",
      "epoch n°1522 : train_loss = 1.8329461812973022, val_loss = 0.7273790240287781\n",
      "epoch n°1523 : train_loss = 1.8397260904312134, val_loss = 0.7242726683616638\n",
      "epoch n°1524 : train_loss = 1.8339544534683228, val_loss = 0.7289252877235413\n",
      "epoch n°1525 : train_loss = 1.8309544324874878, val_loss = 0.7235556244850159\n",
      "epoch n°1526 : train_loss = 1.8343263864517212, val_loss = 0.725968599319458\n",
      "epoch n°1527 : train_loss = 1.8233684301376343, val_loss = 0.7242807745933533\n",
      "epoch n°1528 : train_loss = 1.8314522504806519, val_loss = 0.7210467457771301\n",
      "epoch n°1529 : train_loss = 1.8325114250183105, val_loss = 0.7224008440971375\n",
      "epoch n°1530 : train_loss = 1.8259083032608032, val_loss = 0.724057674407959\n",
      "epoch n°1531 : train_loss = 1.8330917358398438, val_loss = 0.721418023109436\n",
      "epoch n°1532 : train_loss = 1.8371822834014893, val_loss = 0.7242431640625\n",
      "epoch n°1533 : train_loss = 1.8338510990142822, val_loss = 0.7278242111206055\n",
      "epoch n°1534 : train_loss = 1.82625150680542, val_loss = 0.7245829701423645\n",
      "epoch n°1535 : train_loss = 1.8311187028884888, val_loss = 0.7226590514183044\n",
      "epoch n°1536 : train_loss = 1.8283766508102417, val_loss = 0.7253642082214355\n",
      "epoch n°1537 : train_loss = 1.8358402252197266, val_loss = 0.725019097328186\n",
      "epoch n°1538 : train_loss = 1.8364417552947998, val_loss = 0.724265456199646\n",
      "epoch n°1539 : train_loss = 1.8258978128433228, val_loss = 0.7300812602043152\n",
      "epoch n°1540 : train_loss = 1.8222485780715942, val_loss = 0.7232816219329834\n",
      "epoch n°1541 : train_loss = 1.8425384759902954, val_loss = 0.7243802547454834\n",
      "epoch n°1542 : train_loss = 1.8286716938018799, val_loss = 0.7218281626701355\n",
      "epoch n°1543 : train_loss = 1.8316445350646973, val_loss = 0.7253603935241699\n",
      "epoch n°1544 : train_loss = 1.8361008167266846, val_loss = 0.7277690172195435\n",
      "epoch n°1545 : train_loss = 1.8276959657669067, val_loss = 0.7216687202453613\n",
      "epoch n°1546 : train_loss = 1.818604826927185, val_loss = 0.7248471975326538\n",
      "epoch n°1547 : train_loss = 1.8255747556686401, val_loss = 0.728253960609436\n",
      "epoch n°1548 : train_loss = 1.8148608207702637, val_loss = 0.7192548513412476\n",
      "epoch n°1549 : train_loss = 1.8358091115951538, val_loss = 0.7242080569267273\n",
      "epoch n°1550 : train_loss = 1.8267329931259155, val_loss = 0.7181070446968079\n",
      "epoch n°1551 : train_loss = 1.8255270719528198, val_loss = 0.7162361145019531\n",
      "epoch n°1552 : train_loss = 1.8264241218566895, val_loss = 0.7236853837966919\n",
      "epoch n°1553 : train_loss = 1.8366769552230835, val_loss = 0.7216690182685852\n",
      "epoch n°1554 : train_loss = 1.8361560106277466, val_loss = 0.723211407661438\n",
      "epoch n°1555 : train_loss = 1.8335317373275757, val_loss = 0.7166095972061157\n",
      "epoch n°1556 : train_loss = 1.8250120878219604, val_loss = 0.7239533066749573\n",
      "epoch n°1557 : train_loss = 1.842361330986023, val_loss = 0.7274735569953918\n",
      "epoch n°1558 : train_loss = 1.8317162990570068, val_loss = 0.7200960516929626\n",
      "epoch n°1559 : train_loss = 1.821111798286438, val_loss = 0.7243354320526123\n",
      "epoch n°1560 : train_loss = 1.8268088102340698, val_loss = 0.7248328328132629\n",
      "epoch n°1561 : train_loss = 1.827772617340088, val_loss = 0.7250510454177856\n",
      "epoch n°1562 : train_loss = 1.819913387298584, val_loss = 0.7276662588119507\n",
      "epoch n°1563 : train_loss = 1.8304778337478638, val_loss = 0.7285472750663757\n",
      "epoch n°1564 : train_loss = 1.817640781402588, val_loss = 0.7227489352226257\n",
      "epoch n°1565 : train_loss = 1.8273284435272217, val_loss = 0.7188035249710083\n",
      "epoch n°1566 : train_loss = 1.8310517072677612, val_loss = 0.7253534197807312\n",
      "epoch n°1567 : train_loss = 1.8273141384124756, val_loss = 0.7195804715156555\n",
      "epoch n°1568 : train_loss = 1.8340203762054443, val_loss = 0.7220295071601868\n",
      "epoch n°1569 : train_loss = 1.8344632387161255, val_loss = 0.7220528721809387\n",
      "epoch n°1570 : train_loss = 1.8199992179870605, val_loss = 0.7262375950813293\n",
      "epoch n°1571 : train_loss = 1.8328479528427124, val_loss = 0.7285974621772766\n",
      "epoch n°1572 : train_loss = 1.830082893371582, val_loss = 0.7213894128799438\n",
      "epoch n°1573 : train_loss = 1.8245121240615845, val_loss = 0.7245044112205505\n",
      "epoch n°1574 : train_loss = 1.8306924104690552, val_loss = 0.7317042946815491\n",
      "epoch n°1575 : train_loss = 1.8243502378463745, val_loss = 0.7217848300933838\n",
      "epoch n°1576 : train_loss = 1.8269031047821045, val_loss = 0.7297231554985046\n",
      "epoch n°1577 : train_loss = 1.8246148824691772, val_loss = 0.722915768623352\n",
      "epoch n°1578 : train_loss = 1.8354662656784058, val_loss = 0.7226369976997375\n",
      "epoch n°1579 : train_loss = 1.8263875246047974, val_loss = 0.7264105677604675\n",
      "epoch n°1580 : train_loss = 1.8270673751831055, val_loss = 0.7219194769859314\n",
      "epoch n°1581 : train_loss = 1.825415849685669, val_loss = 0.7263230681419373\n",
      "epoch n°1582 : train_loss = 1.8321272134780884, val_loss = 0.7269333004951477\n",
      "epoch n°1583 : train_loss = 1.8236782550811768, val_loss = 0.7219105362892151\n",
      "epoch n°1584 : train_loss = 1.820873498916626, val_loss = 0.7257656455039978\n",
      "epoch n°1585 : train_loss = 1.823721170425415, val_loss = 0.7209857702255249\n",
      "epoch n°1586 : train_loss = 1.8334722518920898, val_loss = 0.7263104319572449\n",
      "epoch n°1587 : train_loss = 1.82664954662323, val_loss = 0.7272641658782959\n",
      "epoch n°1588 : train_loss = 1.8217648267745972, val_loss = 0.7246358394622803\n",
      "epoch n°1589 : train_loss = 1.8178613185882568, val_loss = 0.7204939723014832\n",
      "epoch n°1590 : train_loss = 1.8289510011672974, val_loss = 0.7228466868400574\n",
      "epoch n°1591 : train_loss = 1.8175472021102905, val_loss = 0.7248281240463257\n",
      "epoch n°1592 : train_loss = 1.8186668157577515, val_loss = 0.7261229157447815\n",
      "epoch n°1593 : train_loss = 1.8222830295562744, val_loss = 0.7237583994865417\n",
      "epoch n°1594 : train_loss = 1.8146021366119385, val_loss = 0.7267893552780151\n",
      "epoch n°1595 : train_loss = 1.8293449878692627, val_loss = 0.7234675288200378\n",
      "epoch n°1596 : train_loss = 1.8238670825958252, val_loss = 0.7230141758918762\n",
      "epoch n°1597 : train_loss = 1.8251253366470337, val_loss = 0.7267518043518066\n",
      "epoch n°1598 : train_loss = 1.8300981521606445, val_loss = 0.7190250754356384\n",
      "epoch n°1599 : train_loss = 1.8223260641098022, val_loss = 0.7215190529823303\n",
      "epoch n°1600 : train_loss = 1.8167445659637451, val_loss = 0.7232473492622375\n",
      "epoch n°1601 : train_loss = 1.8208037614822388, val_loss = 0.7211493849754333\n",
      "epoch n°1602 : train_loss = 1.8216196298599243, val_loss = 0.7293440699577332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1603 : train_loss = 1.8227680921554565, val_loss = 0.7197228670120239\n",
      "epoch n°1604 : train_loss = 1.8255484104156494, val_loss = 0.7232158184051514\n",
      "epoch n°1605 : train_loss = 1.8257278203964233, val_loss = 0.7228371500968933\n",
      "epoch n°1606 : train_loss = 1.8175318241119385, val_loss = 0.7235504984855652\n",
      "epoch n°1607 : train_loss = 1.8258236646652222, val_loss = 0.7263652682304382\n",
      "epoch n°1608 : train_loss = 1.8281924724578857, val_loss = 0.7239780426025391\n",
      "epoch n°1609 : train_loss = 1.8408546447753906, val_loss = 0.7206467390060425\n",
      "epoch n°1610 : train_loss = 1.8308271169662476, val_loss = 0.722265899181366\n",
      "epoch n°1611 : train_loss = 1.8302663564682007, val_loss = 0.7177819609642029\n",
      "epoch n°1612 : train_loss = 1.8223795890808105, val_loss = 0.7210942506790161\n",
      "epoch n°1613 : train_loss = 1.8237844705581665, val_loss = 0.7202147841453552\n",
      "epoch n°1614 : train_loss = 1.819367527961731, val_loss = 0.7228673100471497\n",
      "epoch n°1615 : train_loss = 1.8241404294967651, val_loss = 0.7183508276939392\n",
      "epoch n°1616 : train_loss = 1.823146104812622, val_loss = 0.7253162264823914\n",
      "epoch n°1617 : train_loss = 1.817359209060669, val_loss = 0.7263035178184509\n",
      "epoch n°1618 : train_loss = 1.8327890634536743, val_loss = 0.7247301936149597\n",
      "epoch n°1619 : train_loss = 1.8350803852081299, val_loss = 0.7240594029426575\n",
      "epoch n°1620 : train_loss = 1.8275166749954224, val_loss = 0.7234077453613281\n",
      "epoch n°1621 : train_loss = 1.8180299997329712, val_loss = 0.7214592695236206\n",
      "epoch n°1622 : train_loss = 1.8273016214370728, val_loss = 0.7212360501289368\n",
      "epoch n°1623 : train_loss = 1.8297326564788818, val_loss = 0.7166975736618042\n",
      "epoch n°1624 : train_loss = 1.8193148374557495, val_loss = 0.715937614440918\n",
      "epoch n°1625 : train_loss = 1.8218389749526978, val_loss = 0.720321536064148\n",
      "epoch n°1626 : train_loss = 1.8338127136230469, val_loss = 0.7219494581222534\n",
      "epoch n°1627 : train_loss = 1.8112568855285645, val_loss = 0.721614420413971\n",
      "epoch n°1628 : train_loss = 1.8301465511322021, val_loss = 0.7182356715202332\n",
      "epoch n°1629 : train_loss = 1.8238694667816162, val_loss = 0.7230145931243896\n",
      "epoch n°1630 : train_loss = 1.8315351009368896, val_loss = 0.7251738905906677\n",
      "epoch n°1631 : train_loss = 1.835431694984436, val_loss = 0.7232580780982971\n",
      "epoch n°1632 : train_loss = 1.8200669288635254, val_loss = 0.7270398139953613\n",
      "epoch n°1633 : train_loss = 1.8266515731811523, val_loss = 0.7207075357437134\n",
      "epoch n°1634 : train_loss = 1.8238263130187988, val_loss = 0.7155442237854004\n",
      "epoch n°1635 : train_loss = 1.824066162109375, val_loss = 0.7257129549980164\n",
      "epoch n°1636 : train_loss = 1.8204712867736816, val_loss = 0.7281020283699036\n",
      "epoch n°1637 : train_loss = 1.8147321939468384, val_loss = 0.7229663133621216\n",
      "epoch n°1638 : train_loss = 1.8214036226272583, val_loss = 0.7253719568252563\n",
      "epoch n°1639 : train_loss = 1.8277708292007446, val_loss = 0.7273103594779968\n",
      "epoch n°1640 : train_loss = 1.8187708854675293, val_loss = 0.7234349846839905\n",
      "epoch n°1641 : train_loss = 1.8207921981811523, val_loss = 0.7222437262535095\n",
      "epoch n°1642 : train_loss = 1.8095057010650635, val_loss = 0.7253866195678711\n",
      "epoch n°1643 : train_loss = 1.8259507417678833, val_loss = 0.719993531703949\n",
      "epoch n°1644 : train_loss = 1.8300431966781616, val_loss = 0.7274422645568848\n",
      "epoch n°1645 : train_loss = 1.820009708404541, val_loss = 0.7215583920478821\n",
      "epoch n°1646 : train_loss = 1.8196290731430054, val_loss = 0.7265666723251343\n",
      "epoch n°1647 : train_loss = 1.8196359872817993, val_loss = 0.725666880607605\n",
      "epoch n°1648 : train_loss = 1.819992184638977, val_loss = 0.7213080525398254\n",
      "epoch n°1649 : train_loss = 1.8218644857406616, val_loss = 0.7238851189613342\n",
      "epoch n°1650 : train_loss = 1.8260338306427002, val_loss = 0.7189255356788635\n",
      "epoch n°1651 : train_loss = 1.8127714395523071, val_loss = 0.7223100066184998\n",
      "epoch n°1652 : train_loss = 1.8196825981140137, val_loss = 0.7213915586471558\n",
      "epoch n°1653 : train_loss = 1.8228660821914673, val_loss = 0.7219501733779907\n",
      "epoch n°1654 : train_loss = 1.8245919942855835, val_loss = 0.7181974053382874\n",
      "epoch n°1655 : train_loss = 1.8224211931228638, val_loss = 0.7241556644439697\n",
      "epoch n°1656 : train_loss = 1.822926640510559, val_loss = 0.7197542786598206\n",
      "epoch n°1657 : train_loss = 1.818387508392334, val_loss = 0.722996711730957\n",
      "epoch n°1658 : train_loss = 1.8164198398590088, val_loss = 0.7205733060836792\n",
      "epoch n°1659 : train_loss = 1.8191286325454712, val_loss = 0.7253441214561462\n",
      "epoch n°1660 : train_loss = 1.812332034111023, val_loss = 0.7270426154136658\n",
      "epoch n°1661 : train_loss = 1.8314650058746338, val_loss = 0.7276337146759033\n",
      "epoch n°1662 : train_loss = 1.8243238925933838, val_loss = 0.7191164493560791\n",
      "epoch n°1663 : train_loss = 1.8164623975753784, val_loss = 0.726783275604248\n",
      "epoch n°1664 : train_loss = 1.817137598991394, val_loss = 0.7222304940223694\n",
      "epoch n°1665 : train_loss = 1.8214465379714966, val_loss = 0.722524881362915\n",
      "epoch n°1666 : train_loss = 1.8207900524139404, val_loss = 0.7246241569519043\n",
      "epoch n°1667 : train_loss = 1.8282325267791748, val_loss = 0.7220540642738342\n",
      "epoch n°1668 : train_loss = 1.818432331085205, val_loss = 0.7265096306800842\n",
      "epoch n°1669 : train_loss = 1.8199284076690674, val_loss = 0.7198643684387207\n",
      "epoch n°1670 : train_loss = 1.8263243436813354, val_loss = 0.7239726185798645\n",
      "epoch n°1671 : train_loss = 1.8211994171142578, val_loss = 0.7284700870513916\n",
      "epoch n°1672 : train_loss = 1.8227217197418213, val_loss = 0.716912567615509\n",
      "epoch n°1673 : train_loss = 1.8247724771499634, val_loss = 0.7226352691650391\n",
      "epoch n°1674 : train_loss = 1.8121737241744995, val_loss = 0.7199952602386475\n",
      "epoch n°1675 : train_loss = 1.8121836185455322, val_loss = 0.7147147059440613\n",
      "epoch n°1676 : train_loss = 1.8230499029159546, val_loss = 0.7246436476707458\n",
      "epoch n°1677 : train_loss = 1.8163440227508545, val_loss = 0.7216050028800964\n",
      "epoch n°1678 : train_loss = 1.8236976861953735, val_loss = 0.721102774143219\n",
      "epoch n°1679 : train_loss = 1.8141151666641235, val_loss = 0.724880039691925\n",
      "epoch n°1680 : train_loss = 1.8101072311401367, val_loss = 0.7228530049324036\n",
      "epoch n°1681 : train_loss = 1.8254464864730835, val_loss = 0.7246091365814209\n",
      "epoch n°1682 : train_loss = 1.8117637634277344, val_loss = 0.7194759845733643\n",
      "epoch n°1683 : train_loss = 1.8360671997070312, val_loss = 0.7248319387435913\n",
      "epoch n°1684 : train_loss = 1.820014238357544, val_loss = 0.7234971523284912\n",
      "epoch n°1685 : train_loss = 1.828918695449829, val_loss = 0.7245621681213379\n",
      "epoch n°1686 : train_loss = 1.8226746320724487, val_loss = 0.7204396724700928\n",
      "epoch n°1687 : train_loss = 1.824948787689209, val_loss = 0.7191416025161743\n",
      "epoch n°1688 : train_loss = 1.8175204992294312, val_loss = 0.72379469871521\n",
      "epoch n°1689 : train_loss = 1.8184378147125244, val_loss = 0.7205866575241089\n",
      "epoch n°1690 : train_loss = 1.8154022693634033, val_loss = 0.7213330864906311\n",
      "epoch n°1691 : train_loss = 1.8232719898223877, val_loss = 0.7204384207725525\n",
      "epoch n°1692 : train_loss = 1.8165003061294556, val_loss = 0.7244344353675842\n",
      "epoch n°1693 : train_loss = 1.8178966045379639, val_loss = 0.7217658758163452\n",
      "epoch n°1694 : train_loss = 1.8170157670974731, val_loss = 0.7224896550178528\n",
      "epoch n°1695 : train_loss = 1.8092821836471558, val_loss = 0.7219301462173462\n",
      "epoch n°1696 : train_loss = 1.8257968425750732, val_loss = 0.7216649055480957\n",
      "epoch n°1697 : train_loss = 1.8143715858459473, val_loss = 0.7219957113265991\n",
      "epoch n°1698 : train_loss = 1.822851538658142, val_loss = 0.720474123954773\n",
      "epoch n°1699 : train_loss = 1.815415382385254, val_loss = 0.7236739993095398\n",
      "epoch n°1700 : train_loss = 1.8138201236724854, val_loss = 0.7247838377952576\n",
      "epoch n°1701 : train_loss = 1.82035231590271, val_loss = 0.7184135913848877\n",
      "epoch n°1702 : train_loss = 1.8132790327072144, val_loss = 0.72495436668396\n",
      "epoch n°1703 : train_loss = 1.8097646236419678, val_loss = 0.7246831655502319\n",
      "epoch n°1704 : train_loss = 1.8122644424438477, val_loss = 0.7206981778144836\n",
      "epoch n°1705 : train_loss = 1.8142937421798706, val_loss = 0.7271688580513\n",
      "epoch n°1706 : train_loss = 1.8116235733032227, val_loss = 0.7244269847869873\n",
      "epoch n°1707 : train_loss = 1.8144224882125854, val_loss = 0.72232985496521\n",
      "epoch n°1708 : train_loss = 1.8158059120178223, val_loss = 0.7222498059272766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1709 : train_loss = 1.8103314638137817, val_loss = 0.7187858819961548\n",
      "epoch n°1710 : train_loss = 1.8195632696151733, val_loss = 0.7227898240089417\n",
      "epoch n°1711 : train_loss = 1.8106799125671387, val_loss = 0.7245904207229614\n",
      "epoch n°1712 : train_loss = 1.816819190979004, val_loss = 0.7219982147216797\n",
      "epoch n°1713 : train_loss = 1.8110973834991455, val_loss = 0.7209417819976807\n",
      "epoch n°1714 : train_loss = 1.8099335432052612, val_loss = 0.7164667248725891\n",
      "epoch n°1715 : train_loss = 1.8067951202392578, val_loss = 0.7203778028488159\n",
      "epoch n°1716 : train_loss = 1.8147671222686768, val_loss = 0.7231687307357788\n",
      "epoch n°1717 : train_loss = 1.8175623416900635, val_loss = 0.7225280404090881\n",
      "epoch n°1718 : train_loss = 1.8163080215454102, val_loss = 0.7176939845085144\n",
      "epoch n°1719 : train_loss = 1.8219555616378784, val_loss = 0.7201563119888306\n",
      "epoch n°1720 : train_loss = 1.8263053894042969, val_loss = 0.7230461239814758\n",
      "epoch n°1721 : train_loss = 1.8259326219558716, val_loss = 0.7296406030654907\n",
      "epoch n°1722 : train_loss = 1.8096227645874023, val_loss = 0.7215843796730042\n",
      "epoch n°1723 : train_loss = 1.8153271675109863, val_loss = 0.7187293171882629\n",
      "epoch n°1724 : train_loss = 1.8106789588928223, val_loss = 0.7226426005363464\n",
      "epoch n°1725 : train_loss = 1.8196423053741455, val_loss = 0.7216681838035583\n",
      "epoch n°1726 : train_loss = 1.819523572921753, val_loss = 0.7232213020324707\n",
      "epoch n°1727 : train_loss = 1.8063604831695557, val_loss = 0.7195544838905334\n",
      "epoch n°1728 : train_loss = 1.826768398284912, val_loss = 0.72102290391922\n",
      "epoch n°1729 : train_loss = 1.8143420219421387, val_loss = 0.7216915488243103\n",
      "epoch n°1730 : train_loss = 1.8179391622543335, val_loss = 0.7269533276557922\n",
      "epoch n°1731 : train_loss = 1.8179800510406494, val_loss = 0.7264342308044434\n",
      "epoch n°1732 : train_loss = 1.818481683731079, val_loss = 0.7197458148002625\n",
      "epoch n°1733 : train_loss = 1.8099298477172852, val_loss = 0.7218254208564758\n",
      "epoch n°1734 : train_loss = 1.8231101036071777, val_loss = 0.724153995513916\n",
      "epoch n°1735 : train_loss = 1.8142526149749756, val_loss = 0.7201971411705017\n",
      "epoch n°1736 : train_loss = 1.8109679222106934, val_loss = 0.7217457294464111\n",
      "epoch n°1737 : train_loss = 1.8206475973129272, val_loss = 0.7215492129325867\n",
      "epoch n°1738 : train_loss = 1.8046892881393433, val_loss = 0.723161518573761\n",
      "epoch n°1739 : train_loss = 1.8056241273880005, val_loss = 0.717101514339447\n",
      "epoch n°1740 : train_loss = 1.8079336881637573, val_loss = 0.7171310782432556\n",
      "epoch n°1741 : train_loss = 1.814178228378296, val_loss = 0.7244099378585815\n",
      "epoch n°1742 : train_loss = 1.8181846141815186, val_loss = 0.7227082252502441\n",
      "epoch n°1743 : train_loss = 1.821738839149475, val_loss = 0.7211755514144897\n",
      "epoch n°1744 : train_loss = 1.8173236846923828, val_loss = 0.7176401615142822\n",
      "epoch n°1745 : train_loss = 1.8155922889709473, val_loss = 0.7216383218765259\n",
      "epoch n°1746 : train_loss = 1.814416527748108, val_loss = 0.7248773574829102\n",
      "epoch n°1747 : train_loss = 1.8290678262710571, val_loss = 0.7253841161727905\n",
      "epoch n°1748 : train_loss = 1.8092303276062012, val_loss = 0.7204818725585938\n",
      "epoch n°1749 : train_loss = 1.8150030374526978, val_loss = 0.7217474579811096\n",
      "epoch n°1750 : train_loss = 1.8135476112365723, val_loss = 0.7244902849197388\n",
      "epoch n°1751 : train_loss = 1.8015060424804688, val_loss = 0.7232331037521362\n",
      "epoch n°1752 : train_loss = 1.8117637634277344, val_loss = 0.725105345249176\n",
      "epoch n°1753 : train_loss = 1.8141616582870483, val_loss = 0.7237693071365356\n",
      "epoch n°1754 : train_loss = 1.819176435470581, val_loss = 0.7239825129508972\n",
      "epoch n°1755 : train_loss = 1.8125786781311035, val_loss = 0.7251474261283875\n",
      "epoch n°1756 : train_loss = 1.803543210029602, val_loss = 0.7259817123413086\n",
      "epoch n°1757 : train_loss = 1.8211756944656372, val_loss = 0.7243672013282776\n",
      "epoch n°1758 : train_loss = 1.8075422048568726, val_loss = 0.7201899290084839\n",
      "epoch n°1759 : train_loss = 1.8155544996261597, val_loss = 0.7235406041145325\n",
      "epoch n°1760 : train_loss = 1.8160593509674072, val_loss = 0.724254846572876\n",
      "epoch n°1761 : train_loss = 1.8078705072402954, val_loss = 0.7239683270454407\n",
      "epoch n°1762 : train_loss = 1.8125895261764526, val_loss = 0.7231093645095825\n",
      "epoch n°1763 : train_loss = 1.8211101293563843, val_loss = 0.7252345085144043\n",
      "epoch n°1764 : train_loss = 1.8101762533187866, val_loss = 0.7246906161308289\n",
      "epoch n°1765 : train_loss = 1.8157398700714111, val_loss = 0.7203168869018555\n",
      "epoch n°1766 : train_loss = 1.8168357610702515, val_loss = 0.716010332107544\n",
      "epoch n°1767 : train_loss = 1.8075568675994873, val_loss = 0.7235870957374573\n",
      "epoch n°1768 : train_loss = 1.8097906112670898, val_loss = 0.720831036567688\n",
      "epoch n°1769 : train_loss = 1.8201509714126587, val_loss = 0.7200808525085449\n",
      "epoch n°1770 : train_loss = 1.8122578859329224, val_loss = 0.71805340051651\n",
      "epoch n°1771 : train_loss = 1.8226505517959595, val_loss = 0.7237498760223389\n",
      "epoch n°1772 : train_loss = 1.8207002878189087, val_loss = 0.7174128890037537\n",
      "epoch n°1773 : train_loss = 1.806122899055481, val_loss = 0.7213386297225952\n",
      "epoch n°1774 : train_loss = 1.8083666563034058, val_loss = 0.7199245691299438\n",
      "epoch n°1775 : train_loss = 1.8143093585968018, val_loss = 0.721317708492279\n",
      "epoch n°1776 : train_loss = 1.8070502281188965, val_loss = 0.7177913784980774\n",
      "epoch n°1777 : train_loss = 1.8108023405075073, val_loss = 0.7213879227638245\n",
      "epoch n°1778 : train_loss = 1.8213962316513062, val_loss = 0.7200552821159363\n",
      "epoch n°1779 : train_loss = 1.8138610124588013, val_loss = 0.7221190929412842\n",
      "epoch n°1780 : train_loss = 1.8078266382217407, val_loss = 0.7236447930335999\n",
      "epoch n°1781 : train_loss = 1.813965082168579, val_loss = 0.7256999015808105\n",
      "epoch n°1782 : train_loss = 1.8181650638580322, val_loss = 0.7175101637840271\n",
      "epoch n°1783 : train_loss = 1.8033061027526855, val_loss = 0.7168565988540649\n",
      "epoch n°1784 : train_loss = 1.8152732849121094, val_loss = 0.7192932963371277\n",
      "epoch n°1785 : train_loss = 1.8145421743392944, val_loss = 0.71933913230896\n",
      "epoch n°1786 : train_loss = 1.8108083009719849, val_loss = 0.7189880609512329\n",
      "epoch n°1787 : train_loss = 1.8114261627197266, val_loss = 0.7195372581481934\n",
      "epoch n°1788 : train_loss = 1.8070902824401855, val_loss = 0.7197446227073669\n",
      "epoch n°1789 : train_loss = 1.8108340501785278, val_loss = 0.7214831709861755\n",
      "epoch n°1790 : train_loss = 1.8094220161437988, val_loss = 0.7232252955436707\n",
      "epoch n°1791 : train_loss = 1.8140894174575806, val_loss = 0.7193202376365662\n",
      "epoch n°1792 : train_loss = 1.8120882511138916, val_loss = 0.7181073427200317\n",
      "epoch n°1793 : train_loss = 1.812943935394287, val_loss = 0.7293849587440491\n",
      "epoch n°1794 : train_loss = 1.8093347549438477, val_loss = 0.720167875289917\n",
      "epoch n°1795 : train_loss = 1.8084907531738281, val_loss = 0.7220017910003662\n",
      "epoch n°1796 : train_loss = 1.8051059246063232, val_loss = 0.718443751335144\n",
      "epoch n°1797 : train_loss = 1.8096890449523926, val_loss = 0.7240002155303955\n",
      "epoch n°1798 : train_loss = 1.8118051290512085, val_loss = 0.718852698802948\n",
      "epoch n°1799 : train_loss = 1.8066251277923584, val_loss = 0.7205138206481934\n",
      "epoch n°1800 : train_loss = 1.810509443283081, val_loss = 0.7222905158996582\n",
      "epoch n°1801 : train_loss = 1.8042852878570557, val_loss = 0.7221317291259766\n",
      "epoch n°1802 : train_loss = 1.8222944736480713, val_loss = 0.7175978422164917\n",
      "epoch n°1803 : train_loss = 1.8085225820541382, val_loss = 0.723537027835846\n",
      "epoch n°1804 : train_loss = 1.8009923696517944, val_loss = 0.7227085828781128\n",
      "epoch n°1805 : train_loss = 1.8140618801116943, val_loss = 0.7225450277328491\n",
      "epoch n°1806 : train_loss = 1.7989799976348877, val_loss = 0.7183946967124939\n",
      "epoch n°1807 : train_loss = 1.8107742071151733, val_loss = 0.7195637226104736\n",
      "epoch n°1808 : train_loss = 1.7977570295333862, val_loss = 0.7203165292739868\n",
      "epoch n°1809 : train_loss = 1.8160927295684814, val_loss = 0.7184517979621887\n",
      "epoch n°1810 : train_loss = 1.7988002300262451, val_loss = 0.7184691429138184\n",
      "epoch n°1811 : train_loss = 1.8162269592285156, val_loss = 0.7236592769622803\n",
      "epoch n°1812 : train_loss = 1.822611927986145, val_loss = 0.7224292159080505\n",
      "epoch n°1813 : train_loss = 1.8155443668365479, val_loss = 0.7171734571456909\n",
      "epoch n°1814 : train_loss = 1.8094407320022583, val_loss = 0.7177393436431885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1815 : train_loss = 1.801466464996338, val_loss = 0.7238468527793884\n",
      "epoch n°1816 : train_loss = 1.8055334091186523, val_loss = 0.7219330072402954\n",
      "epoch n°1817 : train_loss = 1.803584098815918, val_loss = 0.7226505279541016\n",
      "epoch n°1818 : train_loss = 1.8093116283416748, val_loss = 0.7216057777404785\n",
      "epoch n°1819 : train_loss = 1.8083584308624268, val_loss = 0.7216252088546753\n",
      "epoch n°1820 : train_loss = 1.8131637573242188, val_loss = 0.7170388698577881\n",
      "epoch n°1821 : train_loss = 1.8083460330963135, val_loss = 0.7199835181236267\n",
      "epoch n°1822 : train_loss = 1.8043744564056396, val_loss = 0.7242471575737\n",
      "epoch n°1823 : train_loss = 1.7950907945632935, val_loss = 0.721058189868927\n",
      "epoch n°1824 : train_loss = 1.811021327972412, val_loss = 0.7236167788505554\n",
      "epoch n°1825 : train_loss = 1.8136162757873535, val_loss = 0.7202487587928772\n",
      "epoch n°1826 : train_loss = 1.808007001876831, val_loss = 0.7261991500854492\n",
      "epoch n°1827 : train_loss = 1.8043683767318726, val_loss = 0.719192624092102\n",
      "epoch n°1828 : train_loss = 1.8104922771453857, val_loss = 0.7220441699028015\n",
      "epoch n°1829 : train_loss = 1.8103505373001099, val_loss = 0.7223008871078491\n",
      "epoch n°1830 : train_loss = 1.8041579723358154, val_loss = 0.7216199636459351\n",
      "epoch n°1831 : train_loss = 1.8075981140136719, val_loss = 0.7208676338195801\n",
      "epoch n°1832 : train_loss = 1.8018473386764526, val_loss = 0.7180706858634949\n",
      "epoch n°1833 : train_loss = 1.816394567489624, val_loss = 0.721928060054779\n",
      "epoch n°1834 : train_loss = 1.8033958673477173, val_loss = 0.7199321389198303\n",
      "epoch n°1835 : train_loss = 1.809592604637146, val_loss = 0.7222729921340942\n",
      "epoch n°1836 : train_loss = 1.8079490661621094, val_loss = 0.7196465134620667\n",
      "epoch n°1837 : train_loss = 1.8099408149719238, val_loss = 0.7229716777801514\n",
      "epoch n°1838 : train_loss = 1.8094780445098877, val_loss = 0.7171976566314697\n",
      "epoch n°1839 : train_loss = 1.8090413808822632, val_loss = 0.7206031084060669\n",
      "epoch n°1840 : train_loss = 1.8098812103271484, val_loss = 0.7212122082710266\n",
      "epoch n°1841 : train_loss = 1.8120447397232056, val_loss = 0.7209153175354004\n",
      "epoch n°1842 : train_loss = 1.8180301189422607, val_loss = 0.7185753583908081\n",
      "epoch n°1843 : train_loss = 1.8043017387390137, val_loss = 0.7239410281181335\n",
      "epoch n°1844 : train_loss = 1.812121868133545, val_loss = 0.7183377742767334\n",
      "epoch n°1845 : train_loss = 1.814314842224121, val_loss = 0.7219877243041992\n",
      "epoch n°1846 : train_loss = 1.8099892139434814, val_loss = 0.7221328616142273\n",
      "epoch n°1847 : train_loss = 1.8097987174987793, val_loss = 0.7236570119857788\n",
      "epoch n°1848 : train_loss = 1.797123908996582, val_loss = 0.7216182351112366\n",
      "epoch n°1849 : train_loss = 1.8170174360275269, val_loss = 0.7254239916801453\n",
      "epoch n°1850 : train_loss = 1.8086011409759521, val_loss = 0.723456084728241\n",
      "epoch n°1851 : train_loss = 1.8139628171920776, val_loss = 0.7253778576850891\n",
      "epoch n°1852 : train_loss = 1.808393955230713, val_loss = 0.7268469333648682\n",
      "epoch n°1853 : train_loss = 1.815006971359253, val_loss = 0.7228742837905884\n",
      "epoch n°1854 : train_loss = 1.813888669013977, val_loss = 0.7217305898666382\n",
      "epoch n°1855 : train_loss = 1.8085867166519165, val_loss = 0.7240583896636963\n",
      "epoch n°1856 : train_loss = 1.806342363357544, val_loss = 0.7187599539756775\n",
      "epoch n°1857 : train_loss = 1.807623028755188, val_loss = 0.7192040085792542\n",
      "epoch n°1858 : train_loss = 1.7963474988937378, val_loss = 0.7198306918144226\n",
      "epoch n°1859 : train_loss = 1.8029972314834595, val_loss = 0.7239131927490234\n",
      "epoch n°1860 : train_loss = 1.8029742240905762, val_loss = 0.7193232774734497\n",
      "epoch n°1861 : train_loss = 1.8046936988830566, val_loss = 0.7221711874008179\n",
      "epoch n°1862 : train_loss = 1.7931582927703857, val_loss = 0.717398464679718\n",
      "epoch n°1863 : train_loss = 1.797472357749939, val_loss = 0.7171429395675659\n",
      "epoch n°1864 : train_loss = 1.8053505420684814, val_loss = 0.7195347547531128\n",
      "epoch n°1865 : train_loss = 1.798526644706726, val_loss = 0.7211637496948242\n",
      "epoch n°1866 : train_loss = 1.8188011646270752, val_loss = 0.7207555174827576\n",
      "epoch n°1867 : train_loss = 1.8059098720550537, val_loss = 0.7265346646308899\n",
      "epoch n°1868 : train_loss = 1.8010987043380737, val_loss = 0.7225039005279541\n",
      "epoch n°1869 : train_loss = 1.8014293909072876, val_loss = 0.7243970036506653\n",
      "epoch n°1870 : train_loss = 1.8034520149230957, val_loss = 0.7201828956604004\n",
      "epoch n°1871 : train_loss = 1.8118648529052734, val_loss = 0.7202251553535461\n",
      "epoch n°1872 : train_loss = 1.8010891675949097, val_loss = 0.7222116589546204\n",
      "epoch n°1873 : train_loss = 1.8015309572219849, val_loss = 0.7209303379058838\n",
      "epoch n°1874 : train_loss = 1.81245756149292, val_loss = 0.7188194990158081\n",
      "epoch n°1875 : train_loss = 1.8035986423492432, val_loss = 0.7198500037193298\n",
      "epoch n°1876 : train_loss = 1.8113912343978882, val_loss = 0.720045268535614\n",
      "epoch n°1877 : train_loss = 1.8134047985076904, val_loss = 0.7207327485084534\n",
      "epoch n°1878 : train_loss = 1.807978630065918, val_loss = 0.723211944103241\n",
      "epoch n°1879 : train_loss = 1.803844928741455, val_loss = 0.7196012735366821\n",
      "epoch n°1880 : train_loss = 1.8061730861663818, val_loss = 0.7237491607666016\n",
      "epoch n°1881 : train_loss = 1.8097248077392578, val_loss = 0.7197031378746033\n",
      "epoch n°1882 : train_loss = 1.8096340894699097, val_loss = 0.7238679528236389\n",
      "epoch n°1883 : train_loss = 1.8101094961166382, val_loss = 0.7187936305999756\n",
      "epoch n°1884 : train_loss = 1.8015235662460327, val_loss = 0.7198374271392822\n",
      "epoch n°1885 : train_loss = 1.800220012664795, val_loss = 0.7185390591621399\n",
      "epoch n°1886 : train_loss = 1.8113075494766235, val_loss = 0.7199583053588867\n",
      "epoch n°1887 : train_loss = 1.8063162565231323, val_loss = 0.7198871374130249\n",
      "epoch n°1888 : train_loss = 1.8006011247634888, val_loss = 0.7240066528320312\n",
      "epoch n°1889 : train_loss = 1.8045481443405151, val_loss = 0.7207581400871277\n",
      "epoch n°1890 : train_loss = 1.8091472387313843, val_loss = 0.7173601984977722\n",
      "epoch n°1891 : train_loss = 1.8137288093566895, val_loss = 0.7225109338760376\n",
      "epoch n°1892 : train_loss = 1.7963743209838867, val_loss = 0.7192283868789673\n",
      "epoch n°1893 : train_loss = 1.803558349609375, val_loss = 0.7195430397987366\n",
      "epoch n°1894 : train_loss = 1.8041613101959229, val_loss = 0.7203673720359802\n",
      "epoch n°1895 : train_loss = 1.8025872707366943, val_loss = 0.72046959400177\n",
      "epoch n°1896 : train_loss = 1.8049689531326294, val_loss = 0.7240103483200073\n",
      "epoch n°1897 : train_loss = 1.8002846240997314, val_loss = 0.7179129719734192\n",
      "epoch n°1898 : train_loss = 1.7995842695236206, val_loss = 0.7202891707420349\n",
      "epoch n°1899 : train_loss = 1.8088297843933105, val_loss = 0.7186962366104126\n",
      "epoch n°1900 : train_loss = 1.8040050268173218, val_loss = 0.7193275690078735\n",
      "epoch n°1901 : train_loss = 1.7945069074630737, val_loss = 0.7272337079048157\n",
      "epoch n°1902 : train_loss = 1.8005155324935913, val_loss = 0.7235163450241089\n",
      "epoch n°1903 : train_loss = 1.808246374130249, val_loss = 0.7241378426551819\n",
      "epoch n°1904 : train_loss = 1.8022712469100952, val_loss = 0.7224915623664856\n",
      "epoch n°1905 : train_loss = 1.7953894138336182, val_loss = 0.7218300700187683\n",
      "epoch n°1906 : train_loss = 1.8151390552520752, val_loss = 0.7239283323287964\n",
      "epoch n°1907 : train_loss = 1.7995859384536743, val_loss = 0.7163605093955994\n",
      "epoch n°1908 : train_loss = 1.7967547178268433, val_loss = 0.715877890586853\n",
      "epoch n°1909 : train_loss = 1.8107860088348389, val_loss = 0.720315158367157\n",
      "epoch n°1910 : train_loss = 1.7976417541503906, val_loss = 0.7178817987442017\n",
      "epoch n°1911 : train_loss = 1.8153319358825684, val_loss = 0.7223392724990845\n",
      "epoch n°1912 : train_loss = 1.8028733730316162, val_loss = 0.717840850353241\n",
      "epoch n°1913 : train_loss = 1.8114840984344482, val_loss = 0.7250311970710754\n",
      "epoch n°1914 : train_loss = 1.800302505493164, val_loss = 0.7238556742668152\n",
      "epoch n°1915 : train_loss = 1.8011126518249512, val_loss = 0.7173633575439453\n",
      "epoch n°1916 : train_loss = 1.8124744892120361, val_loss = 0.7179427146911621\n",
      "epoch n°1917 : train_loss = 1.8095042705535889, val_loss = 0.7236594557762146\n",
      "epoch n°1918 : train_loss = 1.8023816347122192, val_loss = 0.7247840762138367\n",
      "epoch n°1919 : train_loss = 1.8076896667480469, val_loss = 0.7227787375450134\n",
      "epoch n°1920 : train_loss = 1.8005481958389282, val_loss = 0.7207761406898499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°1921 : train_loss = 1.801788330078125, val_loss = 0.7196457386016846\n",
      "epoch n°1922 : train_loss = 1.8016853332519531, val_loss = 0.7160854339599609\n",
      "epoch n°1923 : train_loss = 1.8191341161727905, val_loss = 0.7239432334899902\n",
      "epoch n°1924 : train_loss = 1.8018218278884888, val_loss = 0.7190432548522949\n",
      "epoch n°1925 : train_loss = 1.8010092973709106, val_loss = 0.7178868651390076\n",
      "epoch n°1926 : train_loss = 1.8019208908081055, val_loss = 0.7160611748695374\n",
      "epoch n°1927 : train_loss = 1.8074852228164673, val_loss = 0.7150001525878906\n",
      "epoch n°1928 : train_loss = 1.8156378269195557, val_loss = 0.7193757891654968\n",
      "epoch n°1929 : train_loss = 1.8040266036987305, val_loss = 0.7208951115608215\n",
      "epoch n°1930 : train_loss = 1.7978553771972656, val_loss = 0.7235921025276184\n",
      "epoch n°1931 : train_loss = 1.8082700967788696, val_loss = 0.7199982404708862\n",
      "epoch n°1932 : train_loss = 1.8037877082824707, val_loss = 0.7160200476646423\n",
      "epoch n°1933 : train_loss = 1.8069984912872314, val_loss = 0.7190908193588257\n",
      "epoch n°1934 : train_loss = 1.805444598197937, val_loss = 0.7195389270782471\n",
      "epoch n°1935 : train_loss = 1.8115108013153076, val_loss = 0.7245900630950928\n",
      "epoch n°1936 : train_loss = 1.8109363317489624, val_loss = 0.7179563045501709\n",
      "epoch n°1937 : train_loss = 1.8186371326446533, val_loss = 0.7256090641021729\n",
      "epoch n°1938 : train_loss = 1.8083518743515015, val_loss = 0.7217431664466858\n",
      "epoch n°1939 : train_loss = 1.7978771924972534, val_loss = 0.7265955209732056\n",
      "epoch n°1940 : train_loss = 1.8030073642730713, val_loss = 0.7233022451400757\n",
      "epoch n°1941 : train_loss = 1.8130927085876465, val_loss = 0.7208024859428406\n",
      "epoch n°1942 : train_loss = 1.8052942752838135, val_loss = 0.7183793187141418\n",
      "epoch n°1943 : train_loss = 1.8047373294830322, val_loss = 0.721889317035675\n",
      "epoch n°1944 : train_loss = 1.8080779314041138, val_loss = 0.7142199873924255\n",
      "epoch n°1945 : train_loss = 1.8115307092666626, val_loss = 0.724098801612854\n",
      "epoch n°1946 : train_loss = 1.8156228065490723, val_loss = 0.718949019908905\n",
      "epoch n°1947 : train_loss = 1.8195375204086304, val_loss = 0.7194570302963257\n",
      "epoch n°1948 : train_loss = 1.8017687797546387, val_loss = 0.7235826253890991\n",
      "epoch n°1949 : train_loss = 1.799135684967041, val_loss = 0.7214899659156799\n",
      "epoch n°1950 : train_loss = 1.8009076118469238, val_loss = 0.7189341187477112\n",
      "epoch n°1951 : train_loss = 1.8007428646087646, val_loss = 0.7225945591926575\n",
      "epoch n°1952 : train_loss = 1.8154157400131226, val_loss = 0.7127300500869751\n",
      "epoch n°1953 : train_loss = 1.8096652030944824, val_loss = 0.7216737270355225\n",
      "epoch n°1954 : train_loss = 1.8062094449996948, val_loss = 0.7205338478088379\n",
      "epoch n°1955 : train_loss = 1.8070605993270874, val_loss = 0.7296079397201538\n",
      "epoch n°1956 : train_loss = 1.806976556777954, val_loss = 0.7158799171447754\n",
      "epoch n°1957 : train_loss = 1.8017675876617432, val_loss = 0.7218472957611084\n",
      "epoch n°1958 : train_loss = 1.807550072669983, val_loss = 0.7231844663619995\n",
      "epoch n°1959 : train_loss = 1.8059414625167847, val_loss = 0.715366780757904\n",
      "epoch n°1960 : train_loss = 1.8012709617614746, val_loss = 0.720283567905426\n",
      "epoch n°1961 : train_loss = 1.8010077476501465, val_loss = 0.7241145372390747\n",
      "epoch n°1962 : train_loss = 1.8111300468444824, val_loss = 0.7186587452888489\n",
      "epoch n°1963 : train_loss = 1.8035049438476562, val_loss = 0.7241821885108948\n",
      "epoch n°1964 : train_loss = 1.8140883445739746, val_loss = 0.7271220684051514\n",
      "epoch n°1965 : train_loss = 1.8022758960723877, val_loss = 0.7208786010742188\n",
      "epoch n°1966 : train_loss = 1.8026189804077148, val_loss = 0.7166542410850525\n",
      "epoch n°1967 : train_loss = 1.802166223526001, val_loss = 0.7177940607070923\n",
      "epoch n°1968 : train_loss = 1.8146448135375977, val_loss = 0.7211944460868835\n",
      "epoch n°1969 : train_loss = 1.8097532987594604, val_loss = 0.7211825251579285\n",
      "epoch n°1970 : train_loss = 1.806928277015686, val_loss = 0.7216560244560242\n",
      "epoch n°1971 : train_loss = 1.8042490482330322, val_loss = 0.724376916885376\n",
      "epoch n°1972 : train_loss = 1.8152204751968384, val_loss = 0.7215192914009094\n",
      "epoch n°1973 : train_loss = 1.8012731075286865, val_loss = 0.7228839993476868\n",
      "epoch n°1974 : train_loss = 1.8151121139526367, val_loss = 0.7212260961532593\n",
      "epoch n°1975 : train_loss = 1.8062489032745361, val_loss = 0.7207366228103638\n",
      "epoch n°1976 : train_loss = 1.8098069429397583, val_loss = 0.7190065979957581\n",
      "epoch n°1977 : train_loss = 1.8104760646820068, val_loss = 0.7178090810775757\n",
      "epoch n°1978 : train_loss = 1.8035047054290771, val_loss = 0.715705931186676\n",
      "epoch n°1979 : train_loss = 1.8059277534484863, val_loss = 0.7226230502128601\n",
      "epoch n°1980 : train_loss = 1.8096599578857422, val_loss = 0.7216744422912598\n",
      "epoch n°1981 : train_loss = 1.7983232736587524, val_loss = 0.7224730253219604\n",
      "epoch n°1982 : train_loss = 1.8024173974990845, val_loss = 0.7229896187782288\n",
      "epoch n°1983 : train_loss = 1.8077853918075562, val_loss = 0.7215743064880371\n",
      "epoch n°1984 : train_loss = 1.8085683584213257, val_loss = 0.718208372592926\n",
      "epoch n°1985 : train_loss = 1.802979826927185, val_loss = 0.7186284065246582\n",
      "epoch n°1986 : train_loss = 1.8017122745513916, val_loss = 0.7224041819572449\n",
      "epoch n°1987 : train_loss = 1.80988347530365, val_loss = 0.7228115797042847\n",
      "epoch n°1988 : train_loss = 1.8120402097702026, val_loss = 0.7209181189537048\n",
      "epoch n°1989 : train_loss = 1.8002893924713135, val_loss = 0.7208479642868042\n",
      "epoch n°1990 : train_loss = 1.7984602451324463, val_loss = 0.727317750453949\n",
      "epoch n°1991 : train_loss = 1.792559027671814, val_loss = 0.717841386795044\n",
      "epoch n°1992 : train_loss = 1.8104240894317627, val_loss = 0.7240737676620483\n",
      "epoch n°1993 : train_loss = 1.788750410079956, val_loss = 0.725012481212616\n",
      "epoch n°1994 : train_loss = 1.7991057634353638, val_loss = 0.7217893600463867\n",
      "epoch n°1995 : train_loss = 1.7972594499588013, val_loss = 0.7208642363548279\n",
      "epoch n°1996 : train_loss = 1.8099815845489502, val_loss = 0.7172069549560547\n",
      "epoch n°1997 : train_loss = 1.7948520183563232, val_loss = 0.7210778594017029\n",
      "epoch n°1998 : train_loss = 1.7941725254058838, val_loss = 0.7228925824165344\n",
      "epoch n°1999 : train_loss = 1.7903172969818115, val_loss = 0.7191882729530334\n",
      "epoch n°2000 : train_loss = 1.8076337575912476, val_loss = 0.7226473093032837\n",
      "epoch n°2001 : train_loss = 1.7989522218704224, val_loss = 0.721764326095581\n",
      "epoch n°2002 : train_loss = 1.812707543373108, val_loss = 0.720780611038208\n",
      "epoch n°2003 : train_loss = 1.8036361932754517, val_loss = 0.7240357398986816\n",
      "epoch n°2004 : train_loss = 1.8087533712387085, val_loss = 0.7146033644676208\n",
      "epoch n°2005 : train_loss = 1.8058414459228516, val_loss = 0.7243818044662476\n",
      "epoch n°2006 : train_loss = 1.8034271001815796, val_loss = 0.7197317481040955\n",
      "epoch n°2007 : train_loss = 1.8023312091827393, val_loss = 0.7231720089912415\n",
      "epoch n°2008 : train_loss = 1.8103699684143066, val_loss = 0.7233844995498657\n",
      "epoch n°2009 : train_loss = 1.8056530952453613, val_loss = 0.7215129733085632\n",
      "epoch n°2010 : train_loss = 1.804054617881775, val_loss = 0.7180009484291077\n",
      "epoch n°2011 : train_loss = 1.7996597290039062, val_loss = 0.7177943587303162\n",
      "epoch n°2012 : train_loss = 1.8034746646881104, val_loss = 0.7186715006828308\n",
      "epoch n°2013 : train_loss = 1.8042633533477783, val_loss = 0.7221066355705261\n",
      "epoch n°2014 : train_loss = 1.8110947608947754, val_loss = 0.7197490334510803\n",
      "epoch n°2015 : train_loss = 1.8145391941070557, val_loss = 0.7221835255622864\n",
      "epoch n°2016 : train_loss = 1.8151934146881104, val_loss = 0.7268744707107544\n",
      "epoch n°2017 : train_loss = 1.8051693439483643, val_loss = 0.7256240248680115\n",
      "epoch n°2018 : train_loss = 1.8190869092941284, val_loss = 0.7227358818054199\n",
      "epoch n°2019 : train_loss = 1.8040788173675537, val_loss = 0.7179600596427917\n",
      "epoch n°2020 : train_loss = 1.8083276748657227, val_loss = 0.7237093448638916\n",
      "epoch n°2021 : train_loss = 1.8095299005508423, val_loss = 0.7236186265945435\n",
      "epoch n°2022 : train_loss = 1.8072675466537476, val_loss = 0.7245301008224487\n",
      "epoch n°2023 : train_loss = 1.8013883829116821, val_loss = 0.717851996421814\n",
      "epoch n°2024 : train_loss = 1.7906346321105957, val_loss = 0.7177836298942566\n",
      "epoch n°2025 : train_loss = 1.8079928159713745, val_loss = 0.7259814143180847\n",
      "epoch n°2026 : train_loss = 1.8057140111923218, val_loss = 0.7252503633499146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2027 : train_loss = 1.7993937730789185, val_loss = 0.7194643616676331\n",
      "epoch n°2028 : train_loss = 1.8151600360870361, val_loss = 0.7202967405319214\n",
      "epoch n°2029 : train_loss = 1.8134690523147583, val_loss = 0.7249733805656433\n",
      "epoch n°2030 : train_loss = 1.807922601699829, val_loss = 0.7158495187759399\n",
      "epoch n°2031 : train_loss = 1.80728280544281, val_loss = 0.7169585227966309\n",
      "epoch n°2032 : train_loss = 1.8471606969833374, val_loss = 0.7333796620368958\n",
      "epoch n°2033 : train_loss = 1.8488551378250122, val_loss = 0.7262967228889465\n",
      "epoch n°2034 : train_loss = 1.8400510549545288, val_loss = 0.729530394077301\n",
      "epoch n°2035 : train_loss = 1.8373793363571167, val_loss = 0.7300116419792175\n",
      "epoch n°2036 : train_loss = 1.8367122411727905, val_loss = 0.7329192757606506\n",
      "epoch n°2037 : train_loss = 1.834838628768921, val_loss = 0.7270448207855225\n",
      "epoch n°2038 : train_loss = 1.839640498161316, val_loss = 0.7200481295585632\n",
      "epoch n°2039 : train_loss = 1.84990656375885, val_loss = 0.7230947613716125\n",
      "epoch n°2040 : train_loss = 1.8319098949432373, val_loss = 0.7244552373886108\n",
      "epoch n°2041 : train_loss = 1.843938946723938, val_loss = 0.7245721220970154\n",
      "epoch n°2042 : train_loss = 1.8274222612380981, val_loss = 0.726739764213562\n",
      "epoch n°2043 : train_loss = 1.83163321018219, val_loss = 0.7225244641304016\n",
      "epoch n°2044 : train_loss = 1.850378155708313, val_loss = 0.7284035086631775\n",
      "epoch n°2045 : train_loss = 1.8441060781478882, val_loss = 0.7293746471405029\n",
      "epoch n°2046 : train_loss = 1.8392012119293213, val_loss = 0.7284688949584961\n",
      "epoch n°2047 : train_loss = 1.842659592628479, val_loss = 0.7246865034103394\n",
      "epoch n°2048 : train_loss = 1.8503257036209106, val_loss = 0.7309015989303589\n",
      "epoch n°2049 : train_loss = 1.8376520872116089, val_loss = 0.7245619297027588\n",
      "epoch n°2050 : train_loss = 1.8488550186157227, val_loss = 0.7242487668991089\n",
      "epoch n°2051 : train_loss = 1.8375948667526245, val_loss = 0.7282496094703674\n",
      "epoch n°2052 : train_loss = 1.8457691669464111, val_loss = 0.7291845679283142\n",
      "epoch n°2053 : train_loss = 1.843216061592102, val_loss = 0.7235888838768005\n",
      "epoch n°2054 : train_loss = 1.8368736505508423, val_loss = 0.726540744304657\n",
      "epoch n°2055 : train_loss = 1.8405876159667969, val_loss = 0.7283137440681458\n",
      "epoch n°2056 : train_loss = 1.839968204498291, val_loss = 0.7257659435272217\n",
      "epoch n°2057 : train_loss = 1.8499630689620972, val_loss = 0.725134551525116\n",
      "epoch n°2058 : train_loss = 1.8369240760803223, val_loss = 0.727567195892334\n",
      "epoch n°2059 : train_loss = 1.8516507148742676, val_loss = 0.7268402576446533\n",
      "epoch n°2060 : train_loss = 1.8466075658798218, val_loss = 0.7273374795913696\n",
      "epoch n°2061 : train_loss = 1.8393915891647339, val_loss = 0.7278488278388977\n",
      "epoch n°2062 : train_loss = 1.845686912536621, val_loss = 0.7222957015037537\n",
      "epoch n°2063 : train_loss = 1.8518924713134766, val_loss = 0.7223077416419983\n",
      "epoch n°2064 : train_loss = 1.8440128564834595, val_loss = 0.7255546450614929\n",
      "epoch n°2065 : train_loss = 1.8521387577056885, val_loss = 0.7247797846794128\n",
      "epoch n°2066 : train_loss = 1.8469327688217163, val_loss = 0.7251623272895813\n",
      "epoch n°2067 : train_loss = 1.841049075126648, val_loss = 0.7252113223075867\n",
      "epoch n°2068 : train_loss = 1.8408814668655396, val_loss = 0.7249600291252136\n",
      "epoch n°2069 : train_loss = 1.842657208442688, val_loss = 0.7211625576019287\n",
      "epoch n°2070 : train_loss = 1.8656851053237915, val_loss = 0.7271438241004944\n",
      "epoch n°2071 : train_loss = 1.8503714799880981, val_loss = 0.7282657027244568\n",
      "epoch n°2072 : train_loss = 1.846090316772461, val_loss = 0.721902072429657\n",
      "epoch n°2073 : train_loss = 1.8422458171844482, val_loss = 0.7258586883544922\n",
      "epoch n°2074 : train_loss = 1.84846830368042, val_loss = 0.7258607745170593\n",
      "epoch n°2075 : train_loss = 1.843748688697815, val_loss = 0.7247233390808105\n",
      "epoch n°2076 : train_loss = 1.8347713947296143, val_loss = 0.7233288288116455\n",
      "epoch n°2077 : train_loss = 1.838261604309082, val_loss = 0.7256600856781006\n",
      "epoch n°2078 : train_loss = 1.8571470975875854, val_loss = 0.7309418320655823\n",
      "epoch n°2079 : train_loss = 1.837642788887024, val_loss = 0.7263335585594177\n",
      "epoch n°2080 : train_loss = 1.8482475280761719, val_loss = 0.728023111820221\n",
      "epoch n°2081 : train_loss = 1.8401674032211304, val_loss = 0.7288306951522827\n",
      "epoch n°2082 : train_loss = 1.8432270288467407, val_loss = 0.7242802977561951\n",
      "epoch n°2083 : train_loss = 1.8486298322677612, val_loss = 0.7280840277671814\n",
      "epoch n°2084 : train_loss = 1.8511008024215698, val_loss = 0.7291402816772461\n",
      "epoch n°2085 : train_loss = 1.8396656513214111, val_loss = 0.7292768359184265\n",
      "epoch n°2086 : train_loss = 1.8388622999191284, val_loss = 0.7259554266929626\n",
      "epoch n°2087 : train_loss = 1.8429433107376099, val_loss = 0.725743293762207\n",
      "epoch n°2088 : train_loss = 1.8476688861846924, val_loss = 0.7261025905609131\n",
      "epoch n°2089 : train_loss = 1.8453675508499146, val_loss = 0.7243716716766357\n",
      "epoch n°2090 : train_loss = 1.8432279825210571, val_loss = 0.7224133014678955\n",
      "epoch n°2091 : train_loss = 1.8581082820892334, val_loss = 0.7266682982444763\n",
      "epoch n°2092 : train_loss = 1.8471890687942505, val_loss = 0.7254833579063416\n",
      "epoch n°2093 : train_loss = 1.8530879020690918, val_loss = 0.7272825241088867\n",
      "epoch n°2094 : train_loss = 1.8537486791610718, val_loss = 0.7215470671653748\n",
      "epoch n°2095 : train_loss = 1.846853256225586, val_loss = 0.7237330079078674\n",
      "epoch n°2096 : train_loss = 1.8441907167434692, val_loss = 0.7239133715629578\n",
      "epoch n°2097 : train_loss = 1.8434197902679443, val_loss = 0.7257524728775024\n",
      "epoch n°2098 : train_loss = 1.8531601428985596, val_loss = 0.7263324856758118\n",
      "epoch n°2099 : train_loss = 1.8442145586013794, val_loss = 0.7303456664085388\n",
      "epoch n°2100 : train_loss = 1.857633113861084, val_loss = 0.72621089220047\n",
      "epoch n°2101 : train_loss = 1.8485636711120605, val_loss = 0.7274733781814575\n",
      "epoch n°2102 : train_loss = 1.8472609519958496, val_loss = 0.727139413356781\n",
      "epoch n°2103 : train_loss = 1.8457484245300293, val_loss = 0.7289637327194214\n",
      "epoch n°2104 : train_loss = 1.839028000831604, val_loss = 0.7259995937347412\n",
      "epoch n°2105 : train_loss = 1.8440892696380615, val_loss = 0.72479248046875\n",
      "epoch n°2106 : train_loss = 1.8436052799224854, val_loss = 0.7310723662376404\n",
      "epoch n°2107 : train_loss = 1.8496060371398926, val_loss = 0.7216055989265442\n",
      "epoch n°2108 : train_loss = 1.8497923612594604, val_loss = 0.7351750731468201\n",
      "epoch n°2109 : train_loss = 1.8500479459762573, val_loss = 0.725501537322998\n",
      "epoch n°2110 : train_loss = 1.8407162427902222, val_loss = 0.7182551622390747\n",
      "epoch n°2111 : train_loss = 1.8538787364959717, val_loss = 0.7263623476028442\n",
      "epoch n°2112 : train_loss = 1.8516358137130737, val_loss = 0.7260737419128418\n",
      "epoch n°2113 : train_loss = 1.855033278465271, val_loss = 0.7263430953025818\n",
      "epoch n°2114 : train_loss = 1.8404024839401245, val_loss = 0.7215568423271179\n",
      "epoch n°2115 : train_loss = 1.8505024909973145, val_loss = 0.728503942489624\n",
      "epoch n°2116 : train_loss = 1.8436683416366577, val_loss = 0.7229913473129272\n",
      "epoch n°2117 : train_loss = 1.8415335416793823, val_loss = 0.724849283695221\n",
      "epoch n°2118 : train_loss = 1.8473976850509644, val_loss = 0.7264534831047058\n",
      "epoch n°2119 : train_loss = 1.8359237909317017, val_loss = 0.7280422449111938\n",
      "epoch n°2120 : train_loss = 1.8332624435424805, val_loss = 0.7285217046737671\n",
      "epoch n°2121 : train_loss = 1.8531837463378906, val_loss = 0.7273996472358704\n",
      "epoch n°2122 : train_loss = 1.8387207984924316, val_loss = 0.7276031970977783\n",
      "epoch n°2123 : train_loss = 1.8455705642700195, val_loss = 0.7226517200469971\n",
      "epoch n°2124 : train_loss = 1.8389778137207031, val_loss = 0.7216606736183167\n",
      "epoch n°2125 : train_loss = 1.85688054561615, val_loss = 0.7253189086914062\n",
      "epoch n°2126 : train_loss = 1.8432691097259521, val_loss = 0.7256714105606079\n",
      "epoch n°2127 : train_loss = 1.8393867015838623, val_loss = 0.7320207953453064\n",
      "epoch n°2128 : train_loss = 1.8357328176498413, val_loss = 0.7293631434440613\n",
      "epoch n°2129 : train_loss = 1.8706172704696655, val_loss = 0.7255362272262573\n",
      "epoch n°2130 : train_loss = 1.8528246879577637, val_loss = 0.7249905467033386\n",
      "epoch n°2131 : train_loss = 1.8482097387313843, val_loss = 0.7306218147277832\n",
      "epoch n°2132 : train_loss = 1.8483538627624512, val_loss = 0.7265121936798096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2133 : train_loss = 1.8560786247253418, val_loss = 0.7238050699234009\n",
      "epoch n°2134 : train_loss = 1.8348894119262695, val_loss = 0.7248779535293579\n",
      "epoch n°2135 : train_loss = 1.8482978343963623, val_loss = 0.7265830039978027\n",
      "epoch n°2136 : train_loss = 1.8463964462280273, val_loss = 0.7247283458709717\n",
      "epoch n°2137 : train_loss = 1.8476002216339111, val_loss = 0.7303136587142944\n",
      "epoch n°2138 : train_loss = 1.8378667831420898, val_loss = 0.7283325791358948\n",
      "epoch n°2139 : train_loss = 1.8356527090072632, val_loss = 0.7274615168571472\n",
      "epoch n°2140 : train_loss = 1.8435817956924438, val_loss = 0.7257554531097412\n",
      "epoch n°2141 : train_loss = 1.8496768474578857, val_loss = 0.7245099544525146\n",
      "epoch n°2142 : train_loss = 1.838172197341919, val_loss = 0.7321259379386902\n",
      "epoch n°2143 : train_loss = 1.8562498092651367, val_loss = 0.7235520482063293\n",
      "epoch n°2144 : train_loss = 1.8458056449890137, val_loss = 0.7258584499359131\n",
      "epoch n°2145 : train_loss = 1.8436764478683472, val_loss = 0.7240163087844849\n",
      "epoch n°2146 : train_loss = 1.841054081916809, val_loss = 0.7293957471847534\n",
      "epoch n°2147 : train_loss = 1.8517889976501465, val_loss = 0.7221060991287231\n",
      "epoch n°2148 : train_loss = 1.8471145629882812, val_loss = 0.7215356230735779\n",
      "epoch n°2149 : train_loss = 1.8457210063934326, val_loss = 0.7214277982711792\n",
      "epoch n°2150 : train_loss = 1.8555995225906372, val_loss = 0.7238578200340271\n",
      "epoch n°2151 : train_loss = 1.848367691040039, val_loss = 0.7193280458450317\n",
      "epoch n°2152 : train_loss = 1.8408278226852417, val_loss = 0.7302013635635376\n",
      "epoch n°2153 : train_loss = 1.8401200771331787, val_loss = 0.72584468126297\n",
      "epoch n°2154 : train_loss = 1.8384909629821777, val_loss = 0.7258025407791138\n",
      "epoch n°2155 : train_loss = 1.8488624095916748, val_loss = 0.7271794080734253\n",
      "epoch n°2156 : train_loss = 1.8410265445709229, val_loss = 0.7279295325279236\n",
      "epoch n°2157 : train_loss = 1.8483436107635498, val_loss = 0.7213842868804932\n",
      "epoch n°2158 : train_loss = 1.854189395904541, val_loss = 0.7241238951683044\n",
      "epoch n°2159 : train_loss = 1.8402130603790283, val_loss = 0.7252005338668823\n",
      "epoch n°2160 : train_loss = 1.8404611349105835, val_loss = 0.7274240851402283\n",
      "epoch n°2161 : train_loss = 1.847793459892273, val_loss = 0.7234029769897461\n",
      "epoch n°2162 : train_loss = 1.8456734418869019, val_loss = 0.7200270891189575\n",
      "epoch n°2163 : train_loss = 1.8498231172561646, val_loss = 0.7275604009628296\n",
      "epoch n°2164 : train_loss = 1.8468047380447388, val_loss = 0.7340197563171387\n",
      "epoch n°2165 : train_loss = 1.8601266145706177, val_loss = 0.7250959873199463\n",
      "epoch n°2166 : train_loss = 1.8501853942871094, val_loss = 0.7257027626037598\n",
      "epoch n°2167 : train_loss = 1.8397140502929688, val_loss = 0.7226492762565613\n",
      "epoch n°2168 : train_loss = 1.8546723127365112, val_loss = 0.7268391251564026\n",
      "epoch n°2169 : train_loss = 1.8415987491607666, val_loss = 0.7224035859107971\n",
      "epoch n°2170 : train_loss = 1.854576826095581, val_loss = 0.7268162369728088\n",
      "epoch n°2171 : train_loss = 1.8433208465576172, val_loss = 0.7288263440132141\n",
      "epoch n°2172 : train_loss = 1.8351846933364868, val_loss = 0.7236449122428894\n",
      "epoch n°2173 : train_loss = 1.8426650762557983, val_loss = 0.7264739871025085\n",
      "epoch n°2174 : train_loss = 1.8432697057724, val_loss = 0.7236266732215881\n",
      "epoch n°2175 : train_loss = 1.838137149810791, val_loss = 0.7260496020317078\n",
      "epoch n°2176 : train_loss = 1.851691484451294, val_loss = 0.7247272729873657\n",
      "epoch n°2177 : train_loss = 1.8421688079833984, val_loss = 0.7322222590446472\n",
      "epoch n°2178 : train_loss = 1.841090440750122, val_loss = 0.727777361869812\n",
      "epoch n°2179 : train_loss = 1.8514206409454346, val_loss = 0.7261932492256165\n",
      "epoch n°2180 : train_loss = 1.8488858938217163, val_loss = 0.7213529348373413\n",
      "epoch n°2181 : train_loss = 1.849487066268921, val_loss = 0.7261509299278259\n",
      "epoch n°2182 : train_loss = 1.8391385078430176, val_loss = 0.7252736687660217\n",
      "epoch n°2183 : train_loss = 1.8426657915115356, val_loss = 0.7254278659820557\n",
      "epoch n°2184 : train_loss = 1.848510980606079, val_loss = 0.7257807850837708\n",
      "epoch n°2185 : train_loss = 1.8475682735443115, val_loss = 0.7300654053688049\n",
      "epoch n°2186 : train_loss = 1.8378667831420898, val_loss = 0.7279123067855835\n",
      "epoch n°2187 : train_loss = 1.8491190671920776, val_loss = 0.7183713316917419\n",
      "epoch n°2188 : train_loss = 1.8419631719589233, val_loss = 0.7182841897010803\n",
      "epoch n°2189 : train_loss = 1.8420215845108032, val_loss = 0.7218008041381836\n",
      "epoch n°2190 : train_loss = 1.8397976160049438, val_loss = 0.7245200276374817\n",
      "epoch n°2191 : train_loss = 1.853024959564209, val_loss = 0.7199879288673401\n",
      "epoch n°2192 : train_loss = 1.8442362546920776, val_loss = 0.7249427437782288\n",
      "epoch n°2193 : train_loss = 1.842157006263733, val_loss = 0.7204297184944153\n",
      "epoch n°2194 : train_loss = 1.844771385192871, val_loss = 0.7278259992599487\n",
      "epoch n°2195 : train_loss = 1.8507801294326782, val_loss = 0.7245885729789734\n",
      "epoch n°2196 : train_loss = 1.840762972831726, val_loss = 0.7222432494163513\n",
      "epoch n°2197 : train_loss = 1.8501825332641602, val_loss = 0.7280289530754089\n",
      "epoch n°2198 : train_loss = 1.8475533723831177, val_loss = 0.7237338423728943\n",
      "epoch n°2199 : train_loss = 1.8503683805465698, val_loss = 0.7305833697319031\n",
      "epoch n°2200 : train_loss = 1.845214605331421, val_loss = 0.7209611535072327\n",
      "epoch n°2201 : train_loss = 1.8375351428985596, val_loss = 0.7284541726112366\n",
      "epoch n°2202 : train_loss = 1.8518842458724976, val_loss = 0.7254251837730408\n",
      "epoch n°2203 : train_loss = 1.840119481086731, val_loss = 0.7264078259468079\n",
      "epoch n°2204 : train_loss = 1.8426201343536377, val_loss = 0.7326824069023132\n",
      "epoch n°2205 : train_loss = 1.8489990234375, val_loss = 0.7339316010475159\n",
      "epoch n°2206 : train_loss = 1.8505431413650513, val_loss = 0.7272413372993469\n",
      "epoch n°2207 : train_loss = 1.8424766063690186, val_loss = 0.7301947474479675\n",
      "epoch n°2208 : train_loss = 1.8438466787338257, val_loss = 0.7239210605621338\n",
      "epoch n°2209 : train_loss = 1.8398905992507935, val_loss = 0.7285242676734924\n",
      "epoch n°2210 : train_loss = 1.8415701389312744, val_loss = 0.7252081036567688\n",
      "epoch n°2211 : train_loss = 1.8571391105651855, val_loss = 0.7259988784790039\n",
      "epoch n°2212 : train_loss = 1.8337433338165283, val_loss = 0.7227979898452759\n",
      "epoch n°2213 : train_loss = 1.8384878635406494, val_loss = 0.72921222448349\n",
      "epoch n°2214 : train_loss = 1.8515828847885132, val_loss = 0.722720205783844\n",
      "epoch n°2215 : train_loss = 1.832371473312378, val_loss = 0.7295978665351868\n",
      "epoch n°2216 : train_loss = 1.843919277191162, val_loss = 0.720577597618103\n",
      "epoch n°2217 : train_loss = 1.84249746799469, val_loss = 0.7270994186401367\n",
      "epoch n°2218 : train_loss = 1.8408446311950684, val_loss = 0.7245585322380066\n",
      "epoch n°2219 : train_loss = 1.840031385421753, val_loss = 0.7239072322845459\n",
      "epoch n°2220 : train_loss = 1.8418444395065308, val_loss = 0.7244908809661865\n",
      "epoch n°2221 : train_loss = 1.8478100299835205, val_loss = 0.7309039831161499\n",
      "epoch n°2222 : train_loss = 1.8354684114456177, val_loss = 0.7272465825080872\n",
      "epoch n°2223 : train_loss = 1.8485467433929443, val_loss = 0.7210940718650818\n",
      "epoch n°2224 : train_loss = 1.8480294942855835, val_loss = 0.728510320186615\n",
      "epoch n°2225 : train_loss = 1.8509496450424194, val_loss = 0.7239470481872559\n",
      "epoch n°2226 : train_loss = 1.8313877582550049, val_loss = 0.7270825505256653\n",
      "epoch n°2227 : train_loss = 1.8534682989120483, val_loss = 0.7239412665367126\n",
      "epoch n°2228 : train_loss = 1.8406634330749512, val_loss = 0.7287000417709351\n",
      "epoch n°2229 : train_loss = 1.8513157367706299, val_loss = 0.7226294875144958\n",
      "epoch n°2230 : train_loss = 1.8359391689300537, val_loss = 0.7255735397338867\n",
      "epoch n°2231 : train_loss = 1.8347474336624146, val_loss = 0.7246639728546143\n",
      "epoch n°2232 : train_loss = 1.8540432453155518, val_loss = 0.7242085933685303\n",
      "epoch n°2233 : train_loss = 1.84672212600708, val_loss = 0.7249115705490112\n",
      "epoch n°2234 : train_loss = 1.8479114770889282, val_loss = 0.7267066836357117\n",
      "epoch n°2235 : train_loss = 1.8540661334991455, val_loss = 0.7268252372741699\n",
      "epoch n°2236 : train_loss = 1.8441991806030273, val_loss = 0.7252705097198486\n",
      "epoch n°2237 : train_loss = 1.8560951948165894, val_loss = 0.7238472104072571\n",
      "epoch n°2238 : train_loss = 1.8382256031036377, val_loss = 0.7219353914260864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2239 : train_loss = 1.8423418998718262, val_loss = 0.7282539010047913\n",
      "epoch n°2240 : train_loss = 1.8435355424880981, val_loss = 0.7239625453948975\n",
      "epoch n°2241 : train_loss = 1.8390330076217651, val_loss = 0.725239098072052\n",
      "epoch n°2242 : train_loss = 1.8438128232955933, val_loss = 0.7282354235649109\n",
      "epoch n°2243 : train_loss = 1.8450708389282227, val_loss = 0.7299792766571045\n",
      "epoch n°2244 : train_loss = 1.8456833362579346, val_loss = 0.7242660522460938\n",
      "epoch n°2245 : train_loss = 1.8445215225219727, val_loss = 0.7254191637039185\n",
      "epoch n°2246 : train_loss = 1.8488332033157349, val_loss = 0.7277842164039612\n",
      "epoch n°2247 : train_loss = 1.8447433710098267, val_loss = 0.7311646342277527\n",
      "epoch n°2248 : train_loss = 1.839359998703003, val_loss = 0.7265834808349609\n",
      "epoch n°2249 : train_loss = 1.8394465446472168, val_loss = 0.7253356575965881\n",
      "epoch n°2250 : train_loss = 1.8419853448867798, val_loss = 0.7264682650566101\n",
      "epoch n°2251 : train_loss = 1.8406925201416016, val_loss = 0.7240468263626099\n",
      "epoch n°2252 : train_loss = 1.8461825847625732, val_loss = 0.7231754660606384\n",
      "epoch n°2253 : train_loss = 1.8481403589248657, val_loss = 0.7253899574279785\n",
      "epoch n°2254 : train_loss = 1.8494699001312256, val_loss = 0.7245826125144958\n",
      "epoch n°2255 : train_loss = 1.8379486799240112, val_loss = 0.7221069931983948\n",
      "epoch n°2256 : train_loss = 1.8448972702026367, val_loss = 0.7302569150924683\n",
      "epoch n°2257 : train_loss = 1.8326735496520996, val_loss = 0.7225842475891113\n",
      "epoch n°2258 : train_loss = 1.8445899486541748, val_loss = 0.7276601791381836\n",
      "epoch n°2259 : train_loss = 1.8470790386199951, val_loss = 0.7269294261932373\n",
      "epoch n°2260 : train_loss = 1.8413622379302979, val_loss = 0.726771891117096\n",
      "epoch n°2261 : train_loss = 1.8504140377044678, val_loss = 0.7217663526535034\n",
      "epoch n°2262 : train_loss = 1.843241810798645, val_loss = 0.7221830487251282\n",
      "epoch n°2263 : train_loss = 1.8528155088424683, val_loss = 0.7191082835197449\n",
      "epoch n°2264 : train_loss = 1.8461261987686157, val_loss = 0.7293795943260193\n",
      "epoch n°2265 : train_loss = 1.8372246026992798, val_loss = 0.7299392223358154\n",
      "epoch n°2266 : train_loss = 1.8437426090240479, val_loss = 0.7287895679473877\n",
      "epoch n°2267 : train_loss = 1.8385528326034546, val_loss = 0.7254816293716431\n",
      "epoch n°2268 : train_loss = 1.8446338176727295, val_loss = 0.7280588150024414\n",
      "epoch n°2269 : train_loss = 1.835852026939392, val_loss = 0.7206670045852661\n",
      "epoch n°2270 : train_loss = 1.845996379852295, val_loss = 0.7259372472763062\n",
      "epoch n°2271 : train_loss = 1.8350887298583984, val_loss = 0.7265678644180298\n",
      "epoch n°2272 : train_loss = 1.840535044670105, val_loss = 0.7250194549560547\n",
      "epoch n°2273 : train_loss = 1.8356417417526245, val_loss = 0.7289698123931885\n",
      "epoch n°2274 : train_loss = 1.832840085029602, val_loss = 0.7265867590904236\n",
      "epoch n°2275 : train_loss = 1.836866021156311, val_loss = 0.7235615253448486\n",
      "epoch n°2276 : train_loss = 1.8448542356491089, val_loss = 0.7211322784423828\n",
      "epoch n°2277 : train_loss = 1.8466664552688599, val_loss = 0.723862886428833\n",
      "epoch n°2278 : train_loss = 1.8559340238571167, val_loss = 0.7279397249221802\n",
      "epoch n°2279 : train_loss = 1.8506947755813599, val_loss = 0.7248015403747559\n",
      "epoch n°2280 : train_loss = 1.8315390348434448, val_loss = 0.7238777279853821\n",
      "epoch n°2281 : train_loss = 1.8443838357925415, val_loss = 0.7268214821815491\n",
      "epoch n°2282 : train_loss = 1.8396292924880981, val_loss = 0.7294758558273315\n",
      "epoch n°2283 : train_loss = 1.8347445726394653, val_loss = 0.7225527167320251\n",
      "epoch n°2284 : train_loss = 1.8465733528137207, val_loss = 0.7325407862663269\n",
      "epoch n°2285 : train_loss = 1.8484373092651367, val_loss = 0.7264066338539124\n",
      "epoch n°2286 : train_loss = 1.8348077535629272, val_loss = 0.7216776013374329\n",
      "epoch n°2287 : train_loss = 1.8484922647476196, val_loss = 0.7229863405227661\n",
      "epoch n°2288 : train_loss = 1.8465455770492554, val_loss = 0.7246172428131104\n",
      "epoch n°2289 : train_loss = 1.8450126647949219, val_loss = 0.7251282930374146\n",
      "epoch n°2290 : train_loss = 1.8462536334991455, val_loss = 0.7204931378364563\n",
      "epoch n°2291 : train_loss = 1.8457980155944824, val_loss = 0.7268139719963074\n",
      "epoch n°2292 : train_loss = 1.8381578922271729, val_loss = 0.722253680229187\n",
      "epoch n°2293 : train_loss = 1.845258355140686, val_loss = 0.7264542579650879\n",
      "epoch n°2294 : train_loss = 1.8469175100326538, val_loss = 0.7263996601104736\n",
      "epoch n°2295 : train_loss = 1.8343780040740967, val_loss = 0.726978600025177\n",
      "epoch n°2296 : train_loss = 1.8412432670593262, val_loss = 0.7234044075012207\n",
      "epoch n°2297 : train_loss = 1.8558703660964966, val_loss = 0.7202081680297852\n",
      "epoch n°2298 : train_loss = 1.835007667541504, val_loss = 0.7247359752655029\n",
      "epoch n°2299 : train_loss = 1.8415379524230957, val_loss = 0.7244964838027954\n",
      "epoch n°2300 : train_loss = 1.8423696756362915, val_loss = 0.7229787707328796\n",
      "epoch n°2301 : train_loss = 1.8414812088012695, val_loss = 0.7266061902046204\n",
      "epoch n°2302 : train_loss = 1.8463525772094727, val_loss = 0.7247540354728699\n",
      "epoch n°2303 : train_loss = 1.8483929634094238, val_loss = 0.7227122783660889\n",
      "epoch n°2304 : train_loss = 1.8251193761825562, val_loss = 0.7241019010543823\n",
      "epoch n°2305 : train_loss = 1.8490679264068604, val_loss = 0.722226083278656\n",
      "epoch n°2306 : train_loss = 1.84929621219635, val_loss = 0.7265510559082031\n",
      "epoch n°2307 : train_loss = 1.8328694105148315, val_loss = 0.7270045280456543\n",
      "epoch n°2308 : train_loss = 1.8422813415527344, val_loss = 0.7283138632774353\n",
      "epoch n°2309 : train_loss = 1.8437762260437012, val_loss = 0.7288289070129395\n",
      "epoch n°2310 : train_loss = 1.8417564630508423, val_loss = 0.7252046465873718\n",
      "epoch n°2311 : train_loss = 1.8453069925308228, val_loss = 0.722856879234314\n",
      "epoch n°2312 : train_loss = 1.8422504663467407, val_loss = 0.7243152856826782\n",
      "epoch n°2313 : train_loss = 1.8309547901153564, val_loss = 0.7178855538368225\n",
      "epoch n°2314 : train_loss = 1.835490107536316, val_loss = 0.7223171591758728\n",
      "epoch n°2315 : train_loss = 1.8457261323928833, val_loss = 0.7174674272537231\n",
      "epoch n°2316 : train_loss = 1.8311291933059692, val_loss = 0.727406919002533\n",
      "epoch n°2317 : train_loss = 1.8397191762924194, val_loss = 0.7260923385620117\n",
      "epoch n°2318 : train_loss = 1.8384099006652832, val_loss = 0.7291070818901062\n",
      "epoch n°2319 : train_loss = 1.8489419221878052, val_loss = 0.7248549461364746\n",
      "epoch n°2320 : train_loss = 1.8371425867080688, val_loss = 0.7208707928657532\n",
      "epoch n°2321 : train_loss = 1.8398488759994507, val_loss = 0.7301656007766724\n",
      "epoch n°2322 : train_loss = 1.8401628732681274, val_loss = 0.7277335524559021\n",
      "epoch n°2323 : train_loss = 1.8304964303970337, val_loss = 0.7216190695762634\n",
      "epoch n°2324 : train_loss = 1.8396052122116089, val_loss = 0.7243077754974365\n",
      "epoch n°2325 : train_loss = 1.838973879814148, val_loss = 0.7278329133987427\n",
      "epoch n°2326 : train_loss = 1.8440800905227661, val_loss = 0.7275421619415283\n",
      "epoch n°2327 : train_loss = 1.845959186553955, val_loss = 0.7224400639533997\n",
      "epoch n°2328 : train_loss = 1.8352773189544678, val_loss = 0.7268843650817871\n",
      "epoch n°2329 : train_loss = 1.842310905456543, val_loss = 0.7201999425888062\n",
      "epoch n°2330 : train_loss = 1.8433960676193237, val_loss = 0.7247941493988037\n",
      "epoch n°2331 : train_loss = 1.8422945737838745, val_loss = 0.725731372833252\n",
      "epoch n°2332 : train_loss = 1.8332068920135498, val_loss = 0.7265515327453613\n",
      "epoch n°2333 : train_loss = 1.8372827768325806, val_loss = 0.7342908382415771\n",
      "epoch n°2334 : train_loss = 1.8385145664215088, val_loss = 0.7244192957878113\n",
      "epoch n°2335 : train_loss = 1.8425718545913696, val_loss = 0.7242051959037781\n",
      "epoch n°2336 : train_loss = 1.8347342014312744, val_loss = 0.7255602478981018\n",
      "epoch n°2337 : train_loss = 1.8438833951950073, val_loss = 0.7244130373001099\n",
      "epoch n°2338 : train_loss = 1.8385875225067139, val_loss = 0.7261676788330078\n",
      "epoch n°2339 : train_loss = 1.8538482189178467, val_loss = 0.7280870079994202\n",
      "epoch n°2340 : train_loss = 1.842749834060669, val_loss = 0.7264171242713928\n",
      "epoch n°2341 : train_loss = 1.8357897996902466, val_loss = 0.7268996238708496\n",
      "epoch n°2342 : train_loss = 1.8396415710449219, val_loss = 0.7257432341575623\n",
      "epoch n°2343 : train_loss = 1.8398956060409546, val_loss = 0.7261978983879089\n",
      "epoch n°2344 : train_loss = 1.839611530303955, val_loss = 0.7240920662879944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2345 : train_loss = 1.829913854598999, val_loss = 0.725189208984375\n",
      "epoch n°2346 : train_loss = 1.8424521684646606, val_loss = 0.7243530750274658\n",
      "epoch n°2347 : train_loss = 1.8547252416610718, val_loss = 0.724328875541687\n",
      "epoch n°2348 : train_loss = 1.8438403606414795, val_loss = 0.724463164806366\n",
      "epoch n°2349 : train_loss = 1.8448597192764282, val_loss = 0.7235950231552124\n",
      "epoch n°2350 : train_loss = 1.8371378183364868, val_loss = 0.7236968278884888\n",
      "epoch n°2351 : train_loss = 1.8372938632965088, val_loss = 0.7234064340591431\n",
      "epoch n°2352 : train_loss = 1.8337427377700806, val_loss = 0.7251094579696655\n",
      "epoch n°2353 : train_loss = 1.8300524950027466, val_loss = 0.7234041690826416\n",
      "epoch n°2354 : train_loss = 1.8440027236938477, val_loss = 0.7312082052230835\n",
      "epoch n°2355 : train_loss = 1.84339439868927, val_loss = 0.7269426584243774\n",
      "epoch n°2356 : train_loss = 1.8483561277389526, val_loss = 0.7218056321144104\n",
      "epoch n°2357 : train_loss = 1.8353817462921143, val_loss = 0.7212745547294617\n",
      "epoch n°2358 : train_loss = 1.8524112701416016, val_loss = 0.7264912724494934\n",
      "epoch n°2359 : train_loss = 1.8483593463897705, val_loss = 0.724042534828186\n",
      "epoch n°2360 : train_loss = 1.8306899070739746, val_loss = 0.7213895320892334\n",
      "epoch n°2361 : train_loss = 1.830485224723816, val_loss = 0.7311673760414124\n",
      "epoch n°2362 : train_loss = 1.8446491956710815, val_loss = 0.7233163118362427\n",
      "epoch n°2363 : train_loss = 1.8347373008728027, val_loss = 0.7203376293182373\n",
      "epoch n°2364 : train_loss = 1.8391813039779663, val_loss = 0.7283695340156555\n",
      "epoch n°2365 : train_loss = 1.8355064392089844, val_loss = 0.7231563925743103\n",
      "epoch n°2366 : train_loss = 1.83881413936615, val_loss = 0.7317244410514832\n",
      "epoch n°2367 : train_loss = 1.8361666202545166, val_loss = 0.7210530638694763\n",
      "epoch n°2368 : train_loss = 1.8410389423370361, val_loss = 0.7267014384269714\n",
      "epoch n°2369 : train_loss = 1.8444736003875732, val_loss = 0.7261922955513\n",
      "epoch n°2370 : train_loss = 1.841822862625122, val_loss = 0.7213944792747498\n",
      "epoch n°2371 : train_loss = 1.8361163139343262, val_loss = 0.7301676273345947\n",
      "epoch n°2372 : train_loss = 1.8422621488571167, val_loss = 0.7208638787269592\n",
      "epoch n°2373 : train_loss = 1.8414798974990845, val_loss = 0.7284114956855774\n",
      "epoch n°2374 : train_loss = 1.8379652500152588, val_loss = 0.7295538187026978\n",
      "epoch n°2375 : train_loss = 1.845501184463501, val_loss = 0.7247194051742554\n",
      "epoch n°2376 : train_loss = 1.8358339071273804, val_loss = 0.7220842838287354\n",
      "epoch n°2377 : train_loss = 1.8359540700912476, val_loss = 0.721914529800415\n",
      "epoch n°2378 : train_loss = 1.8478542566299438, val_loss = 0.725156307220459\n",
      "epoch n°2379 : train_loss = 1.8431718349456787, val_loss = 0.7258004546165466\n",
      "epoch n°2380 : train_loss = 1.849472999572754, val_loss = 0.7210273146629333\n",
      "epoch n°2381 : train_loss = 1.832305908203125, val_loss = 0.731047511100769\n",
      "epoch n°2382 : train_loss = 1.8368535041809082, val_loss = 0.73016357421875\n",
      "epoch n°2383 : train_loss = 1.8386858701705933, val_loss = 0.7275804281234741\n",
      "epoch n°2384 : train_loss = 1.8412636518478394, val_loss = 0.725829005241394\n",
      "epoch n°2385 : train_loss = 1.841860294342041, val_loss = 0.7261735796928406\n",
      "epoch n°2386 : train_loss = 1.8294914960861206, val_loss = 0.7263197898864746\n",
      "epoch n°2387 : train_loss = 1.8375444412231445, val_loss = 0.7212923169136047\n",
      "epoch n°2388 : train_loss = 1.8288724422454834, val_loss = 0.7268434762954712\n",
      "epoch n°2389 : train_loss = 1.8365674018859863, val_loss = 0.7244836688041687\n",
      "epoch n°2390 : train_loss = 1.8365535736083984, val_loss = 0.7222899794578552\n",
      "epoch n°2391 : train_loss = 1.8358303308486938, val_loss = 0.7304791212081909\n",
      "epoch n°2392 : train_loss = 1.834168791770935, val_loss = 0.7244482636451721\n",
      "epoch n°2393 : train_loss = 1.840928077697754, val_loss = 0.7289034128189087\n",
      "epoch n°2394 : train_loss = 1.8356746435165405, val_loss = 0.721945583820343\n",
      "epoch n°2395 : train_loss = 1.8422183990478516, val_loss = 0.724965512752533\n",
      "epoch n°2396 : train_loss = 1.8269362449645996, val_loss = 0.7206224203109741\n",
      "epoch n°2397 : train_loss = 1.8398327827453613, val_loss = 0.7199462652206421\n",
      "epoch n°2398 : train_loss = 1.8374171257019043, val_loss = 0.722310483455658\n",
      "epoch n°2399 : train_loss = 1.8410775661468506, val_loss = 0.7247170805931091\n",
      "epoch n°2400 : train_loss = 1.8299064636230469, val_loss = 0.7289430499076843\n",
      "epoch n°2401 : train_loss = 1.8373039960861206, val_loss = 0.7258408665657043\n",
      "epoch n°2402 : train_loss = 1.8480305671691895, val_loss = 0.7249783277511597\n",
      "epoch n°2403 : train_loss = 1.8361902236938477, val_loss = 0.7220613956451416\n",
      "epoch n°2404 : train_loss = 1.8367446660995483, val_loss = 0.7212879657745361\n",
      "epoch n°2405 : train_loss = 1.8351036310195923, val_loss = 0.7208443284034729\n",
      "epoch n°2406 : train_loss = 1.8375002145767212, val_loss = 0.7210151553153992\n",
      "epoch n°2407 : train_loss = 1.831706166267395, val_loss = 0.7236266732215881\n",
      "epoch n°2408 : train_loss = 1.8436883687973022, val_loss = 0.722465455532074\n",
      "epoch n°2409 : train_loss = 1.8441126346588135, val_loss = 0.7301730513572693\n",
      "epoch n°2410 : train_loss = 1.8410674333572388, val_loss = 0.7259523272514343\n",
      "epoch n°2411 : train_loss = 1.8244847059249878, val_loss = 0.7223374843597412\n",
      "epoch n°2412 : train_loss = 1.8343398571014404, val_loss = 0.7232291102409363\n",
      "epoch n°2413 : train_loss = 1.8346322774887085, val_loss = 0.7273543477058411\n",
      "epoch n°2414 : train_loss = 1.8314337730407715, val_loss = 0.724785327911377\n",
      "epoch n°2415 : train_loss = 1.8502469062805176, val_loss = 0.7266234159469604\n",
      "epoch n°2416 : train_loss = 1.8434587717056274, val_loss = 0.7233898043632507\n",
      "epoch n°2417 : train_loss = 1.8399218320846558, val_loss = 0.7259432673454285\n",
      "epoch n°2418 : train_loss = 1.8397167921066284, val_loss = 0.7262639403343201\n",
      "epoch n°2419 : train_loss = 1.8326619863510132, val_loss = 0.7247727513313293\n",
      "epoch n°2420 : train_loss = 1.8287936449050903, val_loss = 0.7208911776542664\n",
      "epoch n°2421 : train_loss = 1.8324646949768066, val_loss = 0.7260683178901672\n",
      "epoch n°2422 : train_loss = 1.831538438796997, val_loss = 0.7254580855369568\n",
      "epoch n°2423 : train_loss = 1.8422737121582031, val_loss = 0.7245984077453613\n",
      "epoch n°2424 : train_loss = 1.8300096988677979, val_loss = 0.7212956547737122\n",
      "epoch n°2425 : train_loss = 1.8440500497817993, val_loss = 0.73106449842453\n",
      "epoch n°2426 : train_loss = 1.8439375162124634, val_loss = 0.7182195782661438\n",
      "epoch n°2427 : train_loss = 1.8351720571517944, val_loss = 0.7223004102706909\n",
      "epoch n°2428 : train_loss = 1.8447660207748413, val_loss = 0.7231555581092834\n",
      "epoch n°2429 : train_loss = 1.8407999277114868, val_loss = 0.7248024940490723\n",
      "epoch n°2430 : train_loss = 1.8378158807754517, val_loss = 0.7210649847984314\n",
      "epoch n°2431 : train_loss = 1.8478264808654785, val_loss = 0.7285786271095276\n",
      "epoch n°2432 : train_loss = 1.8477048873901367, val_loss = 0.7215640544891357\n",
      "epoch n°2433 : train_loss = 1.825303077697754, val_loss = 0.724350094795227\n",
      "epoch n°2434 : train_loss = 1.837070345878601, val_loss = 0.7334925532341003\n",
      "epoch n°2435 : train_loss = 1.837030053138733, val_loss = 0.721909761428833\n",
      "epoch n°2436 : train_loss = 1.8332362174987793, val_loss = 0.7207624912261963\n",
      "epoch n°2437 : train_loss = 1.8487218618392944, val_loss = 0.7251325249671936\n",
      "epoch n°2438 : train_loss = 1.8338217735290527, val_loss = 0.7204828262329102\n",
      "epoch n°2439 : train_loss = 1.8360892534255981, val_loss = 0.722283124923706\n",
      "epoch n°2440 : train_loss = 1.845521330833435, val_loss = 0.723699152469635\n",
      "epoch n°2441 : train_loss = 1.828465223312378, val_loss = 0.7284905314445496\n",
      "epoch n°2442 : train_loss = 1.8239600658416748, val_loss = 0.7238147854804993\n",
      "epoch n°2443 : train_loss = 1.8345115184783936, val_loss = 0.7303439378738403\n",
      "epoch n°2444 : train_loss = 1.8349164724349976, val_loss = 0.7225912809371948\n",
      "epoch n°2445 : train_loss = 1.835200548171997, val_loss = 0.7228241562843323\n",
      "epoch n°2446 : train_loss = 1.8301661014556885, val_loss = 0.7265060544013977\n",
      "epoch n°2447 : train_loss = 1.8379446268081665, val_loss = 0.7271088361740112\n",
      "epoch n°2448 : train_loss = 1.8399280309677124, val_loss = 0.7242693305015564\n",
      "epoch n°2449 : train_loss = 1.8369628190994263, val_loss = 0.7266150116920471\n",
      "epoch n°2450 : train_loss = 1.8346178531646729, val_loss = 0.7280046343803406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2451 : train_loss = 1.8290038108825684, val_loss = 0.7225456833839417\n",
      "epoch n°2452 : train_loss = 1.8408653736114502, val_loss = 0.726294219493866\n",
      "epoch n°2453 : train_loss = 1.8270175457000732, val_loss = 0.7205820083618164\n",
      "epoch n°2454 : train_loss = 1.8312467336654663, val_loss = 0.7231975793838501\n",
      "epoch n°2455 : train_loss = 1.840082049369812, val_loss = 0.7273339033126831\n",
      "epoch n°2456 : train_loss = 1.8389947414398193, val_loss = 0.7219393253326416\n",
      "epoch n°2457 : train_loss = 1.8382381200790405, val_loss = 0.72217857837677\n",
      "epoch n°2458 : train_loss = 1.8377490043640137, val_loss = 0.7266693711280823\n",
      "epoch n°2459 : train_loss = 1.8370566368103027, val_loss = 0.7251362800598145\n",
      "epoch n°2460 : train_loss = 1.8404446840286255, val_loss = 0.7245377898216248\n",
      "epoch n°2461 : train_loss = 1.841699481010437, val_loss = 0.7241946458816528\n",
      "epoch n°2462 : train_loss = 1.833661675453186, val_loss = 0.7281776666641235\n",
      "epoch n°2463 : train_loss = 1.828019142150879, val_loss = 0.7226638793945312\n",
      "epoch n°2464 : train_loss = 1.831139326095581, val_loss = 0.7230460047721863\n",
      "epoch n°2465 : train_loss = 1.8383283615112305, val_loss = 0.7244735956192017\n",
      "epoch n°2466 : train_loss = 1.8275715112686157, val_loss = 0.7229121327400208\n",
      "epoch n°2467 : train_loss = 1.842941164970398, val_loss = 0.7241246104240417\n",
      "epoch n°2468 : train_loss = 1.8353506326675415, val_loss = 0.7248706817626953\n",
      "epoch n°2469 : train_loss = 1.8402515649795532, val_loss = 0.7209863066673279\n",
      "epoch n°2470 : train_loss = 1.8305236101150513, val_loss = 0.7211891412734985\n",
      "epoch n°2471 : train_loss = 1.828461766242981, val_loss = 0.7299894094467163\n",
      "epoch n°2472 : train_loss = 1.8469772338867188, val_loss = 0.7233170866966248\n",
      "epoch n°2473 : train_loss = 1.8386619091033936, val_loss = 0.7249903082847595\n",
      "epoch n°2474 : train_loss = 1.8286375999450684, val_loss = 0.7239001989364624\n",
      "epoch n°2475 : train_loss = 1.8457436561584473, val_loss = 0.725018322467804\n",
      "epoch n°2476 : train_loss = 1.836369276046753, val_loss = 0.723602831363678\n",
      "epoch n°2477 : train_loss = 1.8357070684432983, val_loss = 0.7195612788200378\n",
      "epoch n°2478 : train_loss = 1.827905297279358, val_loss = 0.7245132923126221\n",
      "epoch n°2479 : train_loss = 1.8263673782348633, val_loss = 0.7214544415473938\n",
      "epoch n°2480 : train_loss = 1.842148780822754, val_loss = 0.7261905670166016\n",
      "epoch n°2481 : train_loss = 1.835383415222168, val_loss = 0.7266178727149963\n",
      "epoch n°2482 : train_loss = 1.8370277881622314, val_loss = 0.7261649370193481\n",
      "epoch n°2483 : train_loss = 1.8376035690307617, val_loss = 0.7243961095809937\n",
      "epoch n°2484 : train_loss = 1.8296469449996948, val_loss = 0.7208951711654663\n",
      "epoch n°2485 : train_loss = 1.834861159324646, val_loss = 0.7228286862373352\n",
      "epoch n°2486 : train_loss = 1.8241702318191528, val_loss = 0.7250829935073853\n",
      "epoch n°2487 : train_loss = 1.8291432857513428, val_loss = 0.7186743021011353\n",
      "epoch n°2488 : train_loss = 1.8359061479568481, val_loss = 0.7249467372894287\n",
      "epoch n°2489 : train_loss = 1.8311623334884644, val_loss = 0.721673309803009\n",
      "epoch n°2490 : train_loss = 1.8267539739608765, val_loss = 0.7284059524536133\n",
      "epoch n°2491 : train_loss = 1.8315397500991821, val_loss = 0.7254542112350464\n",
      "epoch n°2492 : train_loss = 1.8350213766098022, val_loss = 0.7237206697463989\n",
      "epoch n°2493 : train_loss = 1.835365653038025, val_loss = 0.7207224369049072\n",
      "epoch n°2494 : train_loss = 1.8281019926071167, val_loss = 0.7256583571434021\n",
      "epoch n°2495 : train_loss = 1.8355412483215332, val_loss = 0.724523663520813\n",
      "epoch n°2496 : train_loss = 1.8224347829818726, val_loss = 0.7240988612174988\n",
      "epoch n°2497 : train_loss = 1.8303276300430298, val_loss = 0.7239983677864075\n",
      "epoch n°2498 : train_loss = 1.8296080827713013, val_loss = 0.7235804200172424\n",
      "epoch n°2499 : train_loss = 1.8396251201629639, val_loss = 0.7235440015792847\n",
      "epoch n°2500 : train_loss = 1.8308054208755493, val_loss = 0.7281316518783569\n",
      "epoch n°2501 : train_loss = 1.8267525434494019, val_loss = 0.7242954969406128\n",
      "epoch n°2502 : train_loss = 1.8241633176803589, val_loss = 0.7256608605384827\n",
      "epoch n°2503 : train_loss = 1.830075740814209, val_loss = 0.7248071432113647\n",
      "epoch n°2504 : train_loss = 1.8353784084320068, val_loss = 0.7233344316482544\n",
      "epoch n°2505 : train_loss = 1.8238157033920288, val_loss = 0.7242566347122192\n",
      "epoch n°2506 : train_loss = 1.8268718719482422, val_loss = 0.7261911034584045\n",
      "epoch n°2507 : train_loss = 1.8277102708816528, val_loss = 0.7244476079940796\n",
      "epoch n°2508 : train_loss = 1.828118920326233, val_loss = 0.7270216345787048\n",
      "epoch n°2509 : train_loss = 1.832702398300171, val_loss = 0.7230127453804016\n",
      "epoch n°2510 : train_loss = 1.8280422687530518, val_loss = 0.7276252508163452\n",
      "epoch n°2511 : train_loss = 1.8330637216567993, val_loss = 0.7238063216209412\n",
      "epoch n°2512 : train_loss = 1.8370898962020874, val_loss = 0.7223796844482422\n",
      "epoch n°2513 : train_loss = 1.83198881149292, val_loss = 0.7227948307991028\n",
      "epoch n°2514 : train_loss = 1.8364284038543701, val_loss = 0.7271241545677185\n",
      "epoch n°2515 : train_loss = 1.828938364982605, val_loss = 0.7226324081420898\n",
      "epoch n°2516 : train_loss = 1.8496946096420288, val_loss = 0.7239219546318054\n",
      "epoch n°2517 : train_loss = 1.8310768604278564, val_loss = 0.7239295840263367\n",
      "epoch n°2518 : train_loss = 1.8288352489471436, val_loss = 0.7265879511833191\n",
      "epoch n°2519 : train_loss = 1.8265817165374756, val_loss = 0.7199794054031372\n",
      "epoch n°2520 : train_loss = 1.8430964946746826, val_loss = 0.7271571159362793\n",
      "epoch n°2521 : train_loss = 1.8324006795883179, val_loss = 0.7267340421676636\n",
      "epoch n°2522 : train_loss = 1.8287994861602783, val_loss = 0.7257013320922852\n",
      "epoch n°2523 : train_loss = 1.8349641561508179, val_loss = 0.7209840416908264\n",
      "epoch n°2524 : train_loss = 1.8375879526138306, val_loss = 0.7261570692062378\n",
      "epoch n°2525 : train_loss = 1.8396143913269043, val_loss = 0.7282655835151672\n",
      "epoch n°2526 : train_loss = 1.831421971321106, val_loss = 0.7231723666191101\n",
      "epoch n°2527 : train_loss = 1.8264470100402832, val_loss = 0.7254804968833923\n",
      "epoch n°2528 : train_loss = 1.82418692111969, val_loss = 0.7246516942977905\n",
      "epoch n°2529 : train_loss = 1.8264271020889282, val_loss = 0.7197346687316895\n",
      "epoch n°2530 : train_loss = 1.8375871181488037, val_loss = 0.7266111969947815\n",
      "epoch n°2531 : train_loss = 1.827268123626709, val_loss = 0.726252019405365\n",
      "epoch n°2532 : train_loss = 1.8349261283874512, val_loss = 0.7253313064575195\n",
      "epoch n°2533 : train_loss = 1.8363066911697388, val_loss = 0.7235872745513916\n",
      "epoch n°2534 : train_loss = 1.8311513662338257, val_loss = 0.7292436957359314\n",
      "epoch n°2535 : train_loss = 1.8388621807098389, val_loss = 0.7252917885780334\n",
      "epoch n°2536 : train_loss = 1.81914222240448, val_loss = 0.7255303263664246\n",
      "epoch n°2537 : train_loss = 1.8221631050109863, val_loss = 0.7210452556610107\n",
      "epoch n°2538 : train_loss = 1.8221255540847778, val_loss = 0.725676953792572\n",
      "epoch n°2539 : train_loss = 1.8307570219039917, val_loss = 0.7241140604019165\n",
      "epoch n°2540 : train_loss = 1.8390225172042847, val_loss = 0.7256462574005127\n",
      "epoch n°2541 : train_loss = 1.839990496635437, val_loss = 0.723987340927124\n",
      "epoch n°2542 : train_loss = 1.8350085020065308, val_loss = 0.7261049747467041\n",
      "epoch n°2543 : train_loss = 1.8425281047821045, val_loss = 0.7255427837371826\n",
      "epoch n°2544 : train_loss = 1.833071231842041, val_loss = 0.7221615314483643\n",
      "epoch n°2545 : train_loss = 1.8301963806152344, val_loss = 0.7224093675613403\n",
      "epoch n°2546 : train_loss = 1.8414435386657715, val_loss = 0.7244437336921692\n",
      "epoch n°2547 : train_loss = 1.833195686340332, val_loss = 0.7263727784156799\n",
      "epoch n°2548 : train_loss = 1.8340327739715576, val_loss = 0.7234600782394409\n",
      "epoch n°2549 : train_loss = 1.8348731994628906, val_loss = 0.7244707345962524\n",
      "epoch n°2550 : train_loss = 1.8340638875961304, val_loss = 0.7313699126243591\n",
      "epoch n°2551 : train_loss = 1.8308374881744385, val_loss = 0.7265945672988892\n",
      "epoch n°2552 : train_loss = 1.8277679681777954, val_loss = 0.7259847521781921\n",
      "epoch n°2553 : train_loss = 1.8272323608398438, val_loss = 0.7258769273757935\n",
      "epoch n°2554 : train_loss = 1.8332767486572266, val_loss = 0.7194701433181763\n",
      "epoch n°2555 : train_loss = 1.8245978355407715, val_loss = 0.7229551672935486\n",
      "epoch n°2556 : train_loss = 1.8235721588134766, val_loss = 0.7289841771125793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2557 : train_loss = 1.8352882862091064, val_loss = 0.7189124226570129\n",
      "epoch n°2558 : train_loss = 1.8231163024902344, val_loss = 0.721218466758728\n",
      "epoch n°2559 : train_loss = 1.8214545249938965, val_loss = 0.726754367351532\n",
      "epoch n°2560 : train_loss = 1.8307613134384155, val_loss = 0.7253972887992859\n",
      "epoch n°2561 : train_loss = 1.839442491531372, val_loss = 0.7232100367546082\n",
      "epoch n°2562 : train_loss = 1.8281580209732056, val_loss = 0.7245848774909973\n",
      "epoch n°2563 : train_loss = 1.8212506771087646, val_loss = 0.7266703844070435\n",
      "epoch n°2564 : train_loss = 1.8296877145767212, val_loss = 0.7203775644302368\n",
      "epoch n°2565 : train_loss = 1.835114598274231, val_loss = 0.7262839078903198\n",
      "epoch n°2566 : train_loss = 1.8263022899627686, val_loss = 0.7181510329246521\n",
      "epoch n°2567 : train_loss = 1.8258944749832153, val_loss = 0.7201440930366516\n",
      "epoch n°2568 : train_loss = 1.8177385330200195, val_loss = 0.7208889126777649\n",
      "epoch n°2569 : train_loss = 1.8304039239883423, val_loss = 0.7216026186943054\n",
      "epoch n°2570 : train_loss = 1.8355947732925415, val_loss = 0.7222602963447571\n",
      "epoch n°2571 : train_loss = 1.841712236404419, val_loss = 0.7259629964828491\n",
      "epoch n°2572 : train_loss = 1.8238317966461182, val_loss = 0.721080482006073\n",
      "epoch n°2573 : train_loss = 1.829519271850586, val_loss = 0.7247380018234253\n",
      "epoch n°2574 : train_loss = 1.8289892673492432, val_loss = 0.719329833984375\n",
      "epoch n°2575 : train_loss = 1.8286088705062866, val_loss = 0.7158478498458862\n",
      "epoch n°2576 : train_loss = 1.834625005722046, val_loss = 0.7237449884414673\n",
      "epoch n°2577 : train_loss = 1.8265740871429443, val_loss = 0.7260943651199341\n",
      "epoch n°2578 : train_loss = 1.8374298810958862, val_loss = 0.7216922640800476\n",
      "epoch n°2579 : train_loss = 1.8261854648590088, val_loss = 0.7238218188285828\n",
      "epoch n°2580 : train_loss = 1.828421950340271, val_loss = 0.7204984426498413\n",
      "epoch n°2581 : train_loss = 1.8284984827041626, val_loss = 0.7263995409011841\n",
      "epoch n°2582 : train_loss = 1.8231631517410278, val_loss = 0.7249289155006409\n",
      "epoch n°2583 : train_loss = 1.8232771158218384, val_loss = 0.7233412265777588\n",
      "epoch n°2584 : train_loss = 1.8230891227722168, val_loss = 0.7218348979949951\n",
      "epoch n°2585 : train_loss = 1.8307172060012817, val_loss = 0.7270507216453552\n",
      "epoch n°2586 : train_loss = 1.8269175291061401, val_loss = 0.7260615825653076\n",
      "epoch n°2587 : train_loss = 1.834193229675293, val_loss = 0.726157546043396\n",
      "epoch n°2588 : train_loss = 1.8373687267303467, val_loss = 0.7225001454353333\n",
      "epoch n°2589 : train_loss = 1.8285280466079712, val_loss = 0.7254651188850403\n",
      "epoch n°2590 : train_loss = 1.838573694229126, val_loss = 0.7263925075531006\n",
      "epoch n°2591 : train_loss = 1.8288880586624146, val_loss = 0.7248236536979675\n",
      "epoch n°2592 : train_loss = 1.81963312625885, val_loss = 0.7219848036766052\n",
      "epoch n°2593 : train_loss = 1.8283958435058594, val_loss = 0.7209351062774658\n",
      "epoch n°2594 : train_loss = 1.8289703130722046, val_loss = 0.7331281304359436\n",
      "epoch n°2595 : train_loss = 1.8232485055923462, val_loss = 0.7252013087272644\n",
      "epoch n°2596 : train_loss = 1.8363010883331299, val_loss = 0.7215358018875122\n",
      "epoch n°2597 : train_loss = 1.8325287103652954, val_loss = 0.7248335480690002\n",
      "epoch n°2598 : train_loss = 1.8324692249298096, val_loss = 0.7287842035293579\n",
      "epoch n°2599 : train_loss = 1.8192943334579468, val_loss = 0.7288082242012024\n",
      "epoch n°2600 : train_loss = 1.8257874250411987, val_loss = 0.7282435297966003\n",
      "epoch n°2601 : train_loss = 1.8273828029632568, val_loss = 0.7295289635658264\n",
      "epoch n°2602 : train_loss = 1.817719578742981, val_loss = 0.7250197529792786\n",
      "epoch n°2603 : train_loss = 1.8233146667480469, val_loss = 0.7240433096885681\n",
      "epoch n°2604 : train_loss = 1.8271344900131226, val_loss = 0.7229897379875183\n",
      "epoch n°2605 : train_loss = 1.8172906637191772, val_loss = 0.7216301560401917\n",
      "epoch n°2606 : train_loss = 1.8409276008605957, val_loss = 0.7244214415550232\n",
      "epoch n°2607 : train_loss = 1.8349322080612183, val_loss = 0.7245575785636902\n",
      "epoch n°2608 : train_loss = 1.8186755180358887, val_loss = 0.7205470204353333\n",
      "epoch n°2609 : train_loss = 1.8288978338241577, val_loss = 0.7262224555015564\n",
      "epoch n°2610 : train_loss = 1.8216915130615234, val_loss = 0.7251704931259155\n",
      "epoch n°2611 : train_loss = 1.8261561393737793, val_loss = 0.7213544249534607\n",
      "epoch n°2612 : train_loss = 1.827565312385559, val_loss = 0.7192870378494263\n",
      "epoch n°2613 : train_loss = 1.8352938890457153, val_loss = 0.7244513630867004\n",
      "epoch n°2614 : train_loss = 1.8294240236282349, val_loss = 0.7257747650146484\n",
      "epoch n°2615 : train_loss = 1.8294050693511963, val_loss = 0.7246912717819214\n",
      "epoch n°2616 : train_loss = 1.827348232269287, val_loss = 0.7256121635437012\n",
      "epoch n°2617 : train_loss = 1.8237119913101196, val_loss = 0.7234355211257935\n",
      "epoch n°2618 : train_loss = 1.8284986019134521, val_loss = 0.7232281565666199\n",
      "epoch n°2619 : train_loss = 1.8268836736679077, val_loss = 0.7253839373588562\n",
      "epoch n°2620 : train_loss = 1.833757758140564, val_loss = 0.7216198444366455\n",
      "epoch n°2621 : train_loss = 1.8268332481384277, val_loss = 0.7240632772445679\n",
      "epoch n°2622 : train_loss = 1.8266693353652954, val_loss = 0.7231109142303467\n",
      "epoch n°2623 : train_loss = 1.8304429054260254, val_loss = 0.7253658771514893\n",
      "epoch n°2624 : train_loss = 1.8155771493911743, val_loss = 0.7237156629562378\n",
      "epoch n°2625 : train_loss = 1.824363112449646, val_loss = 0.7181549072265625\n",
      "epoch n°2626 : train_loss = 1.8324987888336182, val_loss = 0.7234051823616028\n",
      "epoch n°2627 : train_loss = 1.8203203678131104, val_loss = 0.7264954447746277\n",
      "epoch n°2628 : train_loss = 1.8233448266983032, val_loss = 0.7218142747879028\n",
      "epoch n°2629 : train_loss = 1.8324315547943115, val_loss = 0.7247780561447144\n",
      "epoch n°2630 : train_loss = 1.8193062543869019, val_loss = 0.7235874533653259\n",
      "epoch n°2631 : train_loss = 1.826931118965149, val_loss = 0.7240193486213684\n",
      "epoch n°2632 : train_loss = 1.8385800123214722, val_loss = 0.72443687915802\n",
      "epoch n°2633 : train_loss = 1.8204083442687988, val_loss = 0.7265453934669495\n",
      "epoch n°2634 : train_loss = 1.8211321830749512, val_loss = 0.7234135866165161\n",
      "epoch n°2635 : train_loss = 1.8257194757461548, val_loss = 0.7262656092643738\n",
      "epoch n°2636 : train_loss = 1.8154151439666748, val_loss = 0.7189532518386841\n",
      "epoch n°2637 : train_loss = 1.8296912908554077, val_loss = 0.728888988494873\n",
      "epoch n°2638 : train_loss = 1.82486891746521, val_loss = 0.7277124524116516\n",
      "epoch n°2639 : train_loss = 1.8328129053115845, val_loss = 0.7257380485534668\n",
      "epoch n°2640 : train_loss = 1.8253878355026245, val_loss = 0.7251867055892944\n",
      "epoch n°2641 : train_loss = 1.8257319927215576, val_loss = 0.7236748337745667\n",
      "epoch n°2642 : train_loss = 1.8188577890396118, val_loss = 0.7182267308235168\n",
      "epoch n°2643 : train_loss = 1.8178973197937012, val_loss = 0.7281278967857361\n",
      "epoch n°2644 : train_loss = 1.8215910196304321, val_loss = 0.7236466407775879\n",
      "epoch n°2645 : train_loss = 1.829367995262146, val_loss = 0.7237439751625061\n",
      "epoch n°2646 : train_loss = 1.8241322040557861, val_loss = 0.7228875756263733\n",
      "epoch n°2647 : train_loss = 1.834147334098816, val_loss = 0.7210330963134766\n",
      "epoch n°2648 : train_loss = 1.8265330791473389, val_loss = 0.7211499214172363\n",
      "epoch n°2649 : train_loss = 1.8274179697036743, val_loss = 0.7248957753181458\n",
      "epoch n°2650 : train_loss = 1.8158395290374756, val_loss = 0.7233213782310486\n",
      "epoch n°2651 : train_loss = 1.8260226249694824, val_loss = 0.7249042391777039\n",
      "epoch n°2652 : train_loss = 1.8191763162612915, val_loss = 0.7197147011756897\n",
      "epoch n°2653 : train_loss = 1.8288419246673584, val_loss = 0.7234374284744263\n",
      "epoch n°2654 : train_loss = 1.8231326341629028, val_loss = 0.722373366355896\n",
      "epoch n°2655 : train_loss = 1.8130441904067993, val_loss = 0.7227223515510559\n",
      "epoch n°2656 : train_loss = 1.8236792087554932, val_loss = 0.7281556129455566\n",
      "epoch n°2657 : train_loss = 1.822449803352356, val_loss = 0.7246360778808594\n",
      "epoch n°2658 : train_loss = 1.8399759531021118, val_loss = 0.7253407835960388\n",
      "epoch n°2659 : train_loss = 1.8284718990325928, val_loss = 0.7255085706710815\n",
      "epoch n°2660 : train_loss = 1.8270189762115479, val_loss = 0.7223029136657715\n",
      "epoch n°2661 : train_loss = 1.8325889110565186, val_loss = 0.7220430374145508\n",
      "epoch n°2662 : train_loss = 1.8362133502960205, val_loss = 0.723798394203186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2663 : train_loss = 1.8246809244155884, val_loss = 0.7266197204589844\n",
      "epoch n°2664 : train_loss = 1.8259804248809814, val_loss = 0.7287383079528809\n",
      "epoch n°2665 : train_loss = 1.8210639953613281, val_loss = 0.7224718332290649\n",
      "epoch n°2666 : train_loss = 1.825459599494934, val_loss = 0.7213611602783203\n",
      "epoch n°2667 : train_loss = 1.8386150598526, val_loss = 0.7224048376083374\n",
      "epoch n°2668 : train_loss = 1.8247604370117188, val_loss = 0.7263009548187256\n",
      "epoch n°2669 : train_loss = 1.823270320892334, val_loss = 0.7233995795249939\n",
      "epoch n°2670 : train_loss = 1.8307007551193237, val_loss = 0.7251967787742615\n",
      "epoch n°2671 : train_loss = 1.8161838054656982, val_loss = 0.7224780321121216\n",
      "epoch n°2672 : train_loss = 1.8164575099945068, val_loss = 0.7185128927230835\n",
      "epoch n°2673 : train_loss = 1.8130784034729004, val_loss = 0.7241442203521729\n",
      "epoch n°2674 : train_loss = 1.8289783000946045, val_loss = 0.7229246497154236\n",
      "epoch n°2675 : train_loss = 1.8194360733032227, val_loss = 0.7234163880348206\n",
      "epoch n°2676 : train_loss = 1.8306512832641602, val_loss = 0.7258809804916382\n",
      "epoch n°2677 : train_loss = 1.8218239545822144, val_loss = 0.7249113917350769\n",
      "epoch n°2678 : train_loss = 1.825035810470581, val_loss = 0.7212985157966614\n",
      "epoch n°2679 : train_loss = 1.8441903591156006, val_loss = 0.7278268337249756\n",
      "epoch n°2680 : train_loss = 1.8323285579681396, val_loss = 0.7231215834617615\n",
      "epoch n°2681 : train_loss = 1.8300316333770752, val_loss = 0.7305721640586853\n",
      "epoch n°2682 : train_loss = 1.821791648864746, val_loss = 0.7218294739723206\n",
      "epoch n°2683 : train_loss = 1.8279662132263184, val_loss = 0.7259844541549683\n",
      "epoch n°2684 : train_loss = 1.822373390197754, val_loss = 0.7236394882202148\n",
      "epoch n°2685 : train_loss = 1.8209749460220337, val_loss = 0.7240777611732483\n",
      "epoch n°2686 : train_loss = 1.8353683948516846, val_loss = 0.7204901576042175\n",
      "epoch n°2687 : train_loss = 1.8185064792633057, val_loss = 0.7195112705230713\n",
      "epoch n°2688 : train_loss = 1.8199056386947632, val_loss = 0.7246518731117249\n",
      "epoch n°2689 : train_loss = 1.8327374458312988, val_loss = 0.7217683792114258\n",
      "epoch n°2690 : train_loss = 1.8219821453094482, val_loss = 0.722194254398346\n",
      "epoch n°2691 : train_loss = 1.8269449472427368, val_loss = 0.7180622816085815\n",
      "epoch n°2692 : train_loss = 1.8413687944412231, val_loss = 0.7238035798072815\n",
      "epoch n°2693 : train_loss = 1.820535659790039, val_loss = 0.7220127582550049\n",
      "epoch n°2694 : train_loss = 1.8251726627349854, val_loss = 0.7224869132041931\n",
      "epoch n°2695 : train_loss = 1.823408603668213, val_loss = 0.7247852683067322\n",
      "epoch n°2696 : train_loss = 1.8226404190063477, val_loss = 0.728430449962616\n",
      "epoch n°2697 : train_loss = 1.820877194404602, val_loss = 0.7239696979522705\n",
      "epoch n°2698 : train_loss = 1.8238136768341064, val_loss = 0.7228139638900757\n",
      "epoch n°2699 : train_loss = 1.8213200569152832, val_loss = 0.7253871560096741\n",
      "epoch n°2700 : train_loss = 1.8209657669067383, val_loss = 0.7261636257171631\n",
      "epoch n°2701 : train_loss = 1.817667841911316, val_loss = 0.7219430804252625\n",
      "epoch n°2702 : train_loss = 1.8205199241638184, val_loss = 0.7238867878913879\n",
      "epoch n°2703 : train_loss = 1.8278939723968506, val_loss = 0.7266375422477722\n",
      "epoch n°2704 : train_loss = 1.8213326930999756, val_loss = 0.7214686274528503\n",
      "epoch n°2705 : train_loss = 1.8172011375427246, val_loss = 0.7233765721321106\n",
      "epoch n°2706 : train_loss = 1.8220276832580566, val_loss = 0.7241796255111694\n",
      "epoch n°2707 : train_loss = 1.8345861434936523, val_loss = 0.7281494736671448\n",
      "epoch n°2708 : train_loss = 1.8250532150268555, val_loss = 0.7202115654945374\n",
      "epoch n°2709 : train_loss = 1.8276149034500122, val_loss = 0.7212883830070496\n",
      "epoch n°2710 : train_loss = 1.8303478956222534, val_loss = 0.7243276834487915\n",
      "epoch n°2711 : train_loss = 1.8271994590759277, val_loss = 0.725353479385376\n",
      "epoch n°2712 : train_loss = 1.8152908086776733, val_loss = 0.721019983291626\n",
      "epoch n°2713 : train_loss = 1.8142917156219482, val_loss = 0.7254137992858887\n",
      "epoch n°2714 : train_loss = 1.8248963356018066, val_loss = 0.7263303399085999\n",
      "epoch n°2715 : train_loss = 1.823909878730774, val_loss = 0.7232387661933899\n",
      "epoch n°2716 : train_loss = 1.8308626413345337, val_loss = 0.7197808027267456\n",
      "epoch n°2717 : train_loss = 1.8207416534423828, val_loss = 0.7226426005363464\n",
      "epoch n°2718 : train_loss = 1.8223075866699219, val_loss = 0.7203282713890076\n",
      "epoch n°2719 : train_loss = 1.8293969631195068, val_loss = 0.7239324450492859\n",
      "epoch n°2720 : train_loss = 1.8180876970291138, val_loss = 0.723551869392395\n",
      "epoch n°2721 : train_loss = 1.81318199634552, val_loss = 0.7213575839996338\n",
      "epoch n°2722 : train_loss = 1.8248761892318726, val_loss = 0.7252248525619507\n",
      "epoch n°2723 : train_loss = 1.817463994026184, val_loss = 0.7281094193458557\n",
      "epoch n°2724 : train_loss = 1.8087252378463745, val_loss = 0.7236361503601074\n",
      "epoch n°2725 : train_loss = 1.8251452445983887, val_loss = 0.7220941781997681\n",
      "epoch n°2726 : train_loss = 1.8265936374664307, val_loss = 0.7204572558403015\n",
      "epoch n°2727 : train_loss = 1.82072913646698, val_loss = 0.7242462038993835\n",
      "epoch n°2728 : train_loss = 1.8225358724594116, val_loss = 0.7222864031791687\n",
      "epoch n°2729 : train_loss = 1.8305824995040894, val_loss = 0.7222485542297363\n",
      "epoch n°2730 : train_loss = 1.822156548500061, val_loss = 0.7228468060493469\n",
      "epoch n°2731 : train_loss = 1.825015902519226, val_loss = 0.72576504945755\n",
      "epoch n°2732 : train_loss = 1.8248140811920166, val_loss = 0.7248094081878662\n",
      "epoch n°2733 : train_loss = 1.8211755752563477, val_loss = 0.7269418835639954\n",
      "epoch n°2734 : train_loss = 1.8199812173843384, val_loss = 0.7216417789459229\n",
      "epoch n°2735 : train_loss = 1.8318723440170288, val_loss = 0.723967969417572\n",
      "epoch n°2736 : train_loss = 1.8162974119186401, val_loss = 0.7210657000541687\n",
      "epoch n°2737 : train_loss = 1.8218605518341064, val_loss = 0.7241337895393372\n",
      "epoch n°2738 : train_loss = 1.8254069089889526, val_loss = 0.7228920459747314\n",
      "epoch n°2739 : train_loss = 1.8163197040557861, val_loss = 0.7200722694396973\n",
      "epoch n°2740 : train_loss = 1.8137999773025513, val_loss = 0.7264426350593567\n",
      "epoch n°2741 : train_loss = 1.8185162544250488, val_loss = 0.7226425409317017\n",
      "epoch n°2742 : train_loss = 1.8192120790481567, val_loss = 0.7242251634597778\n",
      "epoch n°2743 : train_loss = 1.816183090209961, val_loss = 0.7276144623756409\n",
      "epoch n°2744 : train_loss = 1.8111286163330078, val_loss = 0.7241888642311096\n",
      "epoch n°2745 : train_loss = 1.8253412246704102, val_loss = 0.7249029278755188\n",
      "epoch n°2746 : train_loss = 1.8162879943847656, val_loss = 0.7217469215393066\n",
      "epoch n°2747 : train_loss = 1.8161619901657104, val_loss = 0.72060626745224\n",
      "epoch n°2748 : train_loss = 1.8205920457839966, val_loss = 0.7245547771453857\n",
      "epoch n°2749 : train_loss = 1.8066152334213257, val_loss = 0.7235497832298279\n",
      "epoch n°2750 : train_loss = 1.8107856512069702, val_loss = 0.7260545492172241\n",
      "epoch n°2751 : train_loss = 1.8245456218719482, val_loss = 0.7228790521621704\n",
      "epoch n°2752 : train_loss = 1.8174089193344116, val_loss = 0.7283307313919067\n",
      "epoch n°2753 : train_loss = 1.82206130027771, val_loss = 0.7283134460449219\n",
      "epoch n°2754 : train_loss = 1.8253134489059448, val_loss = 0.7266424298286438\n",
      "epoch n°2755 : train_loss = 1.8176525831222534, val_loss = 0.7247290015220642\n",
      "epoch n°2756 : train_loss = 1.8243306875228882, val_loss = 0.7226551175117493\n",
      "epoch n°2757 : train_loss = 1.8176586627960205, val_loss = 0.7215946316719055\n",
      "epoch n°2758 : train_loss = 1.8174301385879517, val_loss = 0.7210524082183838\n",
      "epoch n°2759 : train_loss = 1.8193228244781494, val_loss = 0.7257512211799622\n",
      "epoch n°2760 : train_loss = 1.8135689496994019, val_loss = 0.721750020980835\n",
      "epoch n°2761 : train_loss = 1.8284183740615845, val_loss = 0.719758927822113\n",
      "epoch n°2762 : train_loss = 1.8135106563568115, val_loss = 0.7228125333786011\n",
      "epoch n°2763 : train_loss = 1.8140379190444946, val_loss = 0.7287612557411194\n",
      "epoch n°2764 : train_loss = 1.816598892211914, val_loss = 0.722846508026123\n",
      "epoch n°2765 : train_loss = 1.823779821395874, val_loss = 0.7224656343460083\n",
      "epoch n°2766 : train_loss = 1.816860318183899, val_loss = 0.723808765411377\n",
      "epoch n°2767 : train_loss = 1.8172324895858765, val_loss = 0.7199032306671143\n",
      "epoch n°2768 : train_loss = 1.8245265483856201, val_loss = 0.7196632027626038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2769 : train_loss = 1.822658658027649, val_loss = 0.7239599823951721\n",
      "epoch n°2770 : train_loss = 1.818899393081665, val_loss = 0.7152421474456787\n",
      "epoch n°2771 : train_loss = 1.830631136894226, val_loss = 0.7218884229660034\n",
      "epoch n°2772 : train_loss = 1.8238487243652344, val_loss = 0.721062183380127\n",
      "epoch n°2773 : train_loss = 1.8154419660568237, val_loss = 0.7260481119155884\n",
      "epoch n°2774 : train_loss = 1.8169080018997192, val_loss = 0.726008951663971\n",
      "epoch n°2775 : train_loss = 1.8169819116592407, val_loss = 0.725813627243042\n",
      "epoch n°2776 : train_loss = 1.8158543109893799, val_loss = 0.7262206077575684\n",
      "epoch n°2777 : train_loss = 1.817684292793274, val_loss = 0.7211481928825378\n",
      "epoch n°2778 : train_loss = 1.8219197988510132, val_loss = 0.7271547317504883\n",
      "epoch n°2779 : train_loss = 1.8169549703598022, val_loss = 0.7225770354270935\n",
      "epoch n°2780 : train_loss = 1.8112776279449463, val_loss = 0.724013090133667\n",
      "epoch n°2781 : train_loss = 1.822646975517273, val_loss = 0.7261728644371033\n",
      "epoch n°2782 : train_loss = 1.8194180727005005, val_loss = 0.7238634824752808\n",
      "epoch n°2783 : train_loss = 1.828801155090332, val_loss = 0.7213252782821655\n",
      "epoch n°2784 : train_loss = 1.8045916557312012, val_loss = 0.7280423641204834\n",
      "epoch n°2785 : train_loss = 1.8269070386886597, val_loss = 0.7218245267868042\n",
      "epoch n°2786 : train_loss = 1.8279234170913696, val_loss = 0.7238531708717346\n",
      "epoch n°2787 : train_loss = 1.818321943283081, val_loss = 0.7217282056808472\n",
      "epoch n°2788 : train_loss = 1.8137482404708862, val_loss = 0.723702609539032\n",
      "epoch n°2789 : train_loss = 1.8112772703170776, val_loss = 0.7202634215354919\n",
      "epoch n°2790 : train_loss = 1.8274258375167847, val_loss = 0.7272704839706421\n",
      "epoch n°2791 : train_loss = 1.8153495788574219, val_loss = 0.7244008779525757\n",
      "epoch n°2792 : train_loss = 1.8150839805603027, val_loss = 0.7249094247817993\n",
      "epoch n°2793 : train_loss = 1.820444107055664, val_loss = 0.7270504832267761\n",
      "epoch n°2794 : train_loss = 1.8222981691360474, val_loss = 0.7278172969818115\n",
      "epoch n°2795 : train_loss = 1.8180276155471802, val_loss = 0.7185529470443726\n",
      "epoch n°2796 : train_loss = 1.8195538520812988, val_loss = 0.7188615798950195\n",
      "epoch n°2797 : train_loss = 1.8160256147384644, val_loss = 0.7241670489311218\n",
      "epoch n°2798 : train_loss = 1.8191759586334229, val_loss = 0.7281048893928528\n",
      "epoch n°2799 : train_loss = 1.8249900341033936, val_loss = 0.7254101634025574\n",
      "epoch n°2800 : train_loss = 1.8250939846038818, val_loss = 0.7222714424133301\n",
      "epoch n°2801 : train_loss = 1.8192604780197144, val_loss = 0.7253894805908203\n",
      "epoch n°2802 : train_loss = 1.8204494714736938, val_loss = 0.722514808177948\n",
      "epoch n°2803 : train_loss = 1.8098665475845337, val_loss = 0.7206707000732422\n",
      "epoch n°2804 : train_loss = 1.816641092300415, val_loss = 0.7234384417533875\n",
      "epoch n°2805 : train_loss = 1.81708562374115, val_loss = 0.7225995063781738\n",
      "epoch n°2806 : train_loss = 1.8239532709121704, val_loss = 0.7257605791091919\n",
      "epoch n°2807 : train_loss = 1.8155943155288696, val_loss = 0.7248151302337646\n",
      "epoch n°2808 : train_loss = 1.8221639394760132, val_loss = 0.7207459211349487\n",
      "epoch n°2809 : train_loss = 1.8153376579284668, val_loss = 0.7243054509162903\n",
      "epoch n°2810 : train_loss = 1.8139150142669678, val_loss = 0.7223730087280273\n",
      "epoch n°2811 : train_loss = 1.8214703798294067, val_loss = 0.7219842672348022\n",
      "epoch n°2812 : train_loss = 1.8221410512924194, val_loss = 0.7283067107200623\n",
      "epoch n°2813 : train_loss = 1.813888430595398, val_loss = 0.7247692346572876\n",
      "epoch n°2814 : train_loss = 1.8123431205749512, val_loss = 0.7239618897438049\n",
      "epoch n°2815 : train_loss = 1.8173023462295532, val_loss = 0.7255648374557495\n",
      "epoch n°2816 : train_loss = 1.8170396089553833, val_loss = 0.7248756885528564\n",
      "epoch n°2817 : train_loss = 1.8116214275360107, val_loss = 0.7218019962310791\n",
      "epoch n°2818 : train_loss = 1.8228365182876587, val_loss = 0.72247713804245\n",
      "epoch n°2819 : train_loss = 1.8264974355697632, val_loss = 0.7192789316177368\n",
      "epoch n°2820 : train_loss = 1.8155924081802368, val_loss = 0.7213053703308105\n",
      "epoch n°2821 : train_loss = 1.8158127069473267, val_loss = 0.7266030311584473\n",
      "epoch n°2822 : train_loss = 1.8156198263168335, val_loss = 0.7231615781784058\n",
      "epoch n°2823 : train_loss = 1.818834662437439, val_loss = 0.7236484289169312\n",
      "epoch n°2824 : train_loss = 1.8115869760513306, val_loss = 0.7194562554359436\n",
      "epoch n°2825 : train_loss = 1.8089210987091064, val_loss = 0.7229944467544556\n",
      "epoch n°2826 : train_loss = 1.8220494985580444, val_loss = 0.7228016257286072\n",
      "epoch n°2827 : train_loss = 1.813565731048584, val_loss = 0.7222818732261658\n",
      "epoch n°2828 : train_loss = 1.8210407495498657, val_loss = 0.7293031215667725\n",
      "epoch n°2829 : train_loss = 1.8115489482879639, val_loss = 0.7186641097068787\n",
      "epoch n°2830 : train_loss = 1.8244203329086304, val_loss = 0.7213248014450073\n",
      "epoch n°2831 : train_loss = 1.8066697120666504, val_loss = 0.7192824482917786\n",
      "epoch n°2832 : train_loss = 1.8134191036224365, val_loss = 0.725083589553833\n",
      "epoch n°2833 : train_loss = 1.813350796699524, val_loss = 0.7233412265777588\n",
      "epoch n°2834 : train_loss = 1.8140590190887451, val_loss = 0.7255991697311401\n",
      "epoch n°2835 : train_loss = 1.8173718452453613, val_loss = 0.7258227467536926\n",
      "epoch n°2836 : train_loss = 1.825783371925354, val_loss = 0.7273458242416382\n",
      "epoch n°2837 : train_loss = 1.807849407196045, val_loss = 0.7214393615722656\n",
      "epoch n°2838 : train_loss = 1.8127723932266235, val_loss = 0.7181478142738342\n",
      "epoch n°2839 : train_loss = 1.8247052431106567, val_loss = 0.7266117930412292\n",
      "epoch n°2840 : train_loss = 1.8245017528533936, val_loss = 0.7221370339393616\n",
      "epoch n°2841 : train_loss = 1.811145544052124, val_loss = 0.7192610502243042\n",
      "epoch n°2842 : train_loss = 1.816083550453186, val_loss = 0.7205781936645508\n",
      "epoch n°2843 : train_loss = 1.8200339078903198, val_loss = 0.7266106605529785\n",
      "epoch n°2844 : train_loss = 1.8161041736602783, val_loss = 0.7235556244850159\n",
      "epoch n°2845 : train_loss = 1.8080906867980957, val_loss = 0.7221674919128418\n",
      "epoch n°2846 : train_loss = 1.8222938776016235, val_loss = 0.724565327167511\n",
      "epoch n°2847 : train_loss = 1.8107095956802368, val_loss = 0.7231032252311707\n",
      "epoch n°2848 : train_loss = 1.8162760734558105, val_loss = 0.7236171960830688\n",
      "epoch n°2849 : train_loss = 1.8260576725006104, val_loss = 0.7268956303596497\n",
      "epoch n°2850 : train_loss = 1.824055790901184, val_loss = 0.7233087420463562\n",
      "epoch n°2851 : train_loss = 1.8155821561813354, val_loss = 0.7201312780380249\n",
      "epoch n°2852 : train_loss = 1.81342351436615, val_loss = 0.7285768985748291\n",
      "epoch n°2853 : train_loss = 1.8141757249832153, val_loss = 0.7192278504371643\n",
      "epoch n°2854 : train_loss = 1.8172156810760498, val_loss = 0.7264783978462219\n",
      "epoch n°2855 : train_loss = 1.817401647567749, val_loss = 0.7207727432250977\n",
      "epoch n°2856 : train_loss = 1.8162137269973755, val_loss = 0.7272745966911316\n",
      "epoch n°2857 : train_loss = 1.8180780410766602, val_loss = 0.724018931388855\n",
      "epoch n°2858 : train_loss = 1.81596040725708, val_loss = 0.7225250601768494\n",
      "epoch n°2859 : train_loss = 1.815446376800537, val_loss = 0.7242704033851624\n",
      "epoch n°2860 : train_loss = 1.8181445598602295, val_loss = 0.7296572923660278\n",
      "epoch n°2861 : train_loss = 1.817988395690918, val_loss = 0.7261698246002197\n",
      "epoch n°2862 : train_loss = 1.8093873262405396, val_loss = 0.719531774520874\n",
      "epoch n°2863 : train_loss = 1.8124518394470215, val_loss = 0.7228375673294067\n",
      "epoch n°2864 : train_loss = 1.8173022270202637, val_loss = 0.7193028330802917\n",
      "epoch n°2865 : train_loss = 1.8060364723205566, val_loss = 0.7228905558586121\n",
      "epoch n°2866 : train_loss = 1.821325421333313, val_loss = 0.7225147485733032\n",
      "epoch n°2867 : train_loss = 1.8173550367355347, val_loss = 0.7256637811660767\n",
      "epoch n°2868 : train_loss = 1.802161455154419, val_loss = 0.7217317819595337\n",
      "epoch n°2869 : train_loss = 1.8136177062988281, val_loss = 0.7207274436950684\n",
      "epoch n°2870 : train_loss = 1.820554256439209, val_loss = 0.7253657579421997\n",
      "epoch n°2871 : train_loss = 1.8260959386825562, val_loss = 0.7203238010406494\n",
      "epoch n°2872 : train_loss = 1.8101602792739868, val_loss = 0.729947566986084\n",
      "epoch n°2873 : train_loss = 1.808655023574829, val_loss = 0.7200936079025269\n",
      "epoch n°2874 : train_loss = 1.826548457145691, val_loss = 0.72487473487854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2875 : train_loss = 1.819138526916504, val_loss = 0.719993531703949\n",
      "epoch n°2876 : train_loss = 1.8140172958374023, val_loss = 0.7217025756835938\n",
      "epoch n°2877 : train_loss = 1.8120977878570557, val_loss = 0.7266048789024353\n",
      "epoch n°2878 : train_loss = 1.8143881559371948, val_loss = 0.7250816226005554\n",
      "epoch n°2879 : train_loss = 1.8053712844848633, val_loss = 0.7241615653038025\n",
      "epoch n°2880 : train_loss = 1.820081353187561, val_loss = 0.724288284778595\n",
      "epoch n°2881 : train_loss = 1.8139091730117798, val_loss = 0.7257011532783508\n",
      "epoch n°2882 : train_loss = 1.8221091032028198, val_loss = 0.7233465313911438\n",
      "epoch n°2883 : train_loss = 1.8147152662277222, val_loss = 0.7179638743400574\n",
      "epoch n°2884 : train_loss = 1.8164615631103516, val_loss = 0.7202329635620117\n",
      "epoch n°2885 : train_loss = 1.8096107244491577, val_loss = 0.7210105657577515\n",
      "epoch n°2886 : train_loss = 1.824763536453247, val_loss = 0.725257396697998\n",
      "epoch n°2887 : train_loss = 1.8089911937713623, val_loss = 0.7271430492401123\n",
      "epoch n°2888 : train_loss = 1.8216941356658936, val_loss = 0.7250927686691284\n",
      "epoch n°2889 : train_loss = 1.8193538188934326, val_loss = 0.7280550599098206\n",
      "epoch n°2890 : train_loss = 1.8180618286132812, val_loss = 0.7292381525039673\n",
      "epoch n°2891 : train_loss = 1.8134790658950806, val_loss = 0.721817672252655\n",
      "epoch n°2892 : train_loss = 1.8178431987762451, val_loss = 0.7230070233345032\n",
      "epoch n°2893 : train_loss = 1.8202662467956543, val_loss = 0.7244371771812439\n",
      "epoch n°2894 : train_loss = 1.8034143447875977, val_loss = 0.7238708138465881\n",
      "epoch n°2895 : train_loss = 1.8165266513824463, val_loss = 0.7233611941337585\n",
      "epoch n°2896 : train_loss = 1.8103134632110596, val_loss = 0.7247611880302429\n",
      "epoch n°2897 : train_loss = 1.8123334646224976, val_loss = 0.7259151935577393\n",
      "epoch n°2898 : train_loss = 1.8146345615386963, val_loss = 0.7249509692192078\n",
      "epoch n°2899 : train_loss = 1.8195054531097412, val_loss = 0.7256958484649658\n",
      "epoch n°2900 : train_loss = 1.820131778717041, val_loss = 0.7193207144737244\n",
      "epoch n°2901 : train_loss = 1.82198965549469, val_loss = 0.7230557203292847\n",
      "epoch n°2902 : train_loss = 1.8097268342971802, val_loss = 0.721832811832428\n",
      "epoch n°2903 : train_loss = 1.80243980884552, val_loss = 0.7229208946228027\n",
      "epoch n°2904 : train_loss = 1.8218541145324707, val_loss = 0.7179214358329773\n",
      "epoch n°2905 : train_loss = 1.8128355741500854, val_loss = 0.7211380004882812\n",
      "epoch n°2906 : train_loss = 1.8083189725875854, val_loss = 0.7240057587623596\n",
      "epoch n°2907 : train_loss = 1.8062465190887451, val_loss = 0.7258887887001038\n",
      "epoch n°2908 : train_loss = 1.8063219785690308, val_loss = 0.7228257060050964\n",
      "epoch n°2909 : train_loss = 1.805426001548767, val_loss = 0.7253737449645996\n",
      "epoch n°2910 : train_loss = 1.8196402788162231, val_loss = 0.7177308201789856\n",
      "epoch n°2911 : train_loss = 1.8052235841751099, val_loss = 0.7258912920951843\n",
      "epoch n°2912 : train_loss = 1.8275964260101318, val_loss = 0.7236458659172058\n",
      "epoch n°2913 : train_loss = 1.8180546760559082, val_loss = 0.7268179059028625\n",
      "epoch n°2914 : train_loss = 1.8180001974105835, val_loss = 0.7227435111999512\n",
      "epoch n°2915 : train_loss = 1.8244396448135376, val_loss = 0.7229041457176208\n",
      "epoch n°2916 : train_loss = 1.8065024614334106, val_loss = 0.7254982590675354\n",
      "epoch n°2917 : train_loss = 1.8169500827789307, val_loss = 0.729224443435669\n",
      "epoch n°2918 : train_loss = 1.807363748550415, val_loss = 0.7261343002319336\n",
      "epoch n°2919 : train_loss = 1.8162050247192383, val_loss = 0.7307853102684021\n",
      "epoch n°2920 : train_loss = 1.814435601234436, val_loss = 0.7229148745536804\n",
      "epoch n°2921 : train_loss = 1.8116393089294434, val_loss = 0.7264143824577332\n",
      "epoch n°2922 : train_loss = 1.8138206005096436, val_loss = 0.7255083322525024\n",
      "epoch n°2923 : train_loss = 1.8071515560150146, val_loss = 0.7229648232460022\n",
      "epoch n°2924 : train_loss = 1.8188632726669312, val_loss = 0.7192769646644592\n",
      "epoch n°2925 : train_loss = 1.807922601699829, val_loss = 0.7172328233718872\n",
      "epoch n°2926 : train_loss = 1.8143773078918457, val_loss = 0.7209056615829468\n",
      "epoch n°2927 : train_loss = 1.8071188926696777, val_loss = 0.7277929782867432\n",
      "epoch n°2928 : train_loss = 1.8124396800994873, val_loss = 0.7210490107536316\n",
      "epoch n°2929 : train_loss = 1.8220844268798828, val_loss = 0.7285134196281433\n",
      "epoch n°2930 : train_loss = 1.8163021802902222, val_loss = 0.7217108011245728\n",
      "epoch n°2931 : train_loss = 1.8184013366699219, val_loss = 0.7190146446228027\n",
      "epoch n°2932 : train_loss = 1.8011738061904907, val_loss = 0.7199093699455261\n",
      "epoch n°2933 : train_loss = 1.8076974153518677, val_loss = 0.7238418459892273\n",
      "epoch n°2934 : train_loss = 1.810822606086731, val_loss = 0.7237063646316528\n",
      "epoch n°2935 : train_loss = 1.8053412437438965, val_loss = 0.7207531929016113\n",
      "epoch n°2936 : train_loss = 1.8144512176513672, val_loss = 0.7248134613037109\n",
      "epoch n°2937 : train_loss = 1.8178834915161133, val_loss = 0.7180410027503967\n",
      "epoch n°2938 : train_loss = 1.8189126253128052, val_loss = 0.7262699604034424\n",
      "epoch n°2939 : train_loss = 1.8098937273025513, val_loss = 0.723735511302948\n",
      "epoch n°2940 : train_loss = 1.8172283172607422, val_loss = 0.7200915217399597\n",
      "epoch n°2941 : train_loss = 1.812445878982544, val_loss = 0.7203523516654968\n",
      "epoch n°2942 : train_loss = 1.8067309856414795, val_loss = 0.7257745862007141\n",
      "epoch n°2943 : train_loss = 1.8196839094161987, val_loss = 0.7219697833061218\n",
      "epoch n°2944 : train_loss = 1.819293737411499, val_loss = 0.7226632833480835\n",
      "epoch n°2945 : train_loss = 1.812292218208313, val_loss = 0.7221365571022034\n",
      "epoch n°2946 : train_loss = 1.805811882019043, val_loss = 0.7252596020698547\n",
      "epoch n°2947 : train_loss = 1.8111963272094727, val_loss = 0.7208788394927979\n",
      "epoch n°2948 : train_loss = 1.8013826608657837, val_loss = 0.7271627187728882\n",
      "epoch n°2949 : train_loss = 1.8127497434616089, val_loss = 0.7257582545280457\n",
      "epoch n°2950 : train_loss = 1.8089977502822876, val_loss = 0.7255880236625671\n",
      "epoch n°2951 : train_loss = 1.8231743574142456, val_loss = 0.7224097847938538\n",
      "epoch n°2952 : train_loss = 1.8076493740081787, val_loss = 0.7206394076347351\n",
      "epoch n°2953 : train_loss = 1.8188254833221436, val_loss = 0.7243520021438599\n",
      "epoch n°2954 : train_loss = 1.8143390417099, val_loss = 0.7200970649719238\n",
      "epoch n°2955 : train_loss = 1.8192085027694702, val_loss = 0.7186862230300903\n",
      "epoch n°2956 : train_loss = 1.8147778511047363, val_loss = 0.7229848504066467\n",
      "epoch n°2957 : train_loss = 1.8103570938110352, val_loss = 0.7225610613822937\n",
      "epoch n°2958 : train_loss = 1.810360074043274, val_loss = 0.7216525673866272\n",
      "epoch n°2959 : train_loss = 1.8151514530181885, val_loss = 0.7299034595489502\n",
      "epoch n°2960 : train_loss = 1.8143410682678223, val_loss = 0.7190391421318054\n",
      "epoch n°2961 : train_loss = 1.8054003715515137, val_loss = 0.7254990935325623\n",
      "epoch n°2962 : train_loss = 1.8133310079574585, val_loss = 0.7245191931724548\n",
      "epoch n°2963 : train_loss = 1.8119946718215942, val_loss = 0.7229588031768799\n",
      "epoch n°2964 : train_loss = 1.8233742713928223, val_loss = 0.7208589911460876\n",
      "epoch n°2965 : train_loss = 1.7984764575958252, val_loss = 0.7186741232872009\n",
      "epoch n°2966 : train_loss = 1.8113303184509277, val_loss = 0.7225791811943054\n",
      "epoch n°2967 : train_loss = 1.8128997087478638, val_loss = 0.7207852005958557\n",
      "epoch n°2968 : train_loss = 1.8139357566833496, val_loss = 0.7200227975845337\n",
      "epoch n°2969 : train_loss = 1.8056970834732056, val_loss = 0.720031201839447\n",
      "epoch n°2970 : train_loss = 1.8141982555389404, val_loss = 0.7230332493782043\n",
      "epoch n°2971 : train_loss = 1.797974944114685, val_loss = 0.7272867560386658\n",
      "epoch n°2972 : train_loss = 1.8101894855499268, val_loss = 0.716614305973053\n",
      "epoch n°2973 : train_loss = 1.8163422346115112, val_loss = 0.7297694683074951\n",
      "epoch n°2974 : train_loss = 1.814100742340088, val_loss = 0.7187455296516418\n",
      "epoch n°2975 : train_loss = 1.8039683103561401, val_loss = 0.7254914045333862\n",
      "epoch n°2976 : train_loss = 1.7992579936981201, val_loss = 0.7224426865577698\n",
      "epoch n°2977 : train_loss = 1.8168588876724243, val_loss = 0.7216811180114746\n",
      "epoch n°2978 : train_loss = 1.8033840656280518, val_loss = 0.7256174683570862\n",
      "epoch n°2979 : train_loss = 1.8033204078674316, val_loss = 0.724036693572998\n",
      "epoch n°2980 : train_loss = 1.8104972839355469, val_loss = 0.7201184034347534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°2981 : train_loss = 1.8149760961532593, val_loss = 0.7199234366416931\n",
      "epoch n°2982 : train_loss = 1.8109521865844727, val_loss = 0.7189720273017883\n",
      "epoch n°2983 : train_loss = 1.8159291744232178, val_loss = 0.7202198505401611\n",
      "epoch n°2984 : train_loss = 1.8055092096328735, val_loss = 0.7252923250198364\n",
      "epoch n°2985 : train_loss = 1.8113216161727905, val_loss = 0.721525251865387\n",
      "epoch n°2986 : train_loss = 1.8119187355041504, val_loss = 0.7183666229248047\n",
      "epoch n°2987 : train_loss = 1.806330919265747, val_loss = 0.719925582408905\n",
      "epoch n°2988 : train_loss = 1.8107417821884155, val_loss = 0.7231947779655457\n",
      "epoch n°2989 : train_loss = 1.8139245510101318, val_loss = 0.7199735641479492\n",
      "epoch n°2990 : train_loss = 1.8107964992523193, val_loss = 0.7219161987304688\n",
      "epoch n°2991 : train_loss = 1.8143404722213745, val_loss = 0.7247006893157959\n",
      "epoch n°2992 : train_loss = 1.8126814365386963, val_loss = 0.7209309339523315\n",
      "epoch n°2993 : train_loss = 1.80693519115448, val_loss = 0.7251386046409607\n",
      "epoch n°2994 : train_loss = 1.8036507368087769, val_loss = 0.7268657684326172\n",
      "epoch n°2995 : train_loss = 1.8071026802062988, val_loss = 0.7256938815116882\n",
      "epoch n°2996 : train_loss = 1.803781270980835, val_loss = 0.7254633903503418\n",
      "epoch n°2997 : train_loss = 1.8131881952285767, val_loss = 0.7193784117698669\n",
      "epoch n°2998 : train_loss = 1.8059293031692505, val_loss = 0.7285003662109375\n",
      "epoch n°2999 : train_loss = 1.810693621635437, val_loss = 0.7193378806114197\n",
      "epoch n°3000 : train_loss = 1.8172045946121216, val_loss = 0.7197527885437012\n",
      "epoch n°3001 : train_loss = 1.8084698915481567, val_loss = 0.725005567073822\n",
      "epoch n°3002 : train_loss = 1.8007166385650635, val_loss = 0.7240821123123169\n",
      "epoch n°3003 : train_loss = 1.8082011938095093, val_loss = 0.7212139368057251\n",
      "epoch n°3004 : train_loss = 1.8084796667099, val_loss = 0.7242476940155029\n",
      "epoch n°3005 : train_loss = 1.8068125247955322, val_loss = 0.720730721950531\n",
      "epoch n°3006 : train_loss = 1.795960783958435, val_loss = 0.7226923108100891\n",
      "epoch n°3007 : train_loss = 1.8132463693618774, val_loss = 0.7240033149719238\n",
      "epoch n°3008 : train_loss = 1.7999821901321411, val_loss = 0.7185726761817932\n",
      "epoch n°3009 : train_loss = 1.7970441579818726, val_loss = 0.7221679091453552\n",
      "epoch n°3010 : train_loss = 1.7988616228103638, val_loss = 0.7190831899642944\n",
      "epoch n°3011 : train_loss = 1.8139005899429321, val_loss = 0.7276262640953064\n",
      "epoch n°3012 : train_loss = 1.8090964555740356, val_loss = 0.7224429845809937\n",
      "epoch n°3013 : train_loss = 1.8212815523147583, val_loss = 0.7193017601966858\n",
      "epoch n°3014 : train_loss = 1.8022435903549194, val_loss = 0.725928544998169\n",
      "epoch n°3015 : train_loss = 1.8064498901367188, val_loss = 0.7244675159454346\n",
      "epoch n°3016 : train_loss = 1.8111262321472168, val_loss = 0.7200161218643188\n",
      "epoch n°3017 : train_loss = 1.8166029453277588, val_loss = 0.718966007232666\n",
      "epoch n°3018 : train_loss = 1.808074712753296, val_loss = 0.717045247554779\n",
      "epoch n°3019 : train_loss = 1.8041419982910156, val_loss = 0.7192282676696777\n",
      "epoch n°3020 : train_loss = 1.8082681894302368, val_loss = 0.7252950668334961\n",
      "epoch n°3021 : train_loss = 1.8068243265151978, val_loss = 0.7213353514671326\n",
      "epoch n°3022 : train_loss = 1.8119876384735107, val_loss = 0.7211788892745972\n",
      "epoch n°3023 : train_loss = 1.8113542795181274, val_loss = 0.7218078970909119\n",
      "epoch n°3024 : train_loss = 1.815797209739685, val_loss = 0.7248619794845581\n",
      "epoch n°3025 : train_loss = 1.8118878602981567, val_loss = 0.724230170249939\n",
      "epoch n°3026 : train_loss = 1.8123085498809814, val_loss = 0.722350001335144\n",
      "epoch n°3027 : train_loss = 1.8044434785842896, val_loss = 0.721110463142395\n",
      "epoch n°3028 : train_loss = 1.8043967485427856, val_loss = 0.7218179702758789\n",
      "epoch n°3029 : train_loss = 1.8088542222976685, val_loss = 0.7252588272094727\n",
      "epoch n°3030 : train_loss = 1.8075506687164307, val_loss = 0.7190427184104919\n",
      "epoch n°3031 : train_loss = 1.8065659999847412, val_loss = 0.7239479422569275\n",
      "epoch n°3032 : train_loss = 1.8044185638427734, val_loss = 0.7237659096717834\n",
      "epoch n°3033 : train_loss = 1.806279182434082, val_loss = 0.7258460521697998\n",
      "epoch n°3034 : train_loss = 1.8197071552276611, val_loss = 0.719264805316925\n",
      "epoch n°3035 : train_loss = 1.803515911102295, val_loss = 0.7150875329971313\n",
      "epoch n°3036 : train_loss = 1.8021372556686401, val_loss = 0.7179391980171204\n",
      "epoch n°3037 : train_loss = 1.8045786619186401, val_loss = 0.7222129702568054\n",
      "epoch n°3038 : train_loss = 1.8041458129882812, val_loss = 0.722429633140564\n",
      "epoch n°3039 : train_loss = 1.8029348850250244, val_loss = 0.7228627800941467\n",
      "epoch n°3040 : train_loss = 1.7992920875549316, val_loss = 0.7266268134117126\n",
      "epoch n°3041 : train_loss = 1.8153693675994873, val_loss = 0.7250941395759583\n",
      "epoch n°3042 : train_loss = 1.804137945175171, val_loss = 0.724328875541687\n",
      "epoch n°3043 : train_loss = 1.7994011640548706, val_loss = 0.728415310382843\n",
      "epoch n°3044 : train_loss = 1.812425971031189, val_loss = 0.7247074842453003\n",
      "epoch n°3045 : train_loss = 1.8149042129516602, val_loss = 0.724189281463623\n",
      "epoch n°3046 : train_loss = 1.8096716403961182, val_loss = 0.7210391759872437\n",
      "epoch n°3047 : train_loss = 1.8079091310501099, val_loss = 0.7182273864746094\n",
      "epoch n°3048 : train_loss = 1.8010059595108032, val_loss = 0.7191911935806274\n",
      "epoch n°3049 : train_loss = 1.8078266382217407, val_loss = 0.7186838388442993\n",
      "epoch n°3050 : train_loss = 1.8000456094741821, val_loss = 0.7196735143661499\n",
      "epoch n°3051 : train_loss = 1.8063857555389404, val_loss = 0.7191841006278992\n",
      "epoch n°3052 : train_loss = 1.805657982826233, val_loss = 0.725383996963501\n",
      "epoch n°3053 : train_loss = 1.8019229173660278, val_loss = 0.719336986541748\n",
      "epoch n°3054 : train_loss = 1.8165384531021118, val_loss = 0.7259423732757568\n",
      "epoch n°3055 : train_loss = 1.8095844984054565, val_loss = 0.7228325605392456\n",
      "epoch n°3056 : train_loss = 1.7999815940856934, val_loss = 0.7243505716323853\n",
      "epoch n°3057 : train_loss = 1.8006362915039062, val_loss = 0.7238792181015015\n",
      "epoch n°3058 : train_loss = 1.7993981838226318, val_loss = 0.725006103515625\n",
      "epoch n°3059 : train_loss = 1.8065147399902344, val_loss = 0.7263913154602051\n",
      "epoch n°3060 : train_loss = 1.8081231117248535, val_loss = 0.7190135717391968\n",
      "epoch n°3061 : train_loss = 1.8085309267044067, val_loss = 0.725307822227478\n",
      "epoch n°3062 : train_loss = 1.8034026622772217, val_loss = 0.7211269736289978\n",
      "epoch n°3063 : train_loss = 1.8019084930419922, val_loss = 0.7193233966827393\n",
      "epoch n°3064 : train_loss = 1.8043453693389893, val_loss = 0.7207913994789124\n",
      "epoch n°3065 : train_loss = 1.8097373247146606, val_loss = 0.7204920053482056\n",
      "epoch n°3066 : train_loss = 1.8055415153503418, val_loss = 0.7210473418235779\n",
      "epoch n°3067 : train_loss = 1.8113056421279907, val_loss = 0.7191452383995056\n",
      "epoch n°3068 : train_loss = 1.8004881143569946, val_loss = 0.7190399169921875\n",
      "epoch n°3069 : train_loss = 1.8035972118377686, val_loss = 0.7243629693984985\n",
      "epoch n°3070 : train_loss = 1.7948709726333618, val_loss = 0.7234532833099365\n",
      "epoch n°3071 : train_loss = 1.8036959171295166, val_loss = 0.7217336893081665\n",
      "epoch n°3072 : train_loss = 1.8094730377197266, val_loss = 0.7247111797332764\n",
      "epoch n°3073 : train_loss = 1.7933650016784668, val_loss = 0.723665177822113\n",
      "epoch n°3074 : train_loss = 1.7916409969329834, val_loss = 0.7174097895622253\n",
      "epoch n°3075 : train_loss = 1.801937222480774, val_loss = 0.7291357517242432\n",
      "epoch n°3076 : train_loss = 1.8041921854019165, val_loss = 0.7235327959060669\n",
      "epoch n°3077 : train_loss = 1.8081490993499756, val_loss = 0.7183755040168762\n",
      "epoch n°3078 : train_loss = 1.8140416145324707, val_loss = 0.7202877998352051\n",
      "epoch n°3079 : train_loss = 1.7943806648254395, val_loss = 0.720720648765564\n",
      "epoch n°3080 : train_loss = 1.7992467880249023, val_loss = 0.721800684928894\n",
      "epoch n°3081 : train_loss = 1.8042901754379272, val_loss = 0.7254932522773743\n",
      "epoch n°3082 : train_loss = 1.80701744556427, val_loss = 0.7203267812728882\n",
      "epoch n°3083 : train_loss = 1.8040963411331177, val_loss = 0.7185776233673096\n",
      "epoch n°3084 : train_loss = 1.794596791267395, val_loss = 0.7271614074707031\n",
      "epoch n°3085 : train_loss = 1.7935267686843872, val_loss = 0.719303548336029\n",
      "epoch n°3086 : train_loss = 1.8111516237258911, val_loss = 0.7214576005935669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3087 : train_loss = 1.801354169845581, val_loss = 0.7195668816566467\n",
      "epoch n°3088 : train_loss = 1.8081386089324951, val_loss = 0.7227082848548889\n",
      "epoch n°3089 : train_loss = 1.8010022640228271, val_loss = 0.7230503559112549\n",
      "epoch n°3090 : train_loss = 1.7996711730957031, val_loss = 0.7270355224609375\n",
      "epoch n°3091 : train_loss = 1.7956106662750244, val_loss = 0.7209672331809998\n",
      "epoch n°3092 : train_loss = 1.797057032585144, val_loss = 0.7228261828422546\n",
      "epoch n°3093 : train_loss = 1.8025630712509155, val_loss = 0.7204858064651489\n",
      "epoch n°3094 : train_loss = 1.811355710029602, val_loss = 0.7265719175338745\n",
      "epoch n°3095 : train_loss = 1.79910409450531, val_loss = 0.7253616452217102\n",
      "epoch n°3096 : train_loss = 1.804976463317871, val_loss = 0.7220340371131897\n",
      "epoch n°3097 : train_loss = 1.8030294179916382, val_loss = 0.7176709771156311\n",
      "epoch n°3098 : train_loss = 1.8030285835266113, val_loss = 0.7264186143875122\n",
      "epoch n°3099 : train_loss = 1.8098896741867065, val_loss = 0.7281374335289001\n",
      "epoch n°3100 : train_loss = 1.807391881942749, val_loss = 0.7200115323066711\n",
      "epoch n°3101 : train_loss = 1.8020001649856567, val_loss = 0.7196171879768372\n",
      "epoch n°3102 : train_loss = 1.8001821041107178, val_loss = 0.7270104289054871\n",
      "epoch n°3103 : train_loss = 1.8036149740219116, val_loss = 0.7232657670974731\n",
      "epoch n°3104 : train_loss = 1.79649019241333, val_loss = 0.7257807850837708\n",
      "epoch n°3105 : train_loss = 1.8103772401809692, val_loss = 0.7202364802360535\n",
      "epoch n°3106 : train_loss = 1.8097620010375977, val_loss = 0.7238231897354126\n",
      "epoch n°3107 : train_loss = 1.806186556816101, val_loss = 0.7237042784690857\n",
      "epoch n°3108 : train_loss = 1.8062394857406616, val_loss = 0.7245860695838928\n",
      "epoch n°3109 : train_loss = 1.8038958311080933, val_loss = 0.721895158290863\n",
      "epoch n°3110 : train_loss = 1.7971645593643188, val_loss = 0.7221078872680664\n",
      "epoch n°3111 : train_loss = 1.8006492853164673, val_loss = 0.7219393849372864\n",
      "epoch n°3112 : train_loss = 1.7955247163772583, val_loss = 0.7224736213684082\n",
      "epoch n°3113 : train_loss = 1.8060606718063354, val_loss = 0.7188265919685364\n",
      "epoch n°3114 : train_loss = 1.8045400381088257, val_loss = 0.7198008894920349\n",
      "epoch n°3115 : train_loss = 1.7939099073410034, val_loss = 0.7210862636566162\n",
      "epoch n°3116 : train_loss = 1.7883398532867432, val_loss = 0.7244155406951904\n",
      "epoch n°3117 : train_loss = 1.8086947202682495, val_loss = 0.7216367125511169\n",
      "epoch n°3118 : train_loss = 1.8013592958450317, val_loss = 0.7230427861213684\n",
      "epoch n°3119 : train_loss = 1.8030723333358765, val_loss = 0.7247515916824341\n",
      "epoch n°3120 : train_loss = 1.7966574430465698, val_loss = 0.7215577960014343\n",
      "epoch n°3121 : train_loss = 1.797847032546997, val_loss = 0.7197592854499817\n",
      "epoch n°3122 : train_loss = 1.8029357194900513, val_loss = 0.7253097891807556\n",
      "epoch n°3123 : train_loss = 1.8041918277740479, val_loss = 0.724353551864624\n",
      "epoch n°3124 : train_loss = 1.805586814880371, val_loss = 0.7237738370895386\n",
      "epoch n°3125 : train_loss = 1.8075990676879883, val_loss = 0.7208296060562134\n",
      "epoch n°3126 : train_loss = 1.807567834854126, val_loss = 0.7206326723098755\n",
      "epoch n°3127 : train_loss = 1.8067609071731567, val_loss = 0.7197818756103516\n",
      "epoch n°3128 : train_loss = 1.8062278032302856, val_loss = 0.7250279188156128\n",
      "epoch n°3129 : train_loss = 1.799533724784851, val_loss = 0.7207372784614563\n",
      "epoch n°3130 : train_loss = 1.8055022954940796, val_loss = 0.7192203998565674\n",
      "epoch n°3131 : train_loss = 1.7905253171920776, val_loss = 0.7243245244026184\n",
      "epoch n°3132 : train_loss = 1.8051602840423584, val_loss = 0.7202848792076111\n",
      "epoch n°3133 : train_loss = 1.804406762123108, val_loss = 0.7250600457191467\n",
      "epoch n°3134 : train_loss = 1.801865577697754, val_loss = 0.720309317111969\n",
      "epoch n°3135 : train_loss = 1.8016091585159302, val_loss = 0.7222162485122681\n",
      "epoch n°3136 : train_loss = 1.8019139766693115, val_loss = 0.7218430638313293\n",
      "epoch n°3137 : train_loss = 1.794494390487671, val_loss = 0.7223676443099976\n",
      "epoch n°3138 : train_loss = 1.8073538541793823, val_loss = 0.7220549583435059\n",
      "epoch n°3139 : train_loss = 1.8068792819976807, val_loss = 0.7180755138397217\n",
      "epoch n°3140 : train_loss = 1.812047004699707, val_loss = 0.7255392074584961\n",
      "epoch n°3141 : train_loss = 1.802968144416809, val_loss = 0.7209417819976807\n",
      "epoch n°3142 : train_loss = 1.7989495992660522, val_loss = 0.713213324546814\n",
      "epoch n°3143 : train_loss = 1.80783212184906, val_loss = 0.7233166694641113\n",
      "epoch n°3144 : train_loss = 1.8073068857192993, val_loss = 0.7190299034118652\n",
      "epoch n°3145 : train_loss = 1.8042258024215698, val_loss = 0.721455454826355\n",
      "epoch n°3146 : train_loss = 1.8005671501159668, val_loss = 0.7255093455314636\n",
      "epoch n°3147 : train_loss = 1.8038122653961182, val_loss = 0.7205022573471069\n",
      "epoch n°3148 : train_loss = 1.7939234972000122, val_loss = 0.7225083112716675\n",
      "epoch n°3149 : train_loss = 1.8040063381195068, val_loss = 0.7209888100624084\n",
      "epoch n°3150 : train_loss = 1.798710584640503, val_loss = 0.7205984592437744\n",
      "epoch n°3151 : train_loss = 1.7926995754241943, val_loss = 0.7231559157371521\n",
      "epoch n°3152 : train_loss = 1.803332805633545, val_loss = 0.7209088206291199\n",
      "epoch n°3153 : train_loss = 1.8038381338119507, val_loss = 0.7193022966384888\n",
      "epoch n°3154 : train_loss = 1.7873148918151855, val_loss = 0.720252513885498\n",
      "epoch n°3155 : train_loss = 1.797357201576233, val_loss = 0.7202514410018921\n",
      "epoch n°3156 : train_loss = 1.806293249130249, val_loss = 0.7226664423942566\n",
      "epoch n°3157 : train_loss = 1.802327036857605, val_loss = 0.7242392897605896\n",
      "epoch n°3158 : train_loss = 1.8059803247451782, val_loss = 0.7258707284927368\n",
      "epoch n°3159 : train_loss = 1.791973352432251, val_loss = 0.7214721441268921\n",
      "epoch n°3160 : train_loss = 1.8132814168930054, val_loss = 0.7235628366470337\n",
      "epoch n°3161 : train_loss = 1.8048057556152344, val_loss = 0.727567195892334\n",
      "epoch n°3162 : train_loss = 1.803611159324646, val_loss = 0.727256178855896\n",
      "epoch n°3163 : train_loss = 1.7983860969543457, val_loss = 0.7236794233322144\n",
      "epoch n°3164 : train_loss = 1.7898598909378052, val_loss = 0.7192907929420471\n",
      "epoch n°3165 : train_loss = 1.7994598150253296, val_loss = 0.7203954458236694\n",
      "epoch n°3166 : train_loss = 1.804296612739563, val_loss = 0.7235682606697083\n",
      "epoch n°3167 : train_loss = 1.796041488647461, val_loss = 0.7180521488189697\n",
      "epoch n°3168 : train_loss = 1.8043593168258667, val_loss = 0.7186235785484314\n",
      "epoch n°3169 : train_loss = 1.7924342155456543, val_loss = 0.7237331867218018\n",
      "epoch n°3170 : train_loss = 1.7960926294326782, val_loss = 0.7234375476837158\n",
      "epoch n°3171 : train_loss = 1.8016159534454346, val_loss = 0.7220565676689148\n",
      "epoch n°3172 : train_loss = 1.7931879758834839, val_loss = 0.7195500135421753\n",
      "epoch n°3173 : train_loss = 1.8032169342041016, val_loss = 0.7214707136154175\n",
      "epoch n°3174 : train_loss = 1.8027650117874146, val_loss = 0.7251977920532227\n",
      "epoch n°3175 : train_loss = 1.8096990585327148, val_loss = 0.7240188121795654\n",
      "epoch n°3176 : train_loss = 1.7984861135482788, val_loss = 0.72159743309021\n",
      "epoch n°3177 : train_loss = 1.7986350059509277, val_loss = 0.7198758721351624\n",
      "epoch n°3178 : train_loss = 1.8055094480514526, val_loss = 0.7175495028495789\n",
      "epoch n°3179 : train_loss = 1.8028398752212524, val_loss = 0.7232258915901184\n",
      "epoch n°3180 : train_loss = 1.802444338798523, val_loss = 0.7221534252166748\n",
      "epoch n°3181 : train_loss = 1.8041044473648071, val_loss = 0.7218055129051208\n",
      "epoch n°3182 : train_loss = 1.7888102531433105, val_loss = 0.7201691269874573\n",
      "epoch n°3183 : train_loss = 1.7970242500305176, val_loss = 0.724270224571228\n",
      "epoch n°3184 : train_loss = 1.7989301681518555, val_loss = 0.7212626338005066\n",
      "epoch n°3185 : train_loss = 1.7994358539581299, val_loss = 0.7268778681755066\n",
      "epoch n°3186 : train_loss = 1.7989360094070435, val_loss = 0.7186024188995361\n",
      "epoch n°3187 : train_loss = 1.8002400398254395, val_loss = 0.7221410274505615\n",
      "epoch n°3188 : train_loss = 1.7916995286941528, val_loss = 0.7215588092803955\n",
      "epoch n°3189 : train_loss = 1.802221655845642, val_loss = 0.7257135510444641\n",
      "epoch n°3190 : train_loss = 1.8020849227905273, val_loss = 0.7176232933998108\n",
      "epoch n°3191 : train_loss = 1.8008476495742798, val_loss = 0.7177063226699829\n",
      "epoch n°3192 : train_loss = 1.8054226636886597, val_loss = 0.7238397598266602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3193 : train_loss = 1.8014113903045654, val_loss = 0.7239328026771545\n",
      "epoch n°3194 : train_loss = 1.7989815473556519, val_loss = 0.7242806553840637\n",
      "epoch n°3195 : train_loss = 1.7958037853240967, val_loss = 0.7225419878959656\n",
      "epoch n°3196 : train_loss = 1.8030019998550415, val_loss = 0.7160003781318665\n",
      "epoch n°3197 : train_loss = 1.7967058420181274, val_loss = 0.7232346534729004\n",
      "epoch n°3198 : train_loss = 1.7935712337493896, val_loss = 0.7245429754257202\n",
      "epoch n°3199 : train_loss = 1.7965176105499268, val_loss = 0.720326840877533\n",
      "epoch n°3200 : train_loss = 1.807456612586975, val_loss = 0.7180147171020508\n",
      "epoch n°3201 : train_loss = 1.7975165843963623, val_loss = 0.7248989343643188\n",
      "epoch n°3202 : train_loss = 1.7981934547424316, val_loss = 0.7231980562210083\n",
      "epoch n°3203 : train_loss = 1.7936855554580688, val_loss = 0.7212716937065125\n",
      "epoch n°3204 : train_loss = 1.801927924156189, val_loss = 0.720037579536438\n",
      "epoch n°3205 : train_loss = 1.8012574911117554, val_loss = 0.7187446355819702\n",
      "epoch n°3206 : train_loss = 1.7944782972335815, val_loss = 0.7215303182601929\n",
      "epoch n°3207 : train_loss = 1.8010419607162476, val_loss = 0.7233986258506775\n",
      "epoch n°3208 : train_loss = 1.793839931488037, val_loss = 0.7285840511322021\n",
      "epoch n°3209 : train_loss = 1.79521644115448, val_loss = 0.7252997756004333\n",
      "epoch n°3210 : train_loss = 1.8074818849563599, val_loss = 0.7158397436141968\n",
      "epoch n°3211 : train_loss = 1.80307936668396, val_loss = 0.7215926647186279\n",
      "epoch n°3212 : train_loss = 1.7996512651443481, val_loss = 0.7218678593635559\n",
      "epoch n°3213 : train_loss = 1.7989046573638916, val_loss = 0.7248272895812988\n",
      "epoch n°3214 : train_loss = 1.7947971820831299, val_loss = 0.7233126163482666\n",
      "epoch n°3215 : train_loss = 1.79874849319458, val_loss = 0.7221969366073608\n",
      "epoch n°3216 : train_loss = 1.7921875715255737, val_loss = 0.719052791595459\n",
      "epoch n°3217 : train_loss = 1.8076670169830322, val_loss = 0.7244369387626648\n",
      "epoch n°3218 : train_loss = 1.8104909658432007, val_loss = 0.7197239398956299\n",
      "epoch n°3219 : train_loss = 1.7970359325408936, val_loss = 0.725803017616272\n",
      "epoch n°3220 : train_loss = 1.7994189262390137, val_loss = 0.7247011065483093\n",
      "epoch n°3221 : train_loss = 1.8055706024169922, val_loss = 0.7244055271148682\n",
      "epoch n°3222 : train_loss = 1.797099232673645, val_loss = 0.717600405216217\n",
      "epoch n°3223 : train_loss = 1.7903437614440918, val_loss = 0.7167007327079773\n",
      "epoch n°3224 : train_loss = 1.798711895942688, val_loss = 0.722926139831543\n",
      "epoch n°3225 : train_loss = 1.7953635454177856, val_loss = 0.7215730547904968\n",
      "epoch n°3226 : train_loss = 1.7962419986724854, val_loss = 0.7253645658493042\n",
      "epoch n°3227 : train_loss = 1.79788339138031, val_loss = 0.7208203673362732\n",
      "epoch n°3228 : train_loss = 1.7969119548797607, val_loss = 0.7193084955215454\n",
      "epoch n°3229 : train_loss = 1.7873815298080444, val_loss = 0.7230521440505981\n",
      "epoch n°3230 : train_loss = 1.7943869829177856, val_loss = 0.7225131392478943\n",
      "epoch n°3231 : train_loss = 1.7993483543395996, val_loss = 0.7258226275444031\n",
      "epoch n°3232 : train_loss = 1.7983051538467407, val_loss = 0.7246887683868408\n",
      "epoch n°3233 : train_loss = 1.7959219217300415, val_loss = 0.7209048867225647\n",
      "epoch n°3234 : train_loss = 1.7864238023757935, val_loss = 0.7189241647720337\n",
      "epoch n°3235 : train_loss = 1.797767996788025, val_loss = 0.7204470038414001\n",
      "epoch n°3236 : train_loss = 1.7883164882659912, val_loss = 0.7220331430435181\n",
      "epoch n°3237 : train_loss = 1.7947659492492676, val_loss = 0.7215343117713928\n",
      "epoch n°3238 : train_loss = 1.8002009391784668, val_loss = 0.7192850708961487\n",
      "epoch n°3239 : train_loss = 1.79562509059906, val_loss = 0.7226211428642273\n",
      "epoch n°3240 : train_loss = 1.800288200378418, val_loss = 0.7253752946853638\n",
      "epoch n°3241 : train_loss = 1.8053535223007202, val_loss = 0.7222594022750854\n",
      "epoch n°3242 : train_loss = 1.8074758052825928, val_loss = 0.7212226390838623\n",
      "epoch n°3243 : train_loss = 1.8002015352249146, val_loss = 0.7161507606506348\n",
      "epoch n°3244 : train_loss = 1.8028289079666138, val_loss = 0.7179549336433411\n",
      "epoch n°3245 : train_loss = 1.7973746061325073, val_loss = 0.7236796021461487\n",
      "epoch n°3246 : train_loss = 1.7965686321258545, val_loss = 0.7219765186309814\n",
      "epoch n°3247 : train_loss = 1.8002352714538574, val_loss = 0.7221017479896545\n",
      "epoch n°3248 : train_loss = 1.7979487180709839, val_loss = 0.7197401523590088\n",
      "epoch n°3249 : train_loss = 1.7949767112731934, val_loss = 0.7211375832557678\n",
      "epoch n°3250 : train_loss = 1.7974799871444702, val_loss = 0.720065712928772\n",
      "epoch n°3251 : train_loss = 1.7913994789123535, val_loss = 0.7205889821052551\n",
      "epoch n°3252 : train_loss = 1.7929400205612183, val_loss = 0.724668562412262\n",
      "epoch n°3253 : train_loss = 1.7905678749084473, val_loss = 0.7188978791236877\n",
      "epoch n°3254 : train_loss = 1.8042385578155518, val_loss = 0.7230303883552551\n",
      "epoch n°3255 : train_loss = 1.7887393236160278, val_loss = 0.7233800292015076\n",
      "epoch n°3256 : train_loss = 1.7844536304473877, val_loss = 0.7231987714767456\n",
      "epoch n°3257 : train_loss = 1.8017401695251465, val_loss = 0.7197666764259338\n",
      "epoch n°3258 : train_loss = 1.7997268438339233, val_loss = 0.7146185636520386\n",
      "epoch n°3259 : train_loss = 1.7858527898788452, val_loss = 0.7195644378662109\n",
      "epoch n°3260 : train_loss = 1.8018354177474976, val_loss = 0.719325602054596\n",
      "epoch n°3261 : train_loss = 1.7918576002120972, val_loss = 0.718098521232605\n",
      "epoch n°3262 : train_loss = 1.8038970232009888, val_loss = 0.7217660546302795\n",
      "epoch n°3263 : train_loss = 1.8063035011291504, val_loss = 0.7205678224563599\n",
      "epoch n°3264 : train_loss = 1.7976962327957153, val_loss = 0.7187331318855286\n",
      "epoch n°3265 : train_loss = 1.8036872148513794, val_loss = 0.7243948578834534\n",
      "epoch n°3266 : train_loss = 1.7984321117401123, val_loss = 0.7232224941253662\n",
      "epoch n°3267 : train_loss = 1.796773910522461, val_loss = 0.7188449501991272\n",
      "epoch n°3268 : train_loss = 1.792225956916809, val_loss = 0.723101794719696\n",
      "epoch n°3269 : train_loss = 1.7994592189788818, val_loss = 0.722307026386261\n",
      "epoch n°3270 : train_loss = 1.7814069986343384, val_loss = 0.7265219688415527\n",
      "epoch n°3271 : train_loss = 1.7859035730361938, val_loss = 0.7230954766273499\n",
      "epoch n°3272 : train_loss = 1.7903306484222412, val_loss = 0.7203587889671326\n",
      "epoch n°3273 : train_loss = 1.7889212369918823, val_loss = 0.7225921154022217\n",
      "epoch n°3274 : train_loss = 1.7948790788650513, val_loss = 0.7215397357940674\n",
      "epoch n°3275 : train_loss = 1.8013854026794434, val_loss = 0.7209939360618591\n",
      "epoch n°3276 : train_loss = 1.7979447841644287, val_loss = 0.7229630351066589\n",
      "epoch n°3277 : train_loss = 1.7880325317382812, val_loss = 0.7218825221061707\n",
      "epoch n°3278 : train_loss = 1.7959134578704834, val_loss = 0.7184404134750366\n",
      "epoch n°3279 : train_loss = 1.7932775020599365, val_loss = 0.7195985913276672\n",
      "epoch n°3280 : train_loss = 1.7933119535446167, val_loss = 0.7221890091896057\n",
      "epoch n°3281 : train_loss = 1.7983591556549072, val_loss = 0.7222679853439331\n",
      "epoch n°3282 : train_loss = 1.7989603281021118, val_loss = 0.7157526612281799\n",
      "epoch n°3283 : train_loss = 1.7872116565704346, val_loss = 0.7226999998092651\n",
      "epoch n°3284 : train_loss = 1.7892404794692993, val_loss = 0.7217798829078674\n",
      "epoch n°3285 : train_loss = 1.7938518524169922, val_loss = 0.7195348143577576\n",
      "epoch n°3286 : train_loss = 1.7966793775558472, val_loss = 0.7236860394477844\n",
      "epoch n°3287 : train_loss = 1.7859735488891602, val_loss = 0.7178827524185181\n",
      "epoch n°3288 : train_loss = 1.7952508926391602, val_loss = 0.7209542393684387\n",
      "epoch n°3289 : train_loss = 1.7938027381896973, val_loss = 0.7150134444236755\n",
      "epoch n°3290 : train_loss = 1.794872522354126, val_loss = 0.7213431000709534\n",
      "epoch n°3291 : train_loss = 1.7948721647262573, val_loss = 0.7245903015136719\n",
      "epoch n°3292 : train_loss = 1.7956233024597168, val_loss = 0.7174810767173767\n",
      "epoch n°3293 : train_loss = 1.8032433986663818, val_loss = 0.7226266264915466\n",
      "epoch n°3294 : train_loss = 1.7964204549789429, val_loss = 0.719444215297699\n",
      "epoch n°3295 : train_loss = 1.793504238128662, val_loss = 0.7225797176361084\n",
      "epoch n°3296 : train_loss = 1.7829787731170654, val_loss = 0.721470057964325\n",
      "epoch n°3297 : train_loss = 1.8019657135009766, val_loss = 0.7181205153465271\n",
      "epoch n°3298 : train_loss = 1.7892934083938599, val_loss = 0.721622109413147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3299 : train_loss = 1.7915513515472412, val_loss = 0.7195272445678711\n",
      "epoch n°3300 : train_loss = 1.7904229164123535, val_loss = 0.7226028442382812\n",
      "epoch n°3301 : train_loss = 1.7986035346984863, val_loss = 0.719508171081543\n",
      "epoch n°3302 : train_loss = 1.7852221727371216, val_loss = 0.7200204730033875\n",
      "epoch n°3303 : train_loss = 1.8003000020980835, val_loss = 0.720138669013977\n",
      "epoch n°3304 : train_loss = 1.7956966161727905, val_loss = 0.7180235385894775\n",
      "epoch n°3305 : train_loss = 1.809293270111084, val_loss = 0.7201287150382996\n",
      "epoch n°3306 : train_loss = 1.7890766859054565, val_loss = 0.7217978835105896\n",
      "epoch n°3307 : train_loss = 1.7890713214874268, val_loss = 0.7234228253364563\n",
      "epoch n°3308 : train_loss = 1.7883518934249878, val_loss = 0.7242204546928406\n",
      "epoch n°3309 : train_loss = 1.7922288179397583, val_loss = 0.7181066274642944\n",
      "epoch n°3310 : train_loss = 1.805763840675354, val_loss = 0.7242853045463562\n",
      "epoch n°3311 : train_loss = 1.7906782627105713, val_loss = 0.7181376218795776\n",
      "epoch n°3312 : train_loss = 1.7853202819824219, val_loss = 0.7236961722373962\n",
      "epoch n°3313 : train_loss = 1.7917225360870361, val_loss = 0.7253299355506897\n",
      "epoch n°3314 : train_loss = 1.7857660055160522, val_loss = 0.7231087684631348\n",
      "epoch n°3315 : train_loss = 1.7944285869598389, val_loss = 0.7195920348167419\n",
      "epoch n°3316 : train_loss = 1.8011207580566406, val_loss = 0.7218136191368103\n",
      "epoch n°3317 : train_loss = 1.7962688207626343, val_loss = 0.7298498153686523\n",
      "epoch n°3318 : train_loss = 1.791637659072876, val_loss = 0.7260630130767822\n",
      "epoch n°3319 : train_loss = 1.796386957168579, val_loss = 0.7241798639297485\n",
      "epoch n°3320 : train_loss = 1.7887251377105713, val_loss = 0.7186838984489441\n",
      "epoch n°3321 : train_loss = 1.7887531518936157, val_loss = 0.7242618203163147\n",
      "epoch n°3322 : train_loss = 1.7896541357040405, val_loss = 0.7206562757492065\n",
      "epoch n°3323 : train_loss = 1.7983256578445435, val_loss = 0.7201821804046631\n",
      "epoch n°3324 : train_loss = 1.8000013828277588, val_loss = 0.7161788940429688\n",
      "epoch n°3325 : train_loss = 1.7962708473205566, val_loss = 0.7233502268791199\n",
      "epoch n°3326 : train_loss = 1.7940696477890015, val_loss = 0.7181063890457153\n",
      "epoch n°3327 : train_loss = 1.7930595874786377, val_loss = 0.7205963134765625\n",
      "epoch n°3328 : train_loss = 1.7864660024642944, val_loss = 0.7169516682624817\n",
      "epoch n°3329 : train_loss = 1.7829363346099854, val_loss = 0.7177892923355103\n",
      "epoch n°3330 : train_loss = 1.792527198791504, val_loss = 0.7205671072006226\n",
      "epoch n°3331 : train_loss = 1.7942428588867188, val_loss = 0.7263261675834656\n",
      "epoch n°3332 : train_loss = 1.8013691902160645, val_loss = 0.7215310335159302\n",
      "epoch n°3333 : train_loss = 1.8013567924499512, val_loss = 0.7247706651687622\n",
      "epoch n°3334 : train_loss = 1.7870577573776245, val_loss = 0.7210375070571899\n",
      "epoch n°3335 : train_loss = 1.7980858087539673, val_loss = 0.7197533249855042\n",
      "epoch n°3336 : train_loss = 1.7938648462295532, val_loss = 0.7218643426895142\n",
      "epoch n°3337 : train_loss = 1.7867499589920044, val_loss = 0.718743622303009\n",
      "epoch n°3338 : train_loss = 1.7808011770248413, val_loss = 0.7197132110595703\n",
      "epoch n°3339 : train_loss = 1.7792372703552246, val_loss = 0.7238238453865051\n",
      "epoch n°3340 : train_loss = 1.793384075164795, val_loss = 0.7259445190429688\n",
      "epoch n°3341 : train_loss = 1.7920396327972412, val_loss = 0.728628933429718\n",
      "epoch n°3342 : train_loss = 1.7913663387298584, val_loss = 0.7279483675956726\n",
      "epoch n°3343 : train_loss = 1.7990972995758057, val_loss = 0.7235701680183411\n",
      "epoch n°3344 : train_loss = 1.7894978523254395, val_loss = 0.7213917374610901\n",
      "epoch n°3345 : train_loss = 1.7901538610458374, val_loss = 0.7211741805076599\n",
      "epoch n°3346 : train_loss = 1.7923932075500488, val_loss = 0.722288191318512\n",
      "epoch n°3347 : train_loss = 1.7872276306152344, val_loss = 0.7207682728767395\n",
      "epoch n°3348 : train_loss = 1.7872132062911987, val_loss = 0.7221983671188354\n",
      "epoch n°3349 : train_loss = 1.792919635772705, val_loss = 0.717542827129364\n",
      "epoch n°3350 : train_loss = 1.7807700634002686, val_loss = 0.7253193855285645\n",
      "epoch n°3351 : train_loss = 1.796493649482727, val_loss = 0.7201553583145142\n",
      "epoch n°3352 : train_loss = 1.7941782474517822, val_loss = 0.7228778600692749\n",
      "epoch n°3353 : train_loss = 1.7986575365066528, val_loss = 0.7196815013885498\n",
      "epoch n°3354 : train_loss = 1.788360357284546, val_loss = 0.7211132049560547\n",
      "epoch n°3355 : train_loss = 1.7958498001098633, val_loss = 0.7252750992774963\n",
      "epoch n°3356 : train_loss = 1.7801215648651123, val_loss = 0.7217963933944702\n",
      "epoch n°3357 : train_loss = 1.7833201885223389, val_loss = 0.7180066704750061\n",
      "epoch n°3358 : train_loss = 1.7960349321365356, val_loss = 0.7181794047355652\n",
      "epoch n°3359 : train_loss = 1.7993243932724, val_loss = 0.7179141640663147\n",
      "epoch n°3360 : train_loss = 1.7863420248031616, val_loss = 0.7235062718391418\n",
      "epoch n°3361 : train_loss = 1.7917568683624268, val_loss = 0.7245566248893738\n",
      "epoch n°3362 : train_loss = 1.7953895330429077, val_loss = 0.7219805717468262\n",
      "epoch n°3363 : train_loss = 1.794379472732544, val_loss = 0.7230249047279358\n",
      "epoch n°3364 : train_loss = 1.7904462814331055, val_loss = 0.7212534546852112\n",
      "epoch n°3365 : train_loss = 1.7944355010986328, val_loss = 0.7182698249816895\n",
      "epoch n°3366 : train_loss = 1.7960649728775024, val_loss = 0.721854567527771\n",
      "epoch n°3367 : train_loss = 1.7845529317855835, val_loss = 0.7177200317382812\n",
      "epoch n°3368 : train_loss = 1.788717269897461, val_loss = 0.7219843864440918\n",
      "epoch n°3369 : train_loss = 1.7924150228500366, val_loss = 0.7236765623092651\n",
      "epoch n°3370 : train_loss = 1.7864794731140137, val_loss = 0.722396731376648\n",
      "epoch n°3371 : train_loss = 1.7910785675048828, val_loss = 0.7200964689254761\n",
      "epoch n°3372 : train_loss = 1.7862991094589233, val_loss = 0.7199045419692993\n",
      "epoch n°3373 : train_loss = 1.7855193614959717, val_loss = 0.7180464267730713\n",
      "epoch n°3374 : train_loss = 1.7830662727355957, val_loss = 0.7238742113113403\n",
      "epoch n°3375 : train_loss = 1.7902047634124756, val_loss = 0.7195216417312622\n",
      "epoch n°3376 : train_loss = 1.7933908700942993, val_loss = 0.7228806614875793\n",
      "epoch n°3377 : train_loss = 1.796129584312439, val_loss = 0.7172792553901672\n",
      "epoch n°3378 : train_loss = 1.7857232093811035, val_loss = 0.7242459654808044\n",
      "epoch n°3379 : train_loss = 1.7964534759521484, val_loss = 0.7231093049049377\n",
      "epoch n°3380 : train_loss = 1.801471471786499, val_loss = 0.7249650955200195\n",
      "epoch n°3381 : train_loss = 1.8034390211105347, val_loss = 0.7173463106155396\n",
      "epoch n°3382 : train_loss = 1.7944834232330322, val_loss = 0.7193917036056519\n",
      "epoch n°3383 : train_loss = 1.7911823987960815, val_loss = 0.7216139435768127\n",
      "epoch n°3384 : train_loss = 1.7983067035675049, val_loss = 0.7226061224937439\n",
      "epoch n°3385 : train_loss = 1.7927138805389404, val_loss = 0.7170906662940979\n",
      "epoch n°3386 : train_loss = 1.78507399559021, val_loss = 0.7259806394577026\n",
      "epoch n°3387 : train_loss = 1.8036171197891235, val_loss = 0.7201271057128906\n",
      "epoch n°3388 : train_loss = 1.7937016487121582, val_loss = 0.7178149819374084\n",
      "epoch n°3389 : train_loss = 1.7910887002944946, val_loss = 0.7249935269355774\n",
      "epoch n°3390 : train_loss = 1.7901668548583984, val_loss = 0.7171379327774048\n",
      "epoch n°3391 : train_loss = 1.7950283288955688, val_loss = 0.724392294883728\n",
      "epoch n°3392 : train_loss = 1.794643521308899, val_loss = 0.7191439867019653\n",
      "epoch n°3393 : train_loss = 1.7826591730117798, val_loss = 0.71749347448349\n",
      "epoch n°3394 : train_loss = 1.7951655387878418, val_loss = 0.7222177982330322\n",
      "epoch n°3395 : train_loss = 1.7979036569595337, val_loss = 0.7215542793273926\n",
      "epoch n°3396 : train_loss = 1.7804239988327026, val_loss = 0.7186093330383301\n",
      "epoch n°3397 : train_loss = 1.78827965259552, val_loss = 0.7241566777229309\n",
      "epoch n°3398 : train_loss = 1.7898622751235962, val_loss = 0.7201904058456421\n",
      "epoch n°3399 : train_loss = 1.7907865047454834, val_loss = 0.7182972431182861\n",
      "epoch n°3400 : train_loss = 1.7802038192749023, val_loss = 0.721116840839386\n",
      "epoch n°3401 : train_loss = 1.7842495441436768, val_loss = 0.725968599319458\n",
      "epoch n°3402 : train_loss = 1.7931301593780518, val_loss = 0.7193891406059265\n",
      "epoch n°3403 : train_loss = 1.7883450984954834, val_loss = 0.7162085771560669\n",
      "epoch n°3404 : train_loss = 1.7861058712005615, val_loss = 0.7227619290351868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3405 : train_loss = 1.7862192392349243, val_loss = 0.7226116061210632\n",
      "epoch n°3406 : train_loss = 1.7832531929016113, val_loss = 0.7251570224761963\n",
      "epoch n°3407 : train_loss = 1.7852691411972046, val_loss = 0.7268893718719482\n",
      "epoch n°3408 : train_loss = 1.794066071510315, val_loss = 0.7228437662124634\n",
      "epoch n°3409 : train_loss = 1.792349100112915, val_loss = 0.7221440672874451\n",
      "epoch n°3410 : train_loss = 1.7876654863357544, val_loss = 0.7231244444847107\n",
      "epoch n°3411 : train_loss = 1.7918487787246704, val_loss = 0.7171617150306702\n",
      "epoch n°3412 : train_loss = 1.7890374660491943, val_loss = 0.7213393449783325\n",
      "epoch n°3413 : train_loss = 1.784888505935669, val_loss = 0.7213501334190369\n",
      "epoch n°3414 : train_loss = 1.7860544919967651, val_loss = 0.7188294529914856\n",
      "epoch n°3415 : train_loss = 1.786649227142334, val_loss = 0.724016547203064\n",
      "epoch n°3416 : train_loss = 1.789458155632019, val_loss = 0.7199455499649048\n",
      "epoch n°3417 : train_loss = 1.7859432697296143, val_loss = 0.7203157544136047\n",
      "epoch n°3418 : train_loss = 1.7832481861114502, val_loss = 0.7178360819816589\n",
      "epoch n°3419 : train_loss = 1.7847447395324707, val_loss = 0.7249153852462769\n",
      "epoch n°3420 : train_loss = 1.7827187776565552, val_loss = 0.7221785187721252\n",
      "epoch n°3421 : train_loss = 1.7856166362762451, val_loss = 0.7194316387176514\n",
      "epoch n°3422 : train_loss = 1.798356056213379, val_loss = 0.7212750911712646\n",
      "epoch n°3423 : train_loss = 1.7900949716567993, val_loss = 0.7212929725646973\n",
      "epoch n°3424 : train_loss = 1.792592167854309, val_loss = 0.7196600437164307\n",
      "epoch n°3425 : train_loss = 1.787772536277771, val_loss = 0.7204282879829407\n",
      "epoch n°3426 : train_loss = 1.7856630086898804, val_loss = 0.7188577651977539\n",
      "epoch n°3427 : train_loss = 1.7966253757476807, val_loss = 0.7242413759231567\n",
      "epoch n°3428 : train_loss = 1.7779299020767212, val_loss = 0.7235423922538757\n",
      "epoch n°3429 : train_loss = 1.7962734699249268, val_loss = 0.7217092514038086\n",
      "epoch n°3430 : train_loss = 1.787363052368164, val_loss = 0.7178651094436646\n",
      "epoch n°3431 : train_loss = 1.7920268774032593, val_loss = 0.7217710018157959\n",
      "epoch n°3432 : train_loss = 1.7872536182403564, val_loss = 0.7174121141433716\n",
      "epoch n°3433 : train_loss = 1.7977831363677979, val_loss = 0.7204974889755249\n",
      "epoch n°3434 : train_loss = 1.7974834442138672, val_loss = 0.7130408883094788\n",
      "epoch n°3435 : train_loss = 1.7983853816986084, val_loss = 0.7260382771492004\n",
      "epoch n°3436 : train_loss = 1.7876641750335693, val_loss = 0.7187097668647766\n",
      "epoch n°3437 : train_loss = 1.7831857204437256, val_loss = 0.7216326594352722\n",
      "epoch n°3438 : train_loss = 1.7931535243988037, val_loss = 0.7205908894538879\n",
      "epoch n°3439 : train_loss = 1.7853491306304932, val_loss = 0.7177324891090393\n",
      "epoch n°3440 : train_loss = 1.77867591381073, val_loss = 0.7173615097999573\n",
      "epoch n°3441 : train_loss = 1.7845550775527954, val_loss = 0.7254528999328613\n",
      "epoch n°3442 : train_loss = 1.7951123714447021, val_loss = 0.7193870544433594\n",
      "epoch n°3443 : train_loss = 1.7740366458892822, val_loss = 0.7202150225639343\n",
      "epoch n°3444 : train_loss = 1.783711314201355, val_loss = 0.7125199437141418\n",
      "epoch n°3445 : train_loss = 1.7836886644363403, val_loss = 0.7248878479003906\n",
      "epoch n°3446 : train_loss = 1.7855020761489868, val_loss = 0.7210792899131775\n",
      "epoch n°3447 : train_loss = 1.7854257822036743, val_loss = 0.71959388256073\n",
      "epoch n°3448 : train_loss = 1.787011981010437, val_loss = 0.7196817994117737\n",
      "epoch n°3449 : train_loss = 1.7988075017929077, val_loss = 0.7239519357681274\n",
      "epoch n°3450 : train_loss = 1.7857998609542847, val_loss = 0.7218056321144104\n",
      "epoch n°3451 : train_loss = 1.7788150310516357, val_loss = 0.7198448777198792\n",
      "epoch n°3452 : train_loss = 1.7858307361602783, val_loss = 0.7227349877357483\n",
      "epoch n°3453 : train_loss = 1.7814736366271973, val_loss = 0.7205464243888855\n",
      "epoch n°3454 : train_loss = 1.802219271659851, val_loss = 0.7168782353401184\n",
      "epoch n°3455 : train_loss = 1.791031837463379, val_loss = 0.7164815664291382\n",
      "epoch n°3456 : train_loss = 1.777341365814209, val_loss = 0.7253429293632507\n",
      "epoch n°3457 : train_loss = 1.788793921470642, val_loss = 0.722892701625824\n",
      "epoch n°3458 : train_loss = 1.7820751667022705, val_loss = 0.7208607196807861\n",
      "epoch n°3459 : train_loss = 1.7782840728759766, val_loss = 0.7232105731964111\n",
      "epoch n°3460 : train_loss = 1.7845714092254639, val_loss = 0.7202938795089722\n",
      "epoch n°3461 : train_loss = 1.787824273109436, val_loss = 0.718570351600647\n",
      "epoch n°3462 : train_loss = 1.7757586240768433, val_loss = 0.7216701507568359\n",
      "epoch n°3463 : train_loss = 1.784900426864624, val_loss = 0.7240461707115173\n",
      "epoch n°3464 : train_loss = 1.783955454826355, val_loss = 0.72413569688797\n",
      "epoch n°3465 : train_loss = 1.791499137878418, val_loss = 0.7251065373420715\n",
      "epoch n°3466 : train_loss = 1.7895300388336182, val_loss = 0.723136842250824\n",
      "epoch n°3467 : train_loss = 1.7823203802108765, val_loss = 0.7184354066848755\n",
      "epoch n°3468 : train_loss = 1.7767852544784546, val_loss = 0.7221528887748718\n",
      "epoch n°3469 : train_loss = 1.785691738128662, val_loss = 0.720735490322113\n",
      "epoch n°3470 : train_loss = 1.7748650312423706, val_loss = 0.7218731641769409\n",
      "epoch n°3471 : train_loss = 1.7765700817108154, val_loss = 0.7160397171974182\n",
      "epoch n°3472 : train_loss = 1.7822071313858032, val_loss = 0.7230774760246277\n",
      "epoch n°3473 : train_loss = 1.791789174079895, val_loss = 0.7196321487426758\n",
      "epoch n°3474 : train_loss = 1.7878293991088867, val_loss = 0.7186369299888611\n",
      "epoch n°3475 : train_loss = 1.7829402685165405, val_loss = 0.7229276299476624\n",
      "epoch n°3476 : train_loss = 1.7800768613815308, val_loss = 0.721382200717926\n",
      "epoch n°3477 : train_loss = 1.7826181650161743, val_loss = 0.7193191647529602\n",
      "epoch n°3478 : train_loss = 1.788137674331665, val_loss = 0.7223739624023438\n",
      "epoch n°3479 : train_loss = 1.7963722944259644, val_loss = 0.7249642014503479\n",
      "epoch n°3480 : train_loss = 1.7890704870224, val_loss = 0.7204568386077881\n",
      "epoch n°3481 : train_loss = 1.786177635192871, val_loss = 0.7158512473106384\n",
      "epoch n°3482 : train_loss = 1.7879490852355957, val_loss = 0.7191596031188965\n",
      "epoch n°3483 : train_loss = 1.7850275039672852, val_loss = 0.7170543670654297\n",
      "epoch n°3484 : train_loss = 1.780370831489563, val_loss = 0.7179481983184814\n",
      "epoch n°3485 : train_loss = 1.7816851139068604, val_loss = 0.722927987575531\n",
      "epoch n°3486 : train_loss = 1.789750576019287, val_loss = 0.7216861844062805\n",
      "epoch n°3487 : train_loss = 1.788482427597046, val_loss = 0.7188680768013\n",
      "epoch n°3488 : train_loss = 1.768617033958435, val_loss = 0.716596782207489\n",
      "epoch n°3489 : train_loss = 1.7958018779754639, val_loss = 0.7196444869041443\n",
      "epoch n°3490 : train_loss = 1.785149335861206, val_loss = 0.7217034101486206\n",
      "epoch n°3491 : train_loss = 1.7983050346374512, val_loss = 0.7191802859306335\n",
      "epoch n°3492 : train_loss = 1.7874680757522583, val_loss = 0.7197228074073792\n",
      "epoch n°3493 : train_loss = 1.79218327999115, val_loss = 0.7242947220802307\n",
      "epoch n°3494 : train_loss = 1.786984920501709, val_loss = 0.7211205959320068\n",
      "epoch n°3495 : train_loss = 1.7866690158843994, val_loss = 0.7156687378883362\n",
      "epoch n°3496 : train_loss = 1.7777173519134521, val_loss = 0.7149822115898132\n",
      "epoch n°3497 : train_loss = 1.786489725112915, val_loss = 0.7226800322532654\n",
      "epoch n°3498 : train_loss = 1.7890880107879639, val_loss = 0.7197403907775879\n",
      "epoch n°3499 : train_loss = 1.7835006713867188, val_loss = 0.7219265699386597\n",
      "epoch n°3500 : train_loss = 1.7834925651550293, val_loss = 0.7202469110488892\n",
      "epoch n°3501 : train_loss = 1.7804384231567383, val_loss = 0.7195109128952026\n",
      "epoch n°3502 : train_loss = 1.7818272113800049, val_loss = 0.7227697372436523\n",
      "epoch n°3503 : train_loss = 1.7900079488754272, val_loss = 0.7180898785591125\n",
      "epoch n°3504 : train_loss = 1.7782373428344727, val_loss = 0.7226440906524658\n",
      "epoch n°3505 : train_loss = 1.7917858362197876, val_loss = 0.7175498604774475\n",
      "epoch n°3506 : train_loss = 1.77815842628479, val_loss = 0.7182781100273132\n",
      "epoch n°3507 : train_loss = 1.7797919511795044, val_loss = 0.7225339412689209\n",
      "epoch n°3508 : train_loss = 1.7919872999191284, val_loss = 0.7166916131973267\n",
      "epoch n°3509 : train_loss = 1.7739479541778564, val_loss = 0.7209027409553528\n",
      "epoch n°3510 : train_loss = 1.7776148319244385, val_loss = 0.7201125621795654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3511 : train_loss = 1.7824420928955078, val_loss = 0.722106397151947\n",
      "epoch n°3512 : train_loss = 1.7784044742584229, val_loss = 0.7228511571884155\n",
      "epoch n°3513 : train_loss = 1.7767142057418823, val_loss = 0.7220773100852966\n",
      "epoch n°3514 : train_loss = 1.7879753112792969, val_loss = 0.7261308431625366\n",
      "epoch n°3515 : train_loss = 1.786193609237671, val_loss = 0.7254805564880371\n",
      "epoch n°3516 : train_loss = 1.7876412868499756, val_loss = 0.7165333032608032\n",
      "epoch n°3517 : train_loss = 1.78559148311615, val_loss = 0.7165456414222717\n",
      "epoch n°3518 : train_loss = 1.783947467803955, val_loss = 0.7252142429351807\n",
      "epoch n°3519 : train_loss = 1.7827690839767456, val_loss = 0.7243254780769348\n",
      "epoch n°3520 : train_loss = 1.7879809141159058, val_loss = 0.7188714742660522\n",
      "epoch n°3521 : train_loss = 1.7877230644226074, val_loss = 0.7194311022758484\n",
      "epoch n°3522 : train_loss = 1.7908493280410767, val_loss = 0.7239582538604736\n",
      "epoch n°3523 : train_loss = 1.7983640432357788, val_loss = 0.7170628905296326\n",
      "epoch n°3524 : train_loss = 1.7795838117599487, val_loss = 0.7233618497848511\n",
      "epoch n°3525 : train_loss = 1.7844725847244263, val_loss = 0.7170737981796265\n",
      "epoch n°3526 : train_loss = 1.7748302221298218, val_loss = 0.7155147790908813\n",
      "epoch n°3527 : train_loss = 1.7965331077575684, val_loss = 0.7221673727035522\n",
      "epoch n°3528 : train_loss = 1.778275966644287, val_loss = 0.7141855955123901\n",
      "epoch n°3529 : train_loss = 1.7756813764572144, val_loss = 0.7162224650382996\n",
      "epoch n°3530 : train_loss = 1.7831674814224243, val_loss = 0.7216545939445496\n",
      "epoch n°3531 : train_loss = 1.782096266746521, val_loss = 0.7202184796333313\n",
      "epoch n°3532 : train_loss = 1.7768299579620361, val_loss = 0.72427898645401\n",
      "epoch n°3533 : train_loss = 1.785834550857544, val_loss = 0.7205950021743774\n",
      "epoch n°3534 : train_loss = 1.7929376363754272, val_loss = 0.7228188514709473\n",
      "epoch n°3535 : train_loss = 1.7784216403961182, val_loss = 0.7187492251396179\n",
      "epoch n°3536 : train_loss = 1.7929960489273071, val_loss = 0.7202707529067993\n",
      "epoch n°3537 : train_loss = 1.7787361145019531, val_loss = 0.7215333580970764\n",
      "epoch n°3538 : train_loss = 1.7877404689788818, val_loss = 0.7193376421928406\n",
      "epoch n°3539 : train_loss = 1.787054419517517, val_loss = 0.719713568687439\n",
      "epoch n°3540 : train_loss = 1.787623643875122, val_loss = 0.7186005115509033\n",
      "epoch n°3541 : train_loss = 1.79005765914917, val_loss = 0.7193629145622253\n",
      "epoch n°3542 : train_loss = 1.7918306589126587, val_loss = 0.7215204238891602\n",
      "epoch n°3543 : train_loss = 1.7809545993804932, val_loss = 0.7266073822975159\n",
      "epoch n°3544 : train_loss = 1.7888712882995605, val_loss = 0.7229580879211426\n",
      "epoch n°3545 : train_loss = 1.7765769958496094, val_loss = 0.721362292766571\n",
      "epoch n°3546 : train_loss = 1.7771189212799072, val_loss = 0.7194671034812927\n",
      "epoch n°3547 : train_loss = 1.7894915342330933, val_loss = 0.7184844613075256\n",
      "epoch n°3548 : train_loss = 1.7814090251922607, val_loss = 0.7176519632339478\n",
      "epoch n°3549 : train_loss = 1.78896164894104, val_loss = 0.7170244455337524\n",
      "epoch n°3550 : train_loss = 1.77890145778656, val_loss = 0.7199722528457642\n",
      "epoch n°3551 : train_loss = 1.7780380249023438, val_loss = 0.7217898964881897\n",
      "epoch n°3552 : train_loss = 1.7764803171157837, val_loss = 0.7196002006530762\n",
      "epoch n°3553 : train_loss = 1.7926076650619507, val_loss = 0.7237127423286438\n",
      "epoch n°3554 : train_loss = 1.7740397453308105, val_loss = 0.719942033290863\n",
      "epoch n°3555 : train_loss = 1.782240390777588, val_loss = 0.7202736139297485\n",
      "epoch n°3556 : train_loss = 1.7882909774780273, val_loss = 0.7244565486907959\n",
      "epoch n°3557 : train_loss = 1.7853591442108154, val_loss = 0.7221506237983704\n",
      "epoch n°3558 : train_loss = 1.7733912467956543, val_loss = 0.7201694250106812\n",
      "epoch n°3559 : train_loss = 1.782454490661621, val_loss = 0.7213162183761597\n",
      "epoch n°3560 : train_loss = 1.7772272825241089, val_loss = 0.7199073433876038\n",
      "epoch n°3561 : train_loss = 1.7806634902954102, val_loss = 0.7241957187652588\n",
      "epoch n°3562 : train_loss = 1.7849743366241455, val_loss = 0.7205452919006348\n",
      "epoch n°3563 : train_loss = 1.7750300168991089, val_loss = 0.715815544128418\n",
      "epoch n°3564 : train_loss = 1.7853676080703735, val_loss = 0.7168218493461609\n",
      "epoch n°3565 : train_loss = 1.7866451740264893, val_loss = 0.7199170589447021\n",
      "epoch n°3566 : train_loss = 1.788125991821289, val_loss = 0.7201773524284363\n",
      "epoch n°3567 : train_loss = 1.773503303527832, val_loss = 0.719597578048706\n",
      "epoch n°3568 : train_loss = 1.7798388004302979, val_loss = 0.7203711271286011\n",
      "epoch n°3569 : train_loss = 1.783752679824829, val_loss = 0.7231534123420715\n",
      "epoch n°3570 : train_loss = 1.7753511667251587, val_loss = 0.7207884788513184\n",
      "epoch n°3571 : train_loss = 1.7833822965621948, val_loss = 0.721979558467865\n",
      "epoch n°3572 : train_loss = 1.7800848484039307, val_loss = 0.7175277471542358\n",
      "epoch n°3573 : train_loss = 1.7800575494766235, val_loss = 0.7205267548561096\n",
      "epoch n°3574 : train_loss = 1.7881745100021362, val_loss = 0.7234359383583069\n",
      "epoch n°3575 : train_loss = 1.7770341634750366, val_loss = 0.7247805595397949\n",
      "epoch n°3576 : train_loss = 1.7859561443328857, val_loss = 0.7172486782073975\n",
      "epoch n°3577 : train_loss = 1.7803317308425903, val_loss = 0.7194033861160278\n",
      "epoch n°3578 : train_loss = 1.7715851068496704, val_loss = 0.7177255749702454\n",
      "epoch n°3579 : train_loss = 1.7926485538482666, val_loss = 0.7189962267875671\n",
      "epoch n°3580 : train_loss = 1.786454200744629, val_loss = 0.7228987812995911\n",
      "epoch n°3581 : train_loss = 1.7851625680923462, val_loss = 0.7225478291511536\n",
      "epoch n°3582 : train_loss = 1.7756521701812744, val_loss = 0.7214733362197876\n",
      "epoch n°3583 : train_loss = 1.7783734798431396, val_loss = 0.7194707989692688\n",
      "epoch n°3584 : train_loss = 1.7826881408691406, val_loss = 0.7223454117774963\n",
      "epoch n°3585 : train_loss = 1.7941620349884033, val_loss = 0.7208561301231384\n",
      "epoch n°3586 : train_loss = 1.7732501029968262, val_loss = 0.7189249396324158\n",
      "epoch n°3587 : train_loss = 1.77768874168396, val_loss = 0.7194170951843262\n",
      "epoch n°3588 : train_loss = 1.7808337211608887, val_loss = 0.7202654480934143\n",
      "epoch n°3589 : train_loss = 1.7791765928268433, val_loss = 0.7214033007621765\n",
      "epoch n°3590 : train_loss = 1.7719439268112183, val_loss = 0.7184301018714905\n",
      "epoch n°3591 : train_loss = 1.7779754400253296, val_loss = 0.7169077396392822\n",
      "epoch n°3592 : train_loss = 1.7816493511199951, val_loss = 0.7181338667869568\n",
      "epoch n°3593 : train_loss = 1.7773780822753906, val_loss = 0.7173897624015808\n",
      "epoch n°3594 : train_loss = 1.7803841829299927, val_loss = 0.7232598066329956\n",
      "epoch n°3595 : train_loss = 1.789306402206421, val_loss = 0.7190901637077332\n",
      "epoch n°3596 : train_loss = 1.7861278057098389, val_loss = 0.719403088092804\n",
      "epoch n°3597 : train_loss = 1.7892799377441406, val_loss = 0.7196272611618042\n",
      "epoch n°3598 : train_loss = 1.7776387929916382, val_loss = 0.7145624756813049\n",
      "epoch n°3599 : train_loss = 1.7854347229003906, val_loss = 0.7210228443145752\n",
      "epoch n°3600 : train_loss = 1.7800648212432861, val_loss = 0.720322847366333\n",
      "epoch n°3601 : train_loss = 1.7743375301361084, val_loss = 0.72126305103302\n",
      "epoch n°3602 : train_loss = 1.773736596107483, val_loss = 0.7175996899604797\n",
      "epoch n°3603 : train_loss = 1.7894197702407837, val_loss = 0.7238640189170837\n",
      "epoch n°3604 : train_loss = 1.7934306859970093, val_loss = 0.7151312232017517\n",
      "epoch n°3605 : train_loss = 1.7843130826950073, val_loss = 0.7230746746063232\n",
      "epoch n°3606 : train_loss = 1.7778714895248413, val_loss = 0.7175585627555847\n",
      "epoch n°3607 : train_loss = 1.7741106748580933, val_loss = 0.7169516682624817\n",
      "epoch n°3608 : train_loss = 1.7836424112319946, val_loss = 0.7196466326713562\n",
      "epoch n°3609 : train_loss = 1.777062177658081, val_loss = 0.7190337777137756\n",
      "epoch n°3610 : train_loss = 1.7740198373794556, val_loss = 0.7244720458984375\n",
      "epoch n°3611 : train_loss = 1.7715368270874023, val_loss = 0.7174917459487915\n",
      "epoch n°3612 : train_loss = 1.7846155166625977, val_loss = 0.7171310186386108\n",
      "epoch n°3613 : train_loss = 1.780731201171875, val_loss = 0.7197995185852051\n",
      "epoch n°3614 : train_loss = 1.7805126905441284, val_loss = 0.7223153710365295\n",
      "epoch n°3615 : train_loss = 1.7928197383880615, val_loss = 0.7260150909423828\n",
      "epoch n°3616 : train_loss = 1.7836766242980957, val_loss = 0.7202585935592651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3617 : train_loss = 1.7788323163986206, val_loss = 0.712730884552002\n",
      "epoch n°3618 : train_loss = 1.7809369564056396, val_loss = 0.7199503779411316\n",
      "epoch n°3619 : train_loss = 1.779502511024475, val_loss = 0.7185462117195129\n",
      "epoch n°3620 : train_loss = 1.7779057025909424, val_loss = 0.720691978931427\n",
      "epoch n°3621 : train_loss = 1.7864105701446533, val_loss = 0.7221794128417969\n",
      "epoch n°3622 : train_loss = 1.783593773841858, val_loss = 0.7161656618118286\n",
      "epoch n°3623 : train_loss = 1.7800238132476807, val_loss = 0.7200408577919006\n",
      "epoch n°3624 : train_loss = 1.7725193500518799, val_loss = 0.7187554240226746\n",
      "epoch n°3625 : train_loss = 1.7825335264205933, val_loss = 0.7187135219573975\n",
      "epoch n°3626 : train_loss = 1.7950549125671387, val_loss = 0.7167150974273682\n",
      "epoch n°3627 : train_loss = 1.7830883264541626, val_loss = 0.7248649597167969\n",
      "epoch n°3628 : train_loss = 1.7757351398468018, val_loss = 0.7200127243995667\n",
      "epoch n°3629 : train_loss = 1.7789344787597656, val_loss = 0.7177408337593079\n",
      "epoch n°3630 : train_loss = 1.7778087854385376, val_loss = 0.7258003950119019\n",
      "epoch n°3631 : train_loss = 1.7781084775924683, val_loss = 0.7247658371925354\n",
      "epoch n°3632 : train_loss = 1.7754249572753906, val_loss = 0.719350278377533\n",
      "epoch n°3633 : train_loss = 1.78643798828125, val_loss = 0.7222084403038025\n",
      "epoch n°3634 : train_loss = 1.7972623109817505, val_loss = 0.7185316681861877\n",
      "epoch n°3635 : train_loss = 1.7811099290847778, val_loss = 0.7198845744132996\n",
      "epoch n°3636 : train_loss = 1.7913867235183716, val_loss = 0.7171924114227295\n",
      "epoch n°3637 : train_loss = 1.7805113792419434, val_loss = 0.7239797711372375\n",
      "epoch n°3638 : train_loss = 1.7820732593536377, val_loss = 0.7189661860466003\n",
      "epoch n°3639 : train_loss = 1.7785353660583496, val_loss = 0.7237446904182434\n",
      "epoch n°3640 : train_loss = 1.7874795198440552, val_loss = 0.720675528049469\n",
      "epoch n°3641 : train_loss = 1.781270980834961, val_loss = 0.7189725637435913\n",
      "epoch n°3642 : train_loss = 1.7724337577819824, val_loss = 0.7185404896736145\n",
      "epoch n°3643 : train_loss = 1.7812250852584839, val_loss = 0.7210871577262878\n",
      "epoch n°3644 : train_loss = 1.7848328351974487, val_loss = 0.7198312282562256\n",
      "epoch n°3645 : train_loss = 1.791397213935852, val_loss = 0.7202109694480896\n",
      "epoch n°3646 : train_loss = 1.7675795555114746, val_loss = 0.7199504971504211\n",
      "epoch n°3647 : train_loss = 1.7856098413467407, val_loss = 0.7227320075035095\n",
      "epoch n°3648 : train_loss = 1.7737202644348145, val_loss = 0.7226713299751282\n",
      "epoch n°3649 : train_loss = 1.7956494092941284, val_loss = 0.721820592880249\n",
      "epoch n°3650 : train_loss = 1.7830712795257568, val_loss = 0.7145638465881348\n",
      "epoch n°3651 : train_loss = 1.778874397277832, val_loss = 0.7184017300605774\n",
      "epoch n°3652 : train_loss = 1.7812151908874512, val_loss = 0.721611499786377\n",
      "epoch n°3653 : train_loss = 1.7755687236785889, val_loss = 0.7179768085479736\n",
      "epoch n°3654 : train_loss = 1.7817717790603638, val_loss = 0.7207177877426147\n",
      "epoch n°3655 : train_loss = 1.783569097518921, val_loss = 0.7171095013618469\n",
      "epoch n°3656 : train_loss = 1.7857555150985718, val_loss = 0.717242419719696\n",
      "epoch n°3657 : train_loss = 1.7940659523010254, val_loss = 0.7209041714668274\n",
      "epoch n°3658 : train_loss = 1.781600832939148, val_loss = 0.7183521389961243\n",
      "epoch n°3659 : train_loss = 1.7745661735534668, val_loss = 0.7236323356628418\n",
      "epoch n°3660 : train_loss = 1.7811777591705322, val_loss = 0.7208190560340881\n",
      "epoch n°3661 : train_loss = 1.7767356634140015, val_loss = 0.7189348936080933\n",
      "epoch n°3662 : train_loss = 1.7815626859664917, val_loss = 0.7209315896034241\n",
      "epoch n°3663 : train_loss = 1.7848386764526367, val_loss = 0.7184789776802063\n",
      "epoch n°3664 : train_loss = 1.7858482599258423, val_loss = 0.7212736010551453\n",
      "epoch n°3665 : train_loss = 1.7916457653045654, val_loss = 0.720029890537262\n",
      "epoch n°3666 : train_loss = 1.786787509918213, val_loss = 0.7186692357063293\n",
      "epoch n°3667 : train_loss = 1.7758641242980957, val_loss = 0.7184728384017944\n",
      "epoch n°3668 : train_loss = 1.771401047706604, val_loss = 0.7190770506858826\n",
      "epoch n°3669 : train_loss = 1.7771034240722656, val_loss = 0.7181798219680786\n",
      "epoch n°3670 : train_loss = 1.7809416055679321, val_loss = 0.7193577289581299\n",
      "epoch n°3671 : train_loss = 1.779833197593689, val_loss = 0.7190030813217163\n",
      "epoch n°3672 : train_loss = 1.7806774377822876, val_loss = 0.719016432762146\n",
      "epoch n°3673 : train_loss = 1.7800639867782593, val_loss = 0.7207074761390686\n",
      "epoch n°3674 : train_loss = 1.7890263795852661, val_loss = 0.7219875454902649\n",
      "epoch n°3675 : train_loss = 1.7846163511276245, val_loss = 0.7183425426483154\n",
      "epoch n°3676 : train_loss = 1.781592607498169, val_loss = 0.721693754196167\n",
      "epoch n°3677 : train_loss = 1.7743220329284668, val_loss = 0.7145878672599792\n",
      "epoch n°3678 : train_loss = 1.79177725315094, val_loss = 0.7211551666259766\n",
      "epoch n°3679 : train_loss = 1.7708966732025146, val_loss = 0.7214269042015076\n",
      "epoch n°3680 : train_loss = 1.7787673473358154, val_loss = 0.7196595668792725\n",
      "epoch n°3681 : train_loss = 1.7831064462661743, val_loss = 0.7169921398162842\n",
      "epoch n°3682 : train_loss = 1.7825485467910767, val_loss = 0.7186173796653748\n",
      "epoch n°3683 : train_loss = 1.7727142572402954, val_loss = 0.718084454536438\n",
      "epoch n°3684 : train_loss = 1.777358055114746, val_loss = 0.7201646566390991\n",
      "epoch n°3685 : train_loss = 1.7727017402648926, val_loss = 0.7189287543296814\n",
      "epoch n°3686 : train_loss = 1.7712862491607666, val_loss = 0.7200824022293091\n",
      "epoch n°3687 : train_loss = 1.7826097011566162, val_loss = 0.7189479470252991\n",
      "epoch n°3688 : train_loss = 1.7819182872772217, val_loss = 0.7233723402023315\n",
      "epoch n°3689 : train_loss = 1.774727463722229, val_loss = 0.7176668643951416\n",
      "epoch n°3690 : train_loss = 1.7827460765838623, val_loss = 0.7218500375747681\n",
      "epoch n°3691 : train_loss = 1.7843847274780273, val_loss = 0.7229883074760437\n",
      "epoch n°3692 : train_loss = 1.7806371450424194, val_loss = 0.7194975018501282\n",
      "epoch n°3693 : train_loss = 1.783512830734253, val_loss = 0.716064453125\n",
      "epoch n°3694 : train_loss = 1.779832363128662, val_loss = 0.7200084328651428\n",
      "epoch n°3695 : train_loss = 1.77707839012146, val_loss = 0.7262080907821655\n",
      "epoch n°3696 : train_loss = 1.7824875116348267, val_loss = 0.7224246859550476\n",
      "epoch n°3697 : train_loss = 1.7826751470565796, val_loss = 0.7191166877746582\n",
      "epoch n°3698 : train_loss = 1.7799931764602661, val_loss = 0.7227534055709839\n",
      "epoch n°3699 : train_loss = 1.776631474494934, val_loss = 0.7150872945785522\n",
      "epoch n°3700 : train_loss = 1.78085196018219, val_loss = 0.7228584289550781\n",
      "epoch n°3701 : train_loss = 1.7841441631317139, val_loss = 0.7192365527153015\n",
      "epoch n°3702 : train_loss = 1.796438217163086, val_loss = 0.7169961929321289\n",
      "epoch n°3703 : train_loss = 1.77398681640625, val_loss = 0.7157317399978638\n",
      "epoch n°3704 : train_loss = 1.7672977447509766, val_loss = 0.724571168422699\n",
      "epoch n°3705 : train_loss = 1.78605055809021, val_loss = 0.7212466597557068\n",
      "epoch n°3706 : train_loss = 1.7847262620925903, val_loss = 0.7219340205192566\n",
      "epoch n°3707 : train_loss = 1.7820740938186646, val_loss = 0.718903660774231\n",
      "epoch n°3708 : train_loss = 1.7809771299362183, val_loss = 0.720602810382843\n",
      "epoch n°3709 : train_loss = 1.7852189540863037, val_loss = 0.7214174270629883\n",
      "epoch n°3710 : train_loss = 1.7734308242797852, val_loss = 0.7197494506835938\n",
      "epoch n°3711 : train_loss = 1.7822531461715698, val_loss = 0.7196143865585327\n",
      "epoch n°3712 : train_loss = 1.7834608554840088, val_loss = 0.7183659076690674\n",
      "epoch n°3713 : train_loss = 1.7738721370697021, val_loss = 0.7230636477470398\n",
      "epoch n°3714 : train_loss = 1.7843728065490723, val_loss = 0.719828188419342\n",
      "epoch n°3715 : train_loss = 1.7778218984603882, val_loss = 0.7213285565376282\n",
      "epoch n°3716 : train_loss = 1.7794065475463867, val_loss = 0.7199541926383972\n",
      "epoch n°3717 : train_loss = 1.7854933738708496, val_loss = 0.718794047832489\n",
      "epoch n°3718 : train_loss = 1.7787150144577026, val_loss = 0.7201376557350159\n",
      "epoch n°3719 : train_loss = 1.7689085006713867, val_loss = 0.7221062183380127\n",
      "epoch n°3720 : train_loss = 1.7883212566375732, val_loss = 0.7210431098937988\n",
      "epoch n°3721 : train_loss = 1.7712887525558472, val_loss = 0.7184436917304993\n",
      "epoch n°3722 : train_loss = 1.7743217945098877, val_loss = 0.717991054058075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3723 : train_loss = 1.7685996294021606, val_loss = 0.7178882956504822\n",
      "epoch n°3724 : train_loss = 1.7832725048065186, val_loss = 0.7185444831848145\n",
      "epoch n°3725 : train_loss = 1.7794848680496216, val_loss = 0.7178606390953064\n",
      "epoch n°3726 : train_loss = 1.7790865898132324, val_loss = 0.7192553281784058\n",
      "epoch n°3727 : train_loss = 1.7831488847732544, val_loss = 0.7192052602767944\n",
      "epoch n°3728 : train_loss = 1.7780660390853882, val_loss = 0.7221947312355042\n",
      "epoch n°3729 : train_loss = 1.7773386240005493, val_loss = 0.7210221886634827\n",
      "epoch n°3730 : train_loss = 1.7859102487564087, val_loss = 0.7149208784103394\n",
      "epoch n°3731 : train_loss = 1.7768012285232544, val_loss = 0.7189479470252991\n",
      "epoch n°3732 : train_loss = 1.7739263772964478, val_loss = 0.7202792763710022\n",
      "epoch n°3733 : train_loss = 1.783259391784668, val_loss = 0.7177982330322266\n",
      "epoch n°3734 : train_loss = 1.775024652481079, val_loss = 0.7185729146003723\n",
      "epoch n°3735 : train_loss = 1.7850826978683472, val_loss = 0.7180917859077454\n",
      "epoch n°3736 : train_loss = 1.785212755203247, val_loss = 0.7235894799232483\n",
      "epoch n°3737 : train_loss = 1.7854665517807007, val_loss = 0.7233938574790955\n",
      "epoch n°3738 : train_loss = 1.7754631042480469, val_loss = 0.7222557067871094\n",
      "epoch n°3739 : train_loss = 1.779297947883606, val_loss = 0.7172675132751465\n",
      "epoch n°3740 : train_loss = 1.782531499862671, val_loss = 0.7217679619789124\n",
      "epoch n°3741 : train_loss = 1.7746695280075073, val_loss = 0.7196391820907593\n",
      "epoch n°3742 : train_loss = 1.778991460800171, val_loss = 0.7176254987716675\n",
      "epoch n°3743 : train_loss = 1.7770363092422485, val_loss = 0.7120969295501709\n",
      "epoch n°3744 : train_loss = 1.774963140487671, val_loss = 0.7163082361221313\n",
      "epoch n°3745 : train_loss = 1.7763622999191284, val_loss = 0.7196637988090515\n",
      "epoch n°3746 : train_loss = 1.784742832183838, val_loss = 0.7192566394805908\n",
      "epoch n°3747 : train_loss = 1.7694404125213623, val_loss = 0.7201638221740723\n",
      "epoch n°3748 : train_loss = 1.7952897548675537, val_loss = 0.7180124521255493\n",
      "epoch n°3749 : train_loss = 1.7707492113113403, val_loss = 0.7159332036972046\n",
      "epoch n°3750 : train_loss = 1.7787926197052002, val_loss = 0.7183475494384766\n",
      "epoch n°3751 : train_loss = 1.7779546976089478, val_loss = 0.7201001644134521\n",
      "epoch n°3752 : train_loss = 1.7729262113571167, val_loss = 0.7230338454246521\n",
      "epoch n°3753 : train_loss = 1.7735425233840942, val_loss = 0.7184746265411377\n",
      "epoch n°3754 : train_loss = 1.77524733543396, val_loss = 0.7210497856140137\n",
      "epoch n°3755 : train_loss = 1.777206301689148, val_loss = 0.72235107421875\n",
      "epoch n°3756 : train_loss = 1.7789275646209717, val_loss = 0.7189046144485474\n",
      "epoch n°3757 : train_loss = 1.7883135080337524, val_loss = 0.719914436340332\n",
      "epoch n°3758 : train_loss = 1.7847718000411987, val_loss = 0.7185963988304138\n",
      "epoch n°3759 : train_loss = 1.7634949684143066, val_loss = 0.7180981040000916\n",
      "epoch n°3760 : train_loss = 1.7752209901809692, val_loss = 0.7246900796890259\n",
      "epoch n°3761 : train_loss = 1.778885841369629, val_loss = 0.7216665744781494\n",
      "epoch n°3762 : train_loss = 1.7798560857772827, val_loss = 0.7157504558563232\n",
      "epoch n°3763 : train_loss = 1.777876853942871, val_loss = 0.7211812138557434\n",
      "epoch n°3764 : train_loss = 1.7827330827713013, val_loss = 0.7182838916778564\n",
      "epoch n°3765 : train_loss = 1.7802053689956665, val_loss = 0.7182397246360779\n",
      "epoch n°3766 : train_loss = 1.78115975856781, val_loss = 0.7198572158813477\n",
      "epoch n°3767 : train_loss = 1.7800670862197876, val_loss = 0.7227510809898376\n",
      "epoch n°3768 : train_loss = 1.7764942646026611, val_loss = 0.7166378498077393\n",
      "epoch n°3769 : train_loss = 1.7803356647491455, val_loss = 0.7175372838973999\n",
      "epoch n°3770 : train_loss = 1.7760288715362549, val_loss = 0.7207958102226257\n",
      "epoch n°3771 : train_loss = 1.7837469577789307, val_loss = 0.7261303067207336\n",
      "epoch n°3772 : train_loss = 1.7828134298324585, val_loss = 0.7183120250701904\n",
      "epoch n°3773 : train_loss = 1.7802602052688599, val_loss = 0.7197750806808472\n",
      "epoch n°3774 : train_loss = 1.7806061506271362, val_loss = 0.7173364162445068\n",
      "epoch n°3775 : train_loss = 1.7818074226379395, val_loss = 0.7230863571166992\n",
      "epoch n°3776 : train_loss = 1.765974521636963, val_loss = 0.721194863319397\n",
      "epoch n°3777 : train_loss = 1.7798733711242676, val_loss = 0.7167671918869019\n",
      "epoch n°3778 : train_loss = 1.7823574542999268, val_loss = 0.7220572829246521\n",
      "epoch n°3779 : train_loss = 1.7808961868286133, val_loss = 0.7193003296852112\n",
      "epoch n°3780 : train_loss = 1.776706576347351, val_loss = 0.7207158803939819\n",
      "epoch n°3781 : train_loss = 1.7804096937179565, val_loss = 0.7202119827270508\n",
      "epoch n°3782 : train_loss = 1.7815226316452026, val_loss = 0.7194477319717407\n",
      "epoch n°3783 : train_loss = 1.7770962715148926, val_loss = 0.7191805839538574\n",
      "epoch n°3784 : train_loss = 1.7780121564865112, val_loss = 0.7160540223121643\n",
      "epoch n°3785 : train_loss = 1.7874672412872314, val_loss = 0.7207128405570984\n",
      "epoch n°3786 : train_loss = 1.7708574533462524, val_loss = 0.719924807548523\n",
      "epoch n°3787 : train_loss = 1.7686320543289185, val_loss = 0.7264394760131836\n",
      "epoch n°3788 : train_loss = 1.7827178239822388, val_loss = 0.721178412437439\n",
      "epoch n°3789 : train_loss = 1.7775691747665405, val_loss = 0.7168411612510681\n",
      "epoch n°3790 : train_loss = 1.787695050239563, val_loss = 0.7183020114898682\n",
      "epoch n°3791 : train_loss = 1.7778546810150146, val_loss = 0.7262424826622009\n",
      "epoch n°3792 : train_loss = 1.7757480144500732, val_loss = 0.7205421328544617\n",
      "epoch n°3793 : train_loss = 1.7811639308929443, val_loss = 0.7176063656806946\n",
      "epoch n°3794 : train_loss = 1.7736341953277588, val_loss = 0.7169144749641418\n",
      "epoch n°3795 : train_loss = 1.7775529623031616, val_loss = 0.7226470708847046\n",
      "epoch n°3796 : train_loss = 1.778353214263916, val_loss = 0.7237560749053955\n",
      "epoch n°3797 : train_loss = 1.7801951169967651, val_loss = 0.7218228578567505\n",
      "epoch n°3798 : train_loss = 1.7741492986679077, val_loss = 0.7205187678337097\n",
      "epoch n°3799 : train_loss = 1.776307463645935, val_loss = 0.7153133749961853\n",
      "epoch n°3800 : train_loss = 1.7856446504592896, val_loss = 0.7233499884605408\n",
      "epoch n°3801 : train_loss = 1.7829158306121826, val_loss = 0.7227172255516052\n",
      "epoch n°3802 : train_loss = 1.7775487899780273, val_loss = 0.7198060154914856\n",
      "epoch n°3803 : train_loss = 1.7740695476531982, val_loss = 0.7175788283348083\n",
      "epoch n°3804 : train_loss = 1.7662221193313599, val_loss = 0.715783953666687\n",
      "epoch n°3805 : train_loss = 1.7799279689788818, val_loss = 0.7187561392784119\n",
      "epoch n°3806 : train_loss = 1.77907133102417, val_loss = 0.7222886681556702\n",
      "epoch n°3807 : train_loss = 1.7837316989898682, val_loss = 0.7148985266685486\n",
      "epoch n°3808 : train_loss = 1.7761006355285645, val_loss = 0.7210485935211182\n",
      "epoch n°3809 : train_loss = 1.7774323225021362, val_loss = 0.7229019403457642\n",
      "epoch n°3810 : train_loss = 1.7797291278839111, val_loss = 0.7178803086280823\n",
      "epoch n°3811 : train_loss = 1.7876248359680176, val_loss = 0.7186790704727173\n",
      "epoch n°3812 : train_loss = 1.7745471000671387, val_loss = 0.7217435836791992\n",
      "epoch n°3813 : train_loss = 1.7792088985443115, val_loss = 0.7155845165252686\n",
      "epoch n°3814 : train_loss = 1.79302978515625, val_loss = 0.7194145917892456\n",
      "epoch n°3815 : train_loss = 1.7796989679336548, val_loss = 0.7241554260253906\n",
      "epoch n°3816 : train_loss = 1.7763069868087769, val_loss = 0.7148559093475342\n",
      "epoch n°3817 : train_loss = 1.7696665525436401, val_loss = 0.7196252346038818\n",
      "epoch n°3818 : train_loss = 1.7824214696884155, val_loss = 0.715605616569519\n",
      "epoch n°3819 : train_loss = 1.7753928899765015, val_loss = 0.7203353643417358\n",
      "epoch n°3820 : train_loss = 1.7745424509048462, val_loss = 0.722537100315094\n",
      "epoch n°3821 : train_loss = 1.7804309129714966, val_loss = 0.7205941677093506\n",
      "epoch n°3822 : train_loss = 1.7797178030014038, val_loss = 0.7175794839859009\n",
      "epoch n°3823 : train_loss = 1.7693383693695068, val_loss = 0.7178507447242737\n",
      "epoch n°3824 : train_loss = 1.7746129035949707, val_loss = 0.717842698097229\n",
      "epoch n°3825 : train_loss = 1.7784836292266846, val_loss = 0.7189092636108398\n",
      "epoch n°3826 : train_loss = 1.7704936265945435, val_loss = 0.7175641655921936\n",
      "epoch n°3827 : train_loss = 1.7760183811187744, val_loss = 0.7197277545928955\n",
      "epoch n°3828 : train_loss = 1.7815839052200317, val_loss = 0.7166480422019958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3829 : train_loss = 1.7681294679641724, val_loss = 0.718950092792511\n",
      "epoch n°3830 : train_loss = 1.776271939277649, val_loss = 0.721149206161499\n",
      "epoch n°3831 : train_loss = 1.777755618095398, val_loss = 0.7166528105735779\n",
      "epoch n°3832 : train_loss = 1.7751370668411255, val_loss = 0.7152929902076721\n",
      "epoch n°3833 : train_loss = 1.784733533859253, val_loss = 0.722535252571106\n",
      "epoch n°3834 : train_loss = 1.7822535037994385, val_loss = 0.7242899537086487\n",
      "epoch n°3835 : train_loss = 1.7855933904647827, val_loss = 0.7172117829322815\n",
      "epoch n°3836 : train_loss = 1.7765181064605713, val_loss = 0.7190499305725098\n",
      "epoch n°3837 : train_loss = 1.775913119316101, val_loss = 0.7143629789352417\n",
      "epoch n°3838 : train_loss = 1.7731205224990845, val_loss = 0.7213435769081116\n",
      "epoch n°3839 : train_loss = 1.772078514099121, val_loss = 0.7230932712554932\n",
      "epoch n°3840 : train_loss = 1.762864589691162, val_loss = 0.7167994379997253\n",
      "epoch n°3841 : train_loss = 1.784045696258545, val_loss = 0.7219873070716858\n",
      "epoch n°3842 : train_loss = 1.7731847763061523, val_loss = 0.714470624923706\n",
      "epoch n°3843 : train_loss = 1.7743003368377686, val_loss = 0.7170696258544922\n",
      "epoch n°3844 : train_loss = 1.7617586851119995, val_loss = 0.7212733030319214\n",
      "epoch n°3845 : train_loss = 1.7709102630615234, val_loss = 0.7158070802688599\n",
      "epoch n°3846 : train_loss = 1.778877854347229, val_loss = 0.7202039957046509\n",
      "epoch n°3847 : train_loss = 1.7780214548110962, val_loss = 0.7245163917541504\n",
      "epoch n°3848 : train_loss = 1.7850979566574097, val_loss = 0.7215204834938049\n",
      "epoch n°3849 : train_loss = 1.7788914442062378, val_loss = 0.7187710404396057\n",
      "epoch n°3850 : train_loss = 1.782525897026062, val_loss = 0.7203273177146912\n",
      "epoch n°3851 : train_loss = 1.7720121145248413, val_loss = 0.7148797512054443\n",
      "epoch n°3852 : train_loss = 1.7802958488464355, val_loss = 0.7187072038650513\n",
      "epoch n°3853 : train_loss = 1.7737351655960083, val_loss = 0.7209716439247131\n",
      "epoch n°3854 : train_loss = 1.7813023328781128, val_loss = 0.7176427841186523\n",
      "epoch n°3855 : train_loss = 1.7723015546798706, val_loss = 0.7176496386528015\n",
      "epoch n°3856 : train_loss = 1.7793940305709839, val_loss = 0.7244006991386414\n",
      "epoch n°3857 : train_loss = 1.7885231971740723, val_loss = 0.7169767022132874\n",
      "epoch n°3858 : train_loss = 1.7779266834259033, val_loss = 0.7200579047203064\n",
      "epoch n°3859 : train_loss = 1.7692636251449585, val_loss = 0.724419116973877\n",
      "epoch n°3860 : train_loss = 1.77301824092865, val_loss = 0.7203142046928406\n",
      "epoch n°3861 : train_loss = 1.7726565599441528, val_loss = 0.7244073152542114\n",
      "epoch n°3862 : train_loss = 1.7770529985427856, val_loss = 0.716891884803772\n",
      "epoch n°3863 : train_loss = 1.7792890071868896, val_loss = 0.717913806438446\n",
      "epoch n°3864 : train_loss = 1.7909616231918335, val_loss = 0.7177493572235107\n",
      "epoch n°3865 : train_loss = 1.7761353254318237, val_loss = 0.7224416136741638\n",
      "epoch n°3866 : train_loss = 1.7740322351455688, val_loss = 0.7159944772720337\n",
      "epoch n°3867 : train_loss = 1.7747212648391724, val_loss = 0.7218631505966187\n",
      "epoch n°3868 : train_loss = 1.7805240154266357, val_loss = 0.7220761775970459\n",
      "epoch n°3869 : train_loss = 1.7687504291534424, val_loss = 0.720792293548584\n",
      "epoch n°3870 : train_loss = 1.7811447381973267, val_loss = 0.7186764478683472\n",
      "epoch n°3871 : train_loss = 1.773285150527954, val_loss = 0.7176876068115234\n",
      "epoch n°3872 : train_loss = 1.7833045721054077, val_loss = 0.7220621705055237\n",
      "epoch n°3873 : train_loss = 1.771972417831421, val_loss = 0.7240643501281738\n",
      "epoch n°3874 : train_loss = 1.7777984142303467, val_loss = 0.7188222408294678\n",
      "epoch n°3875 : train_loss = 1.7766759395599365, val_loss = 0.7170758843421936\n",
      "epoch n°3876 : train_loss = 1.7772303819656372, val_loss = 0.7182185649871826\n",
      "epoch n°3877 : train_loss = 1.7865508794784546, val_loss = 0.7204815745353699\n",
      "epoch n°3878 : train_loss = 1.7824445962905884, val_loss = 0.7222706079483032\n",
      "epoch n°3879 : train_loss = 1.7811086177825928, val_loss = 0.7209311127662659\n",
      "epoch n°3880 : train_loss = 1.7825762033462524, val_loss = 0.7195441126823425\n",
      "epoch n°3881 : train_loss = 1.77596914768219, val_loss = 0.7240213751792908\n",
      "epoch n°3882 : train_loss = 1.7634493112564087, val_loss = 0.7175483107566833\n",
      "epoch n°3883 : train_loss = 1.7786684036254883, val_loss = 0.719965398311615\n",
      "epoch n°3884 : train_loss = 1.7794315814971924, val_loss = 0.7202783823013306\n",
      "epoch n°3885 : train_loss = 1.773252010345459, val_loss = 0.7206162214279175\n",
      "epoch n°3886 : train_loss = 1.7745441198349, val_loss = 0.7194669246673584\n",
      "epoch n°3887 : train_loss = 1.7762649059295654, val_loss = 0.7225232124328613\n",
      "epoch n°3888 : train_loss = 1.7733526229858398, val_loss = 0.7170694470405579\n",
      "epoch n°3889 : train_loss = 1.7763162851333618, val_loss = 0.7203652858734131\n",
      "epoch n°3890 : train_loss = 1.7709126472473145, val_loss = 0.7195529937744141\n",
      "epoch n°3891 : train_loss = 1.7795640230178833, val_loss = 0.7176892161369324\n",
      "epoch n°3892 : train_loss = 1.7781442403793335, val_loss = 0.7195993661880493\n",
      "epoch n°3893 : train_loss = 1.7854572534561157, val_loss = 0.7248760461807251\n",
      "epoch n°3894 : train_loss = 1.7738025188446045, val_loss = 0.716393768787384\n",
      "epoch n°3895 : train_loss = 1.7637851238250732, val_loss = 0.7185274958610535\n",
      "epoch n°3896 : train_loss = 1.7844654321670532, val_loss = 0.7222223877906799\n",
      "epoch n°3897 : train_loss = 1.7760905027389526, val_loss = 0.7232890725135803\n",
      "epoch n°3898 : train_loss = 1.777872920036316, val_loss = 0.7184345722198486\n",
      "epoch n°3899 : train_loss = 1.7774407863616943, val_loss = 0.7181280255317688\n",
      "epoch n°3900 : train_loss = 1.7659333944320679, val_loss = 0.7214491963386536\n",
      "epoch n°3901 : train_loss = 1.7769851684570312, val_loss = 0.720952570438385\n",
      "epoch n°3902 : train_loss = 1.7767730951309204, val_loss = 0.7215473055839539\n",
      "epoch n°3903 : train_loss = 1.7846062183380127, val_loss = 0.7194279432296753\n",
      "epoch n°3904 : train_loss = 1.7757184505462646, val_loss = 0.7186073660850525\n",
      "epoch n°3905 : train_loss = 1.7917430400848389, val_loss = 0.7235351204872131\n",
      "epoch n°3906 : train_loss = 1.7734555006027222, val_loss = 0.7218518257141113\n",
      "epoch n°3907 : train_loss = 1.768025279045105, val_loss = 0.7220233082771301\n",
      "epoch n°3908 : train_loss = 1.7680476903915405, val_loss = 0.720513105392456\n",
      "epoch n°3909 : train_loss = 1.7746975421905518, val_loss = 0.7182597517967224\n",
      "epoch n°3910 : train_loss = 1.7718833684921265, val_loss = 0.7261229753494263\n",
      "epoch n°3911 : train_loss = 1.7823182344436646, val_loss = 0.7179620862007141\n",
      "epoch n°3912 : train_loss = 1.7733378410339355, val_loss = 0.7212806940078735\n",
      "epoch n°3913 : train_loss = 1.7746477127075195, val_loss = 0.7177537083625793\n",
      "epoch n°3914 : train_loss = 1.7831783294677734, val_loss = 0.7208349108695984\n",
      "epoch n°3915 : train_loss = 1.7814130783081055, val_loss = 0.7159730792045593\n",
      "epoch n°3916 : train_loss = 1.7730990648269653, val_loss = 0.7205023765563965\n",
      "epoch n°3917 : train_loss = 1.7712085247039795, val_loss = 0.7160211205482483\n",
      "epoch n°3918 : train_loss = 1.7767804861068726, val_loss = 0.721112072467804\n",
      "epoch n°3919 : train_loss = 1.7725989818572998, val_loss = 0.7271944284439087\n",
      "epoch n°3920 : train_loss = 1.7699018716812134, val_loss = 0.7196866869926453\n",
      "epoch n°3921 : train_loss = 1.7845945358276367, val_loss = 0.7234185338020325\n",
      "epoch n°3922 : train_loss = 1.7789809703826904, val_loss = 0.7179993391036987\n",
      "epoch n°3923 : train_loss = 1.7819182872772217, val_loss = 0.7140315771102905\n",
      "epoch n°3924 : train_loss = 1.7793875932693481, val_loss = 0.7218550443649292\n",
      "epoch n°3925 : train_loss = 1.76993727684021, val_loss = 0.7216759324073792\n",
      "epoch n°3926 : train_loss = 1.7708549499511719, val_loss = 0.7173622250556946\n",
      "epoch n°3927 : train_loss = 1.7703020572662354, val_loss = 0.7198590636253357\n",
      "epoch n°3928 : train_loss = 1.7758283615112305, val_loss = 0.7238632440567017\n",
      "epoch n°3929 : train_loss = 1.7806718349456787, val_loss = 0.7164005637168884\n",
      "epoch n°3930 : train_loss = 1.771572232246399, val_loss = 0.7157379984855652\n",
      "epoch n°3931 : train_loss = 1.7763618230819702, val_loss = 0.726348340511322\n",
      "epoch n°3932 : train_loss = 1.7713245153427124, val_loss = 0.7208279967308044\n",
      "epoch n°3933 : train_loss = 1.7776857614517212, val_loss = 0.7216153740882874\n",
      "epoch n°3934 : train_loss = 1.77611243724823, val_loss = 0.7207525968551636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°3935 : train_loss = 1.7743761539459229, val_loss = 0.7190220355987549\n",
      "epoch n°3936 : train_loss = 1.7700600624084473, val_loss = 0.7149838209152222\n",
      "epoch n°3937 : train_loss = 1.7853350639343262, val_loss = 0.7143386602401733\n",
      "epoch n°3938 : train_loss = 1.7816194295883179, val_loss = 0.7215990424156189\n",
      "epoch n°3939 : train_loss = 1.7713665962219238, val_loss = 0.7179463505744934\n",
      "epoch n°3940 : train_loss = 1.7859079837799072, val_loss = 0.7190319895744324\n",
      "epoch n°3941 : train_loss = 1.7753639221191406, val_loss = 0.7215585708618164\n",
      "epoch n°3942 : train_loss = 1.7707805633544922, val_loss = 0.7204445004463196\n",
      "epoch n°3943 : train_loss = 1.7793543338775635, val_loss = 0.7170525789260864\n",
      "epoch n°3944 : train_loss = 1.780948519706726, val_loss = 0.7171700596809387\n",
      "epoch n°3945 : train_loss = 1.7727241516113281, val_loss = 0.7173980474472046\n",
      "epoch n°3946 : train_loss = 1.7846672534942627, val_loss = 0.720576286315918\n",
      "epoch n°3947 : train_loss = 1.7703458070755005, val_loss = 0.7177181839942932\n",
      "epoch n°3948 : train_loss = 1.7707191705703735, val_loss = 0.722241997718811\n",
      "epoch n°3949 : train_loss = 1.7762163877487183, val_loss = 0.7195467948913574\n",
      "epoch n°3950 : train_loss = 1.7839410305023193, val_loss = 0.7182721495628357\n",
      "epoch n°3951 : train_loss = 1.786535382270813, val_loss = 0.7177834510803223\n",
      "epoch n°3952 : train_loss = 1.7751054763793945, val_loss = 0.7203600406646729\n",
      "epoch n°3953 : train_loss = 1.7663949728012085, val_loss = 0.7157283425331116\n",
      "epoch n°3954 : train_loss = 1.7707239389419556, val_loss = 0.7169764637947083\n",
      "epoch n°3955 : train_loss = 1.7741062641143799, val_loss = 0.7192585468292236\n",
      "epoch n°3956 : train_loss = 1.7733795642852783, val_loss = 0.7240928411483765\n",
      "epoch n°3957 : train_loss = 1.7795448303222656, val_loss = 0.7238441705703735\n",
      "epoch n°3958 : train_loss = 1.773165225982666, val_loss = 0.7167567610740662\n",
      "epoch n°3959 : train_loss = 1.7834337949752808, val_loss = 0.7254024147987366\n",
      "epoch n°3960 : train_loss = 1.7729016542434692, val_loss = 0.7188964486122131\n",
      "epoch n°3961 : train_loss = 1.7755595445632935, val_loss = 0.7226053476333618\n",
      "epoch n°3962 : train_loss = 1.7629783153533936, val_loss = 0.7209698557853699\n",
      "epoch n°3963 : train_loss = 1.7848658561706543, val_loss = 0.7157799601554871\n",
      "epoch n°3964 : train_loss = 1.77796471118927, val_loss = 0.7163386940956116\n",
      "epoch n°3965 : train_loss = 1.7785263061523438, val_loss = 0.7160850167274475\n",
      "epoch n°3966 : train_loss = 1.7762668132781982, val_loss = 0.7231069803237915\n",
      "epoch n°3967 : train_loss = 1.7729651927947998, val_loss = 0.7205266952514648\n",
      "epoch n°3968 : train_loss = 1.7805405855178833, val_loss = 0.7216655015945435\n",
      "epoch n°3969 : train_loss = 1.7755299806594849, val_loss = 0.7203791737556458\n",
      "epoch n°3970 : train_loss = 1.7707682847976685, val_loss = 0.718316912651062\n",
      "epoch n°3971 : train_loss = 1.7779121398925781, val_loss = 0.7222325801849365\n",
      "epoch n°3972 : train_loss = 1.7731406688690186, val_loss = 0.7204074263572693\n",
      "epoch n°3973 : train_loss = 1.7838987112045288, val_loss = 0.7143559455871582\n",
      "epoch n°3974 : train_loss = 1.7733246088027954, val_loss = 0.7217665910720825\n",
      "epoch n°3975 : train_loss = 1.7831284999847412, val_loss = 0.72281813621521\n",
      "epoch n°3976 : train_loss = 1.7728123664855957, val_loss = 0.7206012010574341\n",
      "epoch n°3977 : train_loss = 1.7709071636199951, val_loss = 0.7172819972038269\n",
      "epoch n°3978 : train_loss = 1.7605319023132324, val_loss = 0.7191469073295593\n",
      "epoch n°3979 : train_loss = 1.7657358646392822, val_loss = 0.7201328873634338\n",
      "epoch n°3980 : train_loss = 1.7770005464553833, val_loss = 0.7189924716949463\n",
      "epoch n°3981 : train_loss = 1.764088749885559, val_loss = 0.7227022647857666\n",
      "epoch n°3982 : train_loss = 1.7713149785995483, val_loss = 0.7169671058654785\n",
      "epoch n°3983 : train_loss = 1.7713284492492676, val_loss = 0.7179585695266724\n",
      "epoch n°3984 : train_loss = 1.7739146947860718, val_loss = 0.7176352739334106\n",
      "epoch n°3985 : train_loss = 1.7729827165603638, val_loss = 0.7219655513763428\n",
      "epoch n°3986 : train_loss = 1.771176815032959, val_loss = 0.7173197865486145\n",
      "epoch n°3987 : train_loss = 1.7762980461120605, val_loss = 0.7199539542198181\n",
      "epoch n°3988 : train_loss = 1.7868419885635376, val_loss = 0.7181366682052612\n",
      "epoch n°3989 : train_loss = 1.7804023027420044, val_loss = 0.7129653692245483\n",
      "epoch n°3990 : train_loss = 1.771498680114746, val_loss = 0.720559298992157\n",
      "epoch n°3991 : train_loss = 1.7731349468231201, val_loss = 0.7165551781654358\n",
      "epoch n°3992 : train_loss = 1.7797365188598633, val_loss = 0.7240079641342163\n",
      "epoch n°3993 : train_loss = 1.77205228805542, val_loss = 0.7198024988174438\n",
      "epoch n°3994 : train_loss = 1.77830171585083, val_loss = 0.7134985327720642\n",
      "epoch n°3995 : train_loss = 1.7808185815811157, val_loss = 0.7233235239982605\n",
      "epoch n°3996 : train_loss = 1.775644063949585, val_loss = 0.7208518981933594\n",
      "epoch n°3997 : train_loss = 1.7785844802856445, val_loss = 0.7229686379432678\n",
      "epoch n°3998 : train_loss = 1.7772393226623535, val_loss = 0.717031717300415\n",
      "epoch n°3999 : train_loss = 1.7771801948547363, val_loss = 0.7201871871948242\n",
      "epoch n°4000 : train_loss = 1.7812973260879517, val_loss = 0.7258196473121643\n",
      "epoch n°4001 : train_loss = 1.7902259826660156, val_loss = 0.7193874716758728\n",
      "epoch n°4002 : train_loss = 1.7807809114456177, val_loss = 0.7212759256362915\n",
      "epoch n°4003 : train_loss = 1.7788153886795044, val_loss = 0.717850387096405\n",
      "epoch n°4004 : train_loss = 1.7651498317718506, val_loss = 0.7229604125022888\n",
      "epoch n°4005 : train_loss = 1.7864406108856201, val_loss = 0.7247359156608582\n",
      "epoch n°4006 : train_loss = 1.7751322984695435, val_loss = 0.7176123857498169\n",
      "epoch n°4007 : train_loss = 1.772301435470581, val_loss = 0.7175583839416504\n",
      "epoch n°4008 : train_loss = 1.7758220434188843, val_loss = 0.7216359376907349\n",
      "epoch n°4009 : train_loss = 1.7847316265106201, val_loss = 0.7203802466392517\n",
      "epoch n°4010 : train_loss = 1.774404525756836, val_loss = 0.7207518815994263\n",
      "epoch n°4011 : train_loss = 1.7866426706314087, val_loss = 0.7173465490341187\n",
      "epoch n°4012 : train_loss = 1.7735743522644043, val_loss = 0.7149683833122253\n",
      "epoch n°4013 : train_loss = 1.7692897319793701, val_loss = 0.7189539670944214\n",
      "epoch n°4014 : train_loss = 1.7723733186721802, val_loss = 0.7202692627906799\n",
      "epoch n°4015 : train_loss = 1.7772986888885498, val_loss = 0.7174297571182251\n",
      "epoch n°4016 : train_loss = 1.7775957584381104, val_loss = 0.7242971658706665\n",
      "epoch n°4017 : train_loss = 1.7748345136642456, val_loss = 0.7212916612625122\n",
      "epoch n°4018 : train_loss = 1.7833526134490967, val_loss = 0.7215325832366943\n",
      "epoch n°4019 : train_loss = 1.7704042196273804, val_loss = 0.7206787467002869\n",
      "epoch n°4020 : train_loss = 1.7806426286697388, val_loss = 0.7187507748603821\n",
      "epoch n°4021 : train_loss = 1.7709046602249146, val_loss = 0.7166986465454102\n",
      "epoch n°4022 : train_loss = 1.769459843635559, val_loss = 0.7198789715766907\n",
      "epoch n°4023 : train_loss = 1.7762346267700195, val_loss = 0.7220558524131775\n",
      "epoch n°4024 : train_loss = 1.7832849025726318, val_loss = 0.7215279936790466\n",
      "epoch n°4025 : train_loss = 1.7757781744003296, val_loss = 0.7247899770736694\n",
      "epoch n°4026 : train_loss = 1.7745641469955444, val_loss = 0.7238571047782898\n",
      "epoch n°4027 : train_loss = 1.7772302627563477, val_loss = 0.7181806564331055\n",
      "epoch n°4028 : train_loss = 1.772703766822815, val_loss = 0.719494104385376\n",
      "epoch n°4029 : train_loss = 1.7793384790420532, val_loss = 0.7180376648902893\n",
      "epoch n°4030 : train_loss = 1.778461217880249, val_loss = 0.7183877229690552\n",
      "epoch n°4031 : train_loss = 1.7721245288848877, val_loss = 0.7219157218933105\n",
      "epoch n°4032 : train_loss = 1.7801320552825928, val_loss = 0.7191909551620483\n",
      "epoch n°4033 : train_loss = 1.7688225507736206, val_loss = 0.7194865942001343\n",
      "epoch n°4034 : train_loss = 1.7673662900924683, val_loss = 0.7208295464515686\n",
      "epoch n°4035 : train_loss = 1.784209966659546, val_loss = 0.7201621532440186\n",
      "epoch n°4036 : train_loss = 1.774631381034851, val_loss = 0.722575306892395\n",
      "epoch n°4037 : train_loss = 1.7649401426315308, val_loss = 0.7221566438674927\n",
      "epoch n°4038 : train_loss = 1.7657976150512695, val_loss = 0.7214608192443848\n",
      "epoch n°4039 : train_loss = 1.7763030529022217, val_loss = 0.7239231467247009\n",
      "epoch n°4040 : train_loss = 1.7804527282714844, val_loss = 0.7240793704986572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°4041 : train_loss = 1.77318274974823, val_loss = 0.7241714596748352\n",
      "epoch n°4042 : train_loss = 1.7757076025009155, val_loss = 0.7169618606567383\n",
      "epoch n°4043 : train_loss = 1.786503553390503, val_loss = 0.7177858948707581\n",
      "epoch n°4044 : train_loss = 1.7811509370803833, val_loss = 0.7178017497062683\n",
      "epoch n°4045 : train_loss = 1.7775827646255493, val_loss = 0.7222458720207214\n",
      "epoch n°4046 : train_loss = 1.7749361991882324, val_loss = 0.7228481769561768\n",
      "epoch n°4047 : train_loss = 1.76359224319458, val_loss = 0.7279378771781921\n",
      "epoch n°4048 : train_loss = 1.7625898122787476, val_loss = 0.7223535180091858\n",
      "epoch n°4049 : train_loss = 1.7736786603927612, val_loss = 0.7186371684074402\n",
      "epoch n°4050 : train_loss = 1.7770031690597534, val_loss = 0.716760516166687\n",
      "epoch n°4051 : train_loss = 1.7806074619293213, val_loss = 0.7195248007774353\n",
      "epoch n°4052 : train_loss = 1.7716400623321533, val_loss = 0.7156587243080139\n",
      "epoch n°4053 : train_loss = 1.7787816524505615, val_loss = 0.7235791683197021\n",
      "epoch n°4054 : train_loss = 1.7735824584960938, val_loss = 0.7214046716690063\n",
      "epoch n°4055 : train_loss = 1.7720972299575806, val_loss = 0.7151229381561279\n",
      "epoch n°4056 : train_loss = 1.7736990451812744, val_loss = 0.7193579077720642\n",
      "epoch n°4057 : train_loss = 1.7782304286956787, val_loss = 0.7145305275917053\n",
      "epoch n°4058 : train_loss = 1.7902798652648926, val_loss = 0.7210296988487244\n",
      "epoch n°4059 : train_loss = 1.7749918699264526, val_loss = 0.7210227847099304\n",
      "epoch n°4060 : train_loss = 1.7779502868652344, val_loss = 0.7218503355979919\n",
      "epoch n°4061 : train_loss = 1.769220232963562, val_loss = 0.7145907878875732\n",
      "epoch n°4062 : train_loss = 1.775375247001648, val_loss = 0.7186663746833801\n",
      "epoch n°4063 : train_loss = 1.7837542295455933, val_loss = 0.7219652533531189\n",
      "epoch n°4064 : train_loss = 1.761053204536438, val_loss = 0.7227734923362732\n",
      "epoch n°4065 : train_loss = 1.7733118534088135, val_loss = 0.7184852361679077\n",
      "epoch n°4066 : train_loss = 1.7732961177825928, val_loss = 0.7200481295585632\n",
      "epoch n°4067 : train_loss = 1.7655067443847656, val_loss = 0.7194563150405884\n",
      "epoch n°4068 : train_loss = 1.771860122680664, val_loss = 0.7210261821746826\n",
      "epoch n°4069 : train_loss = 1.7709943056106567, val_loss = 0.7205601930618286\n",
      "epoch n°4070 : train_loss = 1.7683019638061523, val_loss = 0.7197003960609436\n",
      "epoch n°4071 : train_loss = 1.7806719541549683, val_loss = 0.7165944576263428\n",
      "epoch n°4072 : train_loss = 1.769466519355774, val_loss = 0.7198320627212524\n",
      "epoch n°4073 : train_loss = 1.7845635414123535, val_loss = 0.7192438244819641\n",
      "epoch n°4074 : train_loss = 1.7730721235275269, val_loss = 0.7197785377502441\n",
      "epoch n°4075 : train_loss = 1.7758851051330566, val_loss = 0.7176505327224731\n",
      "epoch n°4076 : train_loss = 1.7720322608947754, val_loss = 0.720336377620697\n",
      "epoch n°4077 : train_loss = 1.7697924375534058, val_loss = 0.7202684283256531\n",
      "epoch n°4078 : train_loss = 1.778672218322754, val_loss = 0.718621551990509\n",
      "epoch n°4079 : train_loss = 1.7771564722061157, val_loss = 0.7171879410743713\n",
      "80278.04339027405\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_6L_12cont_biased sampling.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4992f4b2b0>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f49937d6e20>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON TRAIN DATA : \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/stateATT_6L_12cont_biased sampling.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 2\u001b[0m SCORES \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(fname, train, test, val, N)\u001b[0m\n\u001b[1;32m     20\u001b[0m lpfreqs \u001b[38;5;241m=\u001b[39m lpfreqs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m length \u001b[38;5;241m=\u001b[39m length\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPID\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpfreqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlpfreqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(y_hat,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m y \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(y, \u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 54\u001b[0m, in \u001b[0;36mAttNet.forward\u001b[0;34m(self, x, PID, pfreqs, lpfreqs, pos, length)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,PID,pfreqs,lpfreqs,pos,length):\n\u001b[1;32m     53\u001b[0m     X_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_input(x,PID,pfreqs,lpfreqs,pos,length)\n\u001b[0;32m---> 54\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf(features)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(out)\n\u001b[1;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(out))\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mAttBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_w(x)\n\u001b[1;32m     19\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_w(x)\n\u001b[0;32m---> 20\u001b[0m out,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(out)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1028\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1029\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1036\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight, average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5353\u001b[0m     dropout_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m   5355\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5356\u001b[0m \u001b[38;5;66;03m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[1;32m   5357\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m-> 5358\u001b[0m attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5359\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[1;32m   5360\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "File \u001b[0;32m~/mambaforge/envs/deeplearning/lib/python3.8/site-packages/torch/nn/functional.py:5034\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   5032\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(E)\n\u001b[1;32m   5033\u001b[0m \u001b[38;5;66;03m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[0;32m-> 5034\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   5035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5036\u001b[0m     attn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m attn_mask\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fname = \"models/stateATT_6L_12cont_biased sampling.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_12cont_biased sampling.pth\" \n",
    "SCORES = eval_model_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_12cont_biased sampling.pth\" \n",
    "SCORES = eval_model_acc_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3664e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
