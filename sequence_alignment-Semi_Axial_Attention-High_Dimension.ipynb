{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') #to avoid issues in the dataloading\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1378df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20891a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 4\n",
    "rep = torch.tensor([4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0, 0, 0, 0, 0, 0, 0,0, 0])\n",
    "#max 8 \n",
    "rep = torch.tensor([8, 8, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 5, 7, 8, 8, 8, 8, 8])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0.2, 0.5, 0.3, 0.9, 0.8, 0.5, 0.9, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=2000,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by precomputing a bunch of data on the sequence families\n",
    "        \"\"\"\n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = [] #path of each families in the folder\n",
    "        self.seq_lens = [] #length of each member of the family\n",
    "        self.seq_nums = [] #number of member of the family\n",
    "        self.aa_freqs = [] #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = [] #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.ones(20)\n",
    "                    p_aa_freq = torch.ones(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    #aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    #p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths.append(path)\n",
    "                    self.seq_lens.append(l)\n",
    "                    self.seq_nums.append(n)\n",
    "                    self.aa_freqs.append(aa_freq)\n",
    "                    self.p_aa_freqs.append(p_aa_freq)\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto',rep=rep,rand=rand): \n",
    "        \"\"\"\n",
    "        input idx of the family of the sample\n",
    "        return a Tensor containing several samples from the family corresponding to the index\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        bin_n = len(rep) #for biasing the sampling\n",
    "        \n",
    "        precomputed_pos = [] #positions of the amino-acids\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        #sampling more for big families and long sequences\n",
    "        if type(sample_size) != int:\n",
    "            sample_s = min(n * l,25_000)\n",
    "            coef = round((sample_s)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1): #computing PID and removing aligned '-' ##might need to compute the actual column num\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_j != '-' and aa_i != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0 #case 0/0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = [] #possible position to take\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "            \n",
    "            # biasing for more diverse PID  \n",
    "            bin_idx = int(PID_ij//(1/bin_n))\n",
    "            rep_number = rep[bin_idx].clone()\n",
    "            if torch.rand(1) < rand[bin_idx]:\n",
    "                rep_number+=1\n",
    "            \n",
    "            for _ in range(rep_number):\n",
    "                try:   \n",
    "                    k = np.random.choice(possible_k)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #adding to the output\n",
    "                lengths.append(align_l)\n",
    "                pos_ij = (k + precomputed_pos)\n",
    "                pos.append(pos_ij)\n",
    "                \n",
    "                #computing the windows\n",
    "                window_i = ''\n",
    "                window_j = ''\n",
    "                for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                    if w < 0 or w >= align_l: #case of the edges\n",
    "                        window_i += 'Z'\n",
    "                        window_j += 'Z'\n",
    "                    else:\n",
    "                        window_i += seq_i[w]\n",
    "                        window_j += seq_j[w]\n",
    "\n",
    "                y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "                X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "\n",
    "                X.append(X_i)\n",
    "                y.append(y_j)\n",
    "                PIDs.append(PID_ij)\n",
    "                #computing the local PID\n",
    "                local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "\n",
    "                loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "                try:\n",
    "                    tmp = local_PID_ij/loc_comp  \n",
    "                except:\n",
    "                    tmp = 0 #case 0/0\n",
    "\n",
    "                local_PIDs.append(tmp)\n",
    "                \n",
    "                fam_freqs = self.aa_freqs[idx].clone()\n",
    "                fam_freqs[y_j] -= 1\n",
    "                pfreqs.append(fam_freqs)\n",
    "                \n",
    "                p_i_freqs = self.p_aa_freqs[idx][i].clone()\n",
    "                p_j_freqs = self.p_aa_freqs[idx][j].clone()\n",
    "                p_j_freqs[y_j] -= 1\n",
    "                \n",
    "                local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "\n",
    "                assert y_j < 20\n",
    "                assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache() #clearing the cache\n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        \n",
    "        out = X,y.long(),PIDs,local_PIDs,pos,lengths,pfreqs,local_pfreqs\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6,div=2000)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\\n\\nfname = \\'data/train_dataset.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run This for new data (precomputing)\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6,div=2000)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\n",
    "\n",
    "fname = 'data/train_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12690\n",
      "2724\n",
      "2698\n"
     ]
    }
   ],
   "source": [
    "#To load datasets with all features computed\n",
    "fname = 'data/train_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_dataset.cont_size = CONT_SIZE\n",
    "test_dataset.cont_size = CONT_SIZE\n",
    "val_dataset.cont_size = CONT_SIZE\n",
    "\n",
    "train_dataset.div = 2000\n",
    "test_dataset.div = 2000\n",
    "val_dataset.div = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Transforms a list of tensors to a batch tensor\n",
    "    \"\"\"\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[4] for item in batch],dim=0)\n",
    "    length = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[6] for item in batch],dim=0)\n",
    "    l_pfreqs = torch.cat([item[7] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID, pos, length, pfreqs, l_pfreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06957f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w_tgt = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att_tgt = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        \n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        \n",
    "        Q_tgt = self.Q_w_tgt(tgt)\n",
    "        K_src = self.K_w_src(src)\n",
    "        V_src = self.V_w_src(src)\n",
    "        \n",
    "        out,_ = self.att_tgt(Q_tgt,K_src,V_src,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb64bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastCrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        Q = self.Q_w(tgt)\n",
    "        K = self.K_w(src)\n",
    "        V = self.V_w(src)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1da4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Row-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*n,l,d))).view(b,n,l,d)\n",
    "        return out\n",
    "    \n",
    "class ColAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Column-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*l,n,d))).view((b,n,l,d)) #might not work\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d39d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward2D(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or 2*in_features\n",
    "        hidden_dim = wide_factor * 2*in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(2*in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        \n",
    "        #dst = src.transpose(1, 2).reshape(b,l,n*d)\n",
    "        #src = dst.reshape(b,l,-1,d).transpose(1,2)\n",
    "\n",
    "        out = x.transpose(1, 2).reshape(b,l,n*d)\n",
    "        out = self.lin1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        out = out.reshape(b,l,-1,d).transpose(1,2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4,act2 = None):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = act2 or nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(from the timm library)\n",
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.row_att_block = RowAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff2d = FeedForward2D(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.row_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff2d(out))\n",
    "        out = self.norm2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd38d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Last Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.row_att_block = RowAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff2d = FeedForward2D(in_features,wide_factor=wide_factor)\n",
    "        self.cross_att_block = CrossAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedForward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.norm3 = nn.LayerNorm(in_features)\n",
    "        self.norm4 = nn.LayerNorm(in_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.row_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff2d(out))\n",
    "        out = self.norm2(out)\n",
    "        out = out[:,1] + self.drop_path(self.cross_att_block(out))\n",
    "        out = self.norm3(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier Head of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    \"\"\"\n",
    "    Returns the initialization parameters of the Transformer\n",
    "    \"\"\"\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "085855a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-like neural net\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,embedding_dim=128,input_dim=21,out_size=20,num_seq=2,seq_len=2*CONT_SIZE+4,clf_dims=[256,64],cont_size=CONT_SIZE):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.embedding_dim = embedding_dim or in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.cont_size=cont_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        blocks = []\n",
    "        r = min(len(num_heads),len(head_dims),len(wide_factors),len(drops))\n",
    "        for i,(n_h, h_d,w,d) in enumerate(zip(num_heads,head_dims,wide_factors,drops)):\n",
    "            if i < r-1:\n",
    "                blocks.append(Block(embedding_dim,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "            else:\n",
    "                blocks.append(LastBlock(embedding_dim,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "            \n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        self.clf = Classifier_Head(embedding_dim,clf_dims,out_size=20,seq_len=seq_len)\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "        \n",
    "        pid_layers = [nn.Linear(1,2*in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "        embed_layers =  [nn.Linear(in_features,embedding_dim),nn.GELU()]\n",
    "        self.embedding = nn.Sequential(*embed_layers)\n",
    "        self.average_weigths = nn.Parameter(torch.randn((1,seq_len,1)))\n",
    "\n",
    "    def to_input(self,x,PID,pos,length,pfreqs,l_pfreqs):\n",
    "        \n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1)\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(32**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(32**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "        \n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        \n",
    "        out = torch.stack([seq1,seq2],dim=1)\n",
    "        \n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        pid = rearrange(pid,\"b 1 (n e) -> b n 1 e\",n=2)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        \n",
    "        pfreqs = F.normalize(pfreqs,p=1,dim=1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pf = repeat(pf,\"b 1 d -> b 2 1 d\")\n",
    "        l_pfreqs = F.normalize(l_pfreqs,p=1,dim=2)\n",
    "        lpf = F.pad(l_pfreqs,pad=pad, mode='constant', value=0).unsqueeze(2)\n",
    "        out = torch.cat([out,pf,pid,lpf],dim=2)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def forward(self,x,PID,pos,length,pfreqs,l_pfreqs):\n",
    "        X_input = self.to_input(x,PID,pos,length,pfreqs,l_pfreqs)\n",
    "        X_input = self.embedding(X_input)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        \n",
    "        w = F.softmax(self.average_weigths,dim=1) \n",
    "        clf_input = F.gelu((features * w).sum(dim=1)) #weigthed average\n",
    "        y_pred = self.clf(clf_input)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Used for checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4e3e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "AttNet                                             [1, 20]                   464\n",
      "├─Sequential: 1-1                                  [1, 64]                   --\n",
      "│    └─Linear: 2-1                                 [1, 64]                   128\n",
      "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
      "├─Sequential: 1-2                                  [1, 2, 64, 128]           --\n",
      "│    └─Linear: 2-3                                 [1, 2, 64, 128]           4,224\n",
      "│    └─GELU: 2-4                                   [1, 2, 64, 128]           --\n",
      "├─Sequential: 1-3                                  [1, 64, 128]              --\n",
      "│    └─Block: 2-5                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-2                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-4                     [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-5                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 128]           256\n",
      "│    └─Block: 2-6                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-7                       [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-8                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-10                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-11                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 128]           256\n",
      "│    └─Block: 2-7                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-14                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-16                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-17                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 128]           256\n",
      "│    └─LastBlock: 2-8                              [1, 64, 128]              --\n",
      "│    │    └─RowAttBlock: 3-19                      [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-20                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-22                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-23                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 128]           256\n",
      "│    │    └─CrossAttBlock: 3-25                    [1, 64, 128]              394,368\n",
      "│    │    └─DropPath: 3-26                         [1, 64, 128]              --\n",
      "│    │    └─LayerNorm: 3-27                        [1, 64, 128]              256\n",
      "│    │    └─FeedForward: 3-28                      [1, 64, 128]              65,920\n",
      "│    │    └─DropPath: 3-29                         [1, 64, 128]              --\n",
      "│    │    └─LayerNorm: 3-30                        [1, 64, 128]              256\n",
      "├─Classifier_Head: 1-4                             [1, 20]                   --\n",
      "│    └─Sequential: 2-9                             [1, 20]                   --\n",
      "│    │    └─Linear: 3-31                           [1, 512]                  66,048\n",
      "│    │    └─GELU: 3-32                             [1, 512]                  --\n",
      "│    │    └─Dropout: 3-33                          [1, 512]                  --\n",
      "│    │    └─Linear: 3-34                           [1, 128]                  65,664\n",
      "│    │    └─GELU: 3-35                             [1, 128]                  --\n",
      "│    │    └─Dropout: 3-36                          [1, 128]                  --\n",
      "│    │    └─Linear: 3-37                           [1, 32]                   4,128\n",
      "│    │    └─GELU: 3-38                             [1, 32]                   --\n",
      "│    │    └─Dropout: 3-39                          [1, 32]                   --\n",
      "│    │    └─Linear: 3-40                           [1, 20]                   660\n",
      "====================================================================================================\n",
      "Total params: 3,233,284\n",
      "Trainable params: 3,233,284\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.44\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.22\n",
      "Params size (MB): 7.67\n",
      "Estimated Total Size (MB): 14.89\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,4,8,32,2,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32],embedding_dim=128)\n",
    "b = 1\n",
    "print(torchinfo.summary(model,[(b,4*CONT_SIZE+1,21),(b,),(b,4*CONT_SIZE+1),(b,),(b,20),(b,2,20)]))\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N_train=10,N_test_val=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N_train):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        samples = torch.cat(eval_losses)\n",
    "        score_train = torch.mean(samples).detach().cpu().item()\n",
    "        print(f\"{score_train = }\")\n",
    "        print(f\"{len(samples) = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N_test_val):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "                \n",
    "        samples = torch.cat(eval_losses)\n",
    "        score_test = torch.mean(samples).detach().cpu().item()\n",
    "        print(f\"{score_test = }\")\n",
    "        print(f\"{len(samples) = }\\n\")\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N_test_val):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        samples = torch.cat(eval_losses)\n",
    "        score_val = torch.mean(samples).detach().cpu().item()\n",
    "        print(f\"{score_val = }\")\n",
    "        print(f\"{len(samples) = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0982994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "                \n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "                \n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4d60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_acc_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "                \n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            i = min(i,19)\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True,cont_size=CONT_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    \"\"\"\n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            length = length.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            l_pfreqs = l_pfreqs.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "            l = Loss(y_hat,y)/440 \n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y,PID,lPID,pos,length,pfreqs,l_pfreqs in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                l_pfreqs = l_pfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length,pfreqs,l_pfreqs)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "                \n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#                   input_size,N,head,head_dim,wide_factor,drop_prob \n",
    "params = get_params(32,5,4,64,2,0.1)\n",
    "model = AttNet(*params,clf_dims=[512],embedding_dim=1024)\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/state_RowAttColMLP_Embedding_512.pth\"\n",
    "start = time.time()\n",
    "\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4af7e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"models/state_RowAttColMLP_Embedding_3.pth\",\n",
    "\"models/state_RowAttColMLP_Embedding_4.pth\",\n",
    "\"models/state_RowAttColMLP_Embedding_512.pth\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "629ea6e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "AttNet                                             [1, 20]                   464\n",
      "├─Sequential: 1-1                                  [1, 64]                   --\n",
      "│    └─Linear: 2-1                                 [1, 64]                   128\n",
      "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
      "├─Sequential: 1-2                                  [1, 2, 64, 128]           --\n",
      "│    └─Linear: 2-3                                 [1, 2, 64, 128]           4,224\n",
      "│    └─GELU: 2-4                                   [1, 2, 64, 128]           --\n",
      "├─Sequential: 1-3                                  [1, 64, 128]              --\n",
      "│    └─Block: 2-5                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-2                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-4                     [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-5                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 128]           256\n",
      "│    └─Block: 2-6                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-7                       [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-8                          [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-10                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-11                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 128]           256\n",
      "│    └─Block: 2-7                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-14                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-16                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-17                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 128]           256\n",
      "│    └─Block: 2-8                                  [1, 2, 64, 128]           --\n",
      "│    │    └─RowAttBlock: 3-19                      [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-20                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-22                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-23                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 128]           256\n",
      "│    └─LastBlock: 2-9                              [1, 64, 128]              --\n",
      "│    │    └─RowAttBlock: 3-25                      [1, 2, 64, 128]           394,368\n",
      "│    │    └─DropPath: 3-26                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-27                        [1, 2, 64, 128]           256\n",
      "│    │    └─FeedForward2D: 3-28                    [1, 2, 64, 128]           262,912\n",
      "│    │    └─DropPath: 3-29                         [1, 2, 64, 128]           --\n",
      "│    │    └─LayerNorm: 3-30                        [1, 2, 64, 128]           256\n",
      "│    │    └─CrossAttBlock: 3-31                    [1, 64, 128]              394,368\n",
      "│    │    └─DropPath: 3-32                         [1, 64, 128]              --\n",
      "│    │    └─LayerNorm: 3-33                        [1, 64, 128]              256\n",
      "│    │    └─FeedForward: 3-34                      [1, 64, 128]              65,920\n",
      "│    │    └─DropPath: 3-35                         [1, 64, 128]              --\n",
      "│    │    └─LayerNorm: 3-36                        [1, 64, 128]              256\n",
      "├─Classifier_Head: 1-4                             [1, 20]                   --\n",
      "│    └─Sequential: 2-10                            [1, 20]                   --\n",
      "│    │    └─Linear: 3-37                           [1, 512]                  66,048\n",
      "│    │    └─GELU: 3-38                             [1, 512]                  --\n",
      "│    │    └─Dropout: 3-39                          [1, 512]                  --\n",
      "│    │    └─Linear: 3-40                           [1, 20]                   10,260\n",
      "====================================================================================================\n",
      "Total params: 3,830,884\n",
      "Trainable params: 3,830,884\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.91\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 8.79\n",
      "Params size (MB): 9.01\n",
      "Estimated Total Size (MB): 17.80\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "AttNet                                             [1, 20]                   464\n",
      "├─Sequential: 1-1                                  [1, 64]                   --\n",
      "│    └─Linear: 2-1                                 [1, 64]                   128\n",
      "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
      "├─Sequential: 1-2                                  [1, 2, 64, 256]           --\n",
      "│    └─Linear: 2-3                                 [1, 2, 64, 256]           8,448\n",
      "│    └─GELU: 2-4                                   [1, 2, 64, 256]           --\n",
      "├─Sequential: 1-3                                  [1, 64, 256]              --\n",
      "│    └─Block: 2-5                                  [1, 2, 64, 256]           --\n",
      "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 256]           525,568\n",
      "│    │    └─DropPath: 3-2                          [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 256]           512\n",
      "│    │    └─FeedForward2D: 3-4                     [1, 2, 64, 256]           1,050,112\n",
      "│    │    └─DropPath: 3-5                          [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 256]           512\n",
      "│    └─Block: 2-6                                  [1, 2, 64, 256]           --\n",
      "│    │    └─RowAttBlock: 3-7                       [1, 2, 64, 256]           525,568\n",
      "│    │    └─DropPath: 3-8                          [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 256]           512\n",
      "│    │    └─FeedForward2D: 3-10                    [1, 2, 64, 256]           1,050,112\n",
      "│    │    └─DropPath: 3-11                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 256]           512\n",
      "│    └─Block: 2-7                                  [1, 2, 64, 256]           --\n",
      "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 256]           525,568\n",
      "│    │    └─DropPath: 3-14                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 256]           512\n",
      "│    │    └─FeedForward2D: 3-16                    [1, 2, 64, 256]           1,050,112\n",
      "│    │    └─DropPath: 3-17                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 256]           512\n",
      "│    └─Block: 2-8                                  [1, 2, 64, 256]           --\n",
      "│    │    └─RowAttBlock: 3-19                      [1, 2, 64, 256]           525,568\n",
      "│    │    └─DropPath: 3-20                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 256]           512\n",
      "│    │    └─FeedForward2D: 3-22                    [1, 2, 64, 256]           1,050,112\n",
      "│    │    └─DropPath: 3-23                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 256]           512\n",
      "│    └─LastBlock: 2-9                              [1, 64, 256]              --\n",
      "│    │    └─RowAttBlock: 3-25                      [1, 2, 64, 256]           525,568\n",
      "│    │    └─DropPath: 3-26                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-27                        [1, 2, 64, 256]           512\n",
      "│    │    └─FeedForward2D: 3-28                    [1, 2, 64, 256]           1,050,112\n",
      "│    │    └─DropPath: 3-29                         [1, 2, 64, 256]           --\n",
      "│    │    └─LayerNorm: 3-30                        [1, 2, 64, 256]           512\n",
      "│    │    └─CrossAttBlock: 3-31                    [1, 64, 256]              525,568\n",
      "│    │    └─DropPath: 3-32                         [1, 64, 256]              --\n",
      "│    │    └─LayerNorm: 3-33                        [1, 64, 256]              512\n",
      "│    │    └─FeedForward: 3-34                      [1, 64, 256]              262,912\n",
      "│    │    └─DropPath: 3-35                         [1, 64, 256]              --\n",
      "│    │    └─LayerNorm: 3-36                        [1, 64, 256]              512\n",
      "├─Classifier_Head: 1-4                             [1, 20]                   --\n",
      "│    └─Sequential: 2-10                            [1, 20]                   --\n",
      "│    │    └─Linear: 3-37                           [1, 512]                  131,584\n",
      "│    │    └─GELU: 3-38                             [1, 512]                  --\n",
      "│    │    └─Dropout: 3-39                          [1, 512]                  --\n",
      "│    │    └─Linear: 3-40                           [1, 20]                   10,260\n",
      "====================================================================================================\n",
      "Total params: 8,823,908\n",
      "Trainable params: 8,823,908\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 8.56\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 13.24\n",
      "Params size (MB): 28.98\n",
      "Estimated Total Size (MB): 42.23\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "AttNet                                             [1, 20]                   464\n",
      "├─Sequential: 1-1                                  [1, 64]                   --\n",
      "│    └─Linear: 2-1                                 [1, 64]                   128\n",
      "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
      "├─Sequential: 1-2                                  [1, 2, 64, 512]           --\n",
      "│    └─Linear: 2-3                                 [1, 2, 64, 512]           16,896\n",
      "│    └─GELU: 2-4                                   [1, 2, 64, 512]           --\n",
      "├─Sequential: 1-3                                  [1, 64, 512]              --\n",
      "│    └─Block: 2-5                                  [1, 2, 64, 512]           --\n",
      "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 512]           787,968\n",
      "│    │    └─DropPath: 3-2                          [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 512]           1,024\n",
      "│    │    └─FeedForward2D: 3-4                     [1, 2, 64, 512]           4,197,376\n",
      "│    │    └─DropPath: 3-5                          [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 512]           1,024\n",
      "│    └─Block: 2-6                                  [1, 2, 64, 512]           --\n",
      "│    │    └─RowAttBlock: 3-7                       [1, 2, 64, 512]           787,968\n",
      "│    │    └─DropPath: 3-8                          [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 512]           1,024\n",
      "│    │    └─FeedForward2D: 3-10                    [1, 2, 64, 512]           4,197,376\n",
      "│    │    └─DropPath: 3-11                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 512]           1,024\n",
      "│    └─Block: 2-7                                  [1, 2, 64, 512]           --\n",
      "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 512]           787,968\n",
      "│    │    └─DropPath: 3-14                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 512]           1,024\n",
      "│    │    └─FeedForward2D: 3-16                    [1, 2, 64, 512]           4,197,376\n",
      "│    │    └─DropPath: 3-17                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 512]           1,024\n",
      "│    └─Block: 2-8                                  [1, 2, 64, 512]           --\n",
      "│    │    └─RowAttBlock: 3-19                      [1, 2, 64, 512]           787,968\n",
      "│    │    └─DropPath: 3-20                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 512]           1,024\n",
      "│    │    └─FeedForward2D: 3-22                    [1, 2, 64, 512]           4,197,376\n",
      "│    │    └─DropPath: 3-23                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 512]           1,024\n",
      "│    └─LastBlock: 2-9                              [1, 64, 512]              --\n",
      "│    │    └─RowAttBlock: 3-25                      [1, 2, 64, 512]           787,968\n",
      "│    │    └─DropPath: 3-26                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-27                        [1, 2, 64, 512]           1,024\n",
      "│    │    └─FeedForward2D: 3-28                    [1, 2, 64, 512]           4,197,376\n",
      "│    │    └─DropPath: 3-29                         [1, 2, 64, 512]           --\n",
      "│    │    └─LayerNorm: 3-30                        [1, 2, 64, 512]           1,024\n",
      "│    │    └─CrossAttBlock: 3-31                    [1, 64, 512]              787,968\n",
      "│    │    └─DropPath: 3-32                         [1, 64, 512]              --\n",
      "│    │    └─LayerNorm: 3-33                        [1, 64, 512]              1,024\n",
      "│    │    └─FeedForward: 3-34                      [1, 64, 512]              1,050,112\n",
      "│    │    └─DropPath: 3-35                         [1, 64, 512]              --\n",
      "│    │    └─LayerNorm: 3-36                        [1, 64, 512]              1,024\n",
      "├─Classifier_Head: 1-4                             [1, 20]                   --\n",
      "│    └─Sequential: 2-10                            [1, 20]                   --\n",
      "│    │    └─Linear: 3-37                           [1, 512]                  262,656\n",
      "│    │    └─GELU: 3-38                             [1, 512]                  --\n",
      "│    │    └─Dropout: 3-39                          [1, 512]                  --\n",
      "│    │    └─Linear: 3-40                           [1, 20]                   10,260\n",
      "====================================================================================================\n",
      "Total params: 27,067,492\n",
      "Trainable params: 27,067,492\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 28.11\n",
      "====================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 22.16\n",
      "Params size (MB): 101.95\n",
      "Estimated Total Size (MB): 124.12\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for fname in a:\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        model = torch.load(fp).model.to('cpu').eval()\n",
    "    b = 1\n",
    "    print(torchinfo.summary(model,[(b,4*CONT_SIZE+1,21),(b,),(b,4*CONT_SIZE+1),(b,),(b,20),(b,2,20)]))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b62656a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON TRAIN DATA : \n",
      "score_train = 0.6813517212867737\n",
      "len(samples) = 864977\n",
      "\n",
      "EVALUATING ON TEST DATA : \n",
      "score_test = 0.6923254728317261\n",
      "len(samples) = 927582\n",
      "\n",
      "EVALUATING ON VAL DATA : \n",
      "score_val = 0.6853371262550354\n",
      "len(samples) = 914101\n",
      "\n",
      "EVALUATING ON TRAIN DATA : \n",
      "score_train = 0.6682887673377991\n",
      "len(samples) = 864357\n",
      "\n",
      "EVALUATING ON TEST DATA : \n",
      "score_test = 0.6947290301322937\n",
      "len(samples) = 929439\n",
      "\n",
      "EVALUATING ON VAL DATA : \n",
      "score_val = 0.6876189708709717\n",
      "len(samples) = 913024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = a[0]\n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader,N_test_val=50)\n",
    "\n",
    "fname = a[1]\n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader,N_test_val=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
