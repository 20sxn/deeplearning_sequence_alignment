{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e87bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=1400000,verbose=False):\n",
    "        \n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto'): \n",
    "        #for each sample:\n",
    "        #sample i, j st i != j the two sequences to compare\n",
    "        #sample k the sequence position of the prediction\n",
    "        #compute the file positions of the 25 + 1 AA\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        \n",
    "        precomputed_pos = []\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        if type(sample_size) != int:\n",
    "            coef = round((n**2 * l)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1):\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_i != '-' and aa_j != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = []\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "                    \n",
    "            try:   \n",
    "                k = np.random.choice(possible_k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            lengths.append(align_l)\n",
    "            pos_ij = (k + precomputed_pos)\n",
    "            pos.append(pos_ij)\n",
    "            \n",
    "            window_i = ''\n",
    "            window_j = ''\n",
    "            \n",
    "            for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                if w < 0 or w >= align_l: #case of the edges\n",
    "                    window_i += 'Z'\n",
    "                    window_j += 'Z'\n",
    "                else:\n",
    "                    window_i += seq_i[w]\n",
    "                    window_j += seq_j[w]\n",
    "        \n",
    "            y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "            X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "            \n",
    "            X.append(X_i)\n",
    "            y.append(y_j)\n",
    "            PIDs.append(PID_ij)\n",
    "            local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                         + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "            \n",
    "            loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                         + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "            try:\n",
    "                tmp = local_PID_ij/loc_comp\n",
    "            except:\n",
    "                tmp = 0\n",
    "                \n",
    "            local_PIDs.append(tmp)\n",
    "            pfreqs.append(self.aa_freqs[idx])\n",
    "            p_i_freqs = self.p_aa_freqs[idx][i]\n",
    "            p_j_freqs = self.p_aa_freqs[idx][j]\n",
    "            \n",
    "            local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "            \n",
    "            assert y_j < 20\n",
    "            assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache()   \n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        out = X,y.long(),PIDs,local_PIDs,pfreqs,local_pfreqs,pos,lengths\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[4] for item in batch],dim=0)\n",
    "    lpfreqs = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[6] for item in batch],dim=0)\n",
    "    length = torch.cat([item[7] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID,pfreqs,lpfreqs, pos, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb41b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.reshape((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,seq_len=30,clf_dims=[256,64],cont_size=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        blocks = []\n",
    "        for n_h, h_d,w,d in zip(num_heads,head_dims,wide_factors,drops):\n",
    "            blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.clf = Classifier_Head(in_features,clf_dims,out_size=out_size,seq_len=seq_len)\n",
    "        \n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "    \n",
    "    def to_input(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1)\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "\n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        X = torch.cat([seq1,seq2],dim=1)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        lpf = F.pad(lpfreqs,pad=pad, mode='constant', value=0)\n",
    "\n",
    "        X_input = torch.cat([X,pf,lpf,pid],dim=1)\n",
    "        \n",
    "        return X_input\n",
    "    \n",
    "    def forward(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_input = self.to_input(x,PID,pfreqs,lpfreqs,pos,length)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        out = self.clf(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N=10):\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos, length in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                lPID = lPID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True):\n",
    "    \n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    LossMut = nn.BCELoss(reduction='sum')\n",
    "    EvalLoss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            lpfreqs = lpfreqs.to(device)\n",
    "            length = length.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "            \n",
    "\n",
    "            if use_mut:\n",
    "                X_idx = torch.argmax(X[:,6],dim=1)\n",
    "                y_true = (X_idx == y).float().unsqueeze(1)  #0 if a mutation happens else 1 \n",
    "                y_pred = F.softmax(y_hat,dim=1)\n",
    "                y_pred = y_pred.gather(1,X_idx.view(-1,1)) #the Xth component of y_hat should be predicting ^\n",
    "                l = (Loss(y_hat,y) + LossMut(y_pred,y_true))/311\n",
    "            else:\n",
    "                l = Loss(y_hat,y)/311\n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b97a85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AttNet                                        [1, 20]                   400\n",
       "├─Sequential: 1-1                             [1, 32]                   --\n",
       "│    └─Linear: 2-1                            [1, 32]                   64\n",
       "│    └─Sigmoid: 2-2                           [1, 32]                   --\n",
       "├─Sequential: 1-2                             [1, 30, 32]               --\n",
       "│    └─Block: 2-3                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-1                     [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-2                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-3                    [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-4                   [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-5                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-6                    [1, 30, 32]               64\n",
       "│    └─Block: 2-4                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-7                     [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-8                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-9                    [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-10                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-11                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-12                   [1, 30, 32]               64\n",
       "│    └─Block: 2-5                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-13                    [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-14                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-15                   [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-16                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-17                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-18                   [1, 30, 32]               64\n",
       "│    └─Block: 2-6                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-19                    [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-20                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-21                   [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-22                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-23                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-24                   [1, 30, 32]               64\n",
       "├─Classifier_Head: 1-3                        [1, 20]                   --\n",
       "│    └─Sequential: 2-7                        [1, 20]                   --\n",
       "│    │    └─Linear: 3-25                      [1, 1024]                 984,064\n",
       "│    │    └─GELU: 3-26                        [1, 1024]                 --\n",
       "│    │    └─Dropout: 3-27                     [1, 1024]                 --\n",
       "│    │    └─Linear: 3-28                      [1, 256]                  262,400\n",
       "│    │    └─GELU: 3-29                        [1, 256]                  --\n",
       "│    │    └─Dropout: 3-30                     [1, 256]                  --\n",
       "│    │    └─Linear: 3-31                      [1, 64]                   16,448\n",
       "│    │    └─GELU: 3-32                        [1, 64]                   --\n",
       "│    │    └─Dropout: 3-33                     [1, 64]                   --\n",
       "│    │    └─Linear: 3-34                      [1, 20]                   1,300\n",
       "===============================================================================================\n",
       "Total params: 2,482,468\n",
       "Trainable params: 2,482,468\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.43\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.99\n",
       "Params size (MB): 5.72\n",
       "Estimated Total Size (MB): 6.71\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,4,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model,[(1,25,21),(1,),(1,20),(1,2,20),(1,25),(1,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.7820186614990234, val_loss = 0.8786128163337708\n",
      "epoch n°1 : train_loss = 2.496732711791992, val_loss = 0.8379143476486206\n",
      "epoch n°2 : train_loss = 2.396580696105957, val_loss = 0.8233270645141602\n",
      "epoch n°3 : train_loss = 2.3632419109344482, val_loss = 0.8242983222007751\n",
      "epoch n°4 : train_loss = 2.341736316680908, val_loss = 0.8151569366455078\n",
      "epoch n°5 : train_loss = 2.330084800720215, val_loss = 0.8113887906074524\n",
      "epoch n°6 : train_loss = 2.3223018646240234, val_loss = 0.8084423542022705\n",
      "epoch n°7 : train_loss = 2.3109476566314697, val_loss = 0.8103057146072388\n",
      "epoch n°8 : train_loss = 2.3055741786956787, val_loss = 0.8092596530914307\n",
      "epoch n°9 : train_loss = 2.294175386428833, val_loss = 0.8076428771018982\n",
      "epoch n°10 : train_loss = 2.294893264770508, val_loss = 0.8058606386184692\n",
      "epoch n°11 : train_loss = 2.2987349033355713, val_loss = 0.8114710450172424\n",
      "epoch n°12 : train_loss = 2.287438154220581, val_loss = 0.8090648651123047\n",
      "epoch n°13 : train_loss = 2.2890067100524902, val_loss = 0.8057113885879517\n",
      "epoch n°14 : train_loss = 2.2865777015686035, val_loss = 0.8054680228233337\n",
      "epoch n°15 : train_loss = 2.2886369228363037, val_loss = 0.807421863079071\n",
      "epoch n°16 : train_loss = 2.304713487625122, val_loss = 0.8083859086036682\n",
      "epoch n°17 : train_loss = 2.2970950603485107, val_loss = 0.8084814548492432\n",
      "epoch n°18 : train_loss = 2.3019015789031982, val_loss = 0.8106260895729065\n",
      "epoch n°19 : train_loss = 2.2916510105133057, val_loss = 0.8082827925682068\n",
      "epoch n°20 : train_loss = 2.2929859161376953, val_loss = 0.8109200596809387\n",
      "epoch n°21 : train_loss = 2.29376482963562, val_loss = 0.8068051338195801\n",
      "epoch n°22 : train_loss = 2.293822765350342, val_loss = 0.8103627562522888\n",
      "epoch n°23 : train_loss = 2.2831318378448486, val_loss = 0.8072342872619629\n",
      "epoch n°24 : train_loss = 2.2773091793060303, val_loss = 0.8060383200645447\n",
      "epoch n°25 : train_loss = 2.2620723247528076, val_loss = 0.8086826205253601\n",
      "epoch n°26 : train_loss = 2.265523910522461, val_loss = 0.8060013651847839\n",
      "epoch n°27 : train_loss = 2.2711832523345947, val_loss = 0.8087517023086548\n",
      "epoch n°28 : train_loss = 2.267697334289551, val_loss = 0.8027096390724182\n",
      "epoch n°29 : train_loss = 2.2636404037475586, val_loss = 0.8052103519439697\n",
      "epoch n°30 : train_loss = 2.264707326889038, val_loss = 0.8061383366584778\n",
      "epoch n°31 : train_loss = 2.2554879188537598, val_loss = 0.8041936755180359\n",
      "epoch n°32 : train_loss = 2.2564704418182373, val_loss = 0.8026474118232727\n",
      "epoch n°33 : train_loss = 2.2598936557769775, val_loss = 0.7999715209007263\n",
      "epoch n°34 : train_loss = 2.246602773666382, val_loss = 0.8046768307685852\n",
      "epoch n°35 : train_loss = 2.246410608291626, val_loss = 0.8042854070663452\n",
      "epoch n°36 : train_loss = 2.250720500946045, val_loss = 0.8025525212287903\n",
      "epoch n°37 : train_loss = 2.2509729862213135, val_loss = 0.8055615425109863\n",
      "epoch n°38 : train_loss = 2.2427690029144287, val_loss = 0.8016205430030823\n",
      "epoch n°39 : train_loss = 2.2415859699249268, val_loss = 0.8005189895629883\n",
      "epoch n°40 : train_loss = 2.2349724769592285, val_loss = 0.7997212409973145\n",
      "epoch n°41 : train_loss = 2.233208179473877, val_loss = 0.798565685749054\n",
      "epoch n°42 : train_loss = 2.2413558959960938, val_loss = 0.799863874912262\n",
      "epoch n°43 : train_loss = 2.236600399017334, val_loss = 0.7992711067199707\n",
      "epoch n°44 : train_loss = 2.2444963455200195, val_loss = 0.8021773099899292\n",
      "epoch n°45 : train_loss = 2.241486072540283, val_loss = 0.8004319667816162\n",
      "epoch n°46 : train_loss = 2.231337785720825, val_loss = 0.8047751188278198\n",
      "epoch n°47 : train_loss = 2.236182689666748, val_loss = 0.796010434627533\n",
      "epoch n°48 : train_loss = 2.2554314136505127, val_loss = 0.8031015396118164\n",
      "epoch n°49 : train_loss = 2.260010004043579, val_loss = 0.8071349263191223\n",
      "epoch n°50 : train_loss = 2.2553727626800537, val_loss = 0.8091174364089966\n",
      "epoch n°51 : train_loss = 2.252185821533203, val_loss = 0.8043889403343201\n",
      "epoch n°52 : train_loss = 2.252424955368042, val_loss = 0.8030458688735962\n",
      "epoch n°53 : train_loss = 2.25331711769104, val_loss = 0.8066935539245605\n",
      "epoch n°54 : train_loss = 2.2457432746887207, val_loss = 0.802832305431366\n",
      "epoch n°55 : train_loss = 2.247020721435547, val_loss = 0.8029540777206421\n",
      "epoch n°56 : train_loss = 2.25189471244812, val_loss = 0.8042129278182983\n",
      "epoch n°57 : train_loss = 2.24959659576416, val_loss = 0.8062143921852112\n",
      "epoch n°58 : train_loss = 2.248887062072754, val_loss = 0.8038829565048218\n",
      "epoch n°59 : train_loss = 2.2469263076782227, val_loss = 0.8022701144218445\n",
      "epoch n°60 : train_loss = 2.248150110244751, val_loss = 0.8005268573760986\n",
      "epoch n°61 : train_loss = 2.2415902614593506, val_loss = 0.8050522804260254\n",
      "epoch n°62 : train_loss = 2.243565320968628, val_loss = 0.808303952217102\n",
      "epoch n°63 : train_loss = 2.2459301948547363, val_loss = 0.8021039366722107\n",
      "epoch n°64 : train_loss = 2.235872507095337, val_loss = 0.8050668835639954\n",
      "epoch n°65 : train_loss = 2.250631332397461, val_loss = 0.805242657661438\n",
      "epoch n°66 : train_loss = 2.2388803958892822, val_loss = 0.8043875098228455\n",
      "epoch n°67 : train_loss = 2.2361223697662354, val_loss = 0.7993740439414978\n",
      "epoch n°68 : train_loss = 2.242297887802124, val_loss = 0.8048586845397949\n",
      "epoch n°69 : train_loss = 2.2455968856811523, val_loss = 0.8016293048858643\n",
      "epoch n°70 : train_loss = 2.235499143600464, val_loss = 0.8023629784584045\n",
      "epoch n°71 : train_loss = 2.234875440597534, val_loss = 0.8054166436195374\n",
      "epoch n°72 : train_loss = 2.2366843223571777, val_loss = 0.7999786734580994\n",
      "epoch n°73 : train_loss = 2.2316391468048096, val_loss = 0.8045075535774231\n",
      "epoch n°74 : train_loss = 2.2303919792175293, val_loss = 0.803093433380127\n",
      "epoch n°75 : train_loss = 2.2243809700012207, val_loss = 0.7998178005218506\n",
      "epoch n°76 : train_loss = 2.226383924484253, val_loss = 0.8004491329193115\n",
      "epoch n°77 : train_loss = 2.228398323059082, val_loss = 0.8020570278167725\n",
      "epoch n°78 : train_loss = 2.2290070056915283, val_loss = 0.7979871034622192\n",
      "epoch n°79 : train_loss = 2.2109646797180176, val_loss = 0.7983834743499756\n",
      "epoch n°80 : train_loss = 2.224893093109131, val_loss = 0.8008556365966797\n",
      "epoch n°81 : train_loss = 2.2217020988464355, val_loss = 0.7986378073692322\n",
      "epoch n°82 : train_loss = 2.2113287448883057, val_loss = 0.7990488409996033\n",
      "epoch n°83 : train_loss = 2.2132887840270996, val_loss = 0.8003219962120056\n",
      "epoch n°84 : train_loss = 2.2202911376953125, val_loss = 0.8026090264320374\n",
      "epoch n°85 : train_loss = 2.211232900619507, val_loss = 0.7990873456001282\n",
      "epoch n°86 : train_loss = 2.2118515968322754, val_loss = 0.7948499917984009\n",
      "epoch n°87 : train_loss = 2.2214198112487793, val_loss = 0.798619270324707\n",
      "epoch n°88 : train_loss = 2.204127073287964, val_loss = 0.7949109077453613\n",
      "epoch n°89 : train_loss = 2.2112951278686523, val_loss = 0.7992398142814636\n",
      "epoch n°90 : train_loss = 2.2116174697875977, val_loss = 0.7988238334655762\n",
      "epoch n°91 : train_loss = 2.212040424346924, val_loss = 0.8025785088539124\n",
      "epoch n°92 : train_loss = 2.2096211910247803, val_loss = 0.7983917593955994\n",
      "epoch n°93 : train_loss = 2.210787534713745, val_loss = 0.7956095933914185\n",
      "epoch n°94 : train_loss = 2.205068588256836, val_loss = 0.7987642288208008\n",
      "epoch n°95 : train_loss = 2.2152514457702637, val_loss = 0.8031745553016663\n",
      "epoch n°96 : train_loss = 2.2095277309417725, val_loss = 0.7970138788223267\n",
      "epoch n°97 : train_loss = 2.2037594318389893, val_loss = 0.7989039421081543\n",
      "epoch n°98 : train_loss = 2.2030110359191895, val_loss = 0.7987307906150818\n",
      "epoch n°99 : train_loss = 2.198439598083496, val_loss = 0.799551248550415\n",
      "epoch n°100 : train_loss = 2.2062392234802246, val_loss = 0.7954710125923157\n",
      "epoch n°101 : train_loss = 2.205656051635742, val_loss = 0.7998608350753784\n",
      "epoch n°102 : train_loss = 2.20867919921875, val_loss = 0.7980374693870544\n",
      "epoch n°103 : train_loss = 2.1986050605773926, val_loss = 0.7945769429206848\n",
      "epoch n°104 : train_loss = 2.2034049034118652, val_loss = 0.7925390601158142\n",
      "epoch n°105 : train_loss = 2.2038114070892334, val_loss = 0.7992403507232666\n",
      "epoch n°106 : train_loss = 2.1963627338409424, val_loss = 0.794529139995575\n",
      "epoch n°107 : train_loss = 2.1978583335876465, val_loss = 0.7980973124504089\n",
      "epoch n°108 : train_loss = 2.19978928565979, val_loss = 0.8024671077728271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 2.194643020629883, val_loss = 0.7960415482521057\n",
      "epoch n°110 : train_loss = 2.2085750102996826, val_loss = 0.7987204790115356\n",
      "epoch n°111 : train_loss = 2.1956968307495117, val_loss = 0.795861005783081\n",
      "epoch n°112 : train_loss = 2.2203705310821533, val_loss = 0.7983351945877075\n",
      "epoch n°113 : train_loss = 2.2149548530578613, val_loss = 0.8011788129806519\n",
      "epoch n°114 : train_loss = 2.2284748554229736, val_loss = 0.8026575446128845\n",
      "epoch n°115 : train_loss = 2.2205591201782227, val_loss = 0.8040472269058228\n",
      "epoch n°116 : train_loss = 2.224886178970337, val_loss = 0.799980103969574\n",
      "epoch n°117 : train_loss = 2.2168514728546143, val_loss = 0.8045084476470947\n",
      "epoch n°118 : train_loss = 2.215970039367676, val_loss = 0.800125002861023\n",
      "epoch n°119 : train_loss = 2.2299087047576904, val_loss = 0.7986940741539001\n",
      "epoch n°120 : train_loss = 2.2166528701782227, val_loss = 0.7978545427322388\n",
      "epoch n°121 : train_loss = 2.2222299575805664, val_loss = 0.8009495735168457\n",
      "epoch n°122 : train_loss = 2.2261953353881836, val_loss = 0.8001868724822998\n",
      "epoch n°123 : train_loss = 2.2259390354156494, val_loss = 0.8040609955787659\n",
      "epoch n°124 : train_loss = 2.21658992767334, val_loss = 0.8010129928588867\n",
      "epoch n°125 : train_loss = 2.2199933528900146, val_loss = 0.8001072406768799\n",
      "epoch n°126 : train_loss = 2.217308759689331, val_loss = 0.7962778806686401\n",
      "epoch n°127 : train_loss = 2.222115993499756, val_loss = 0.8010148406028748\n",
      "epoch n°128 : train_loss = 2.2242279052734375, val_loss = 0.8022750616073608\n",
      "epoch n°129 : train_loss = 2.2281384468078613, val_loss = 0.8009287714958191\n",
      "epoch n°130 : train_loss = 2.2196314334869385, val_loss = 0.8004595041275024\n",
      "epoch n°131 : train_loss = 2.2106382846832275, val_loss = 0.7967298030853271\n",
      "epoch n°132 : train_loss = 2.2187747955322266, val_loss = 0.8017026782035828\n",
      "epoch n°133 : train_loss = 2.2183241844177246, val_loss = 0.8021702766418457\n",
      "epoch n°134 : train_loss = 2.217848777770996, val_loss = 0.7994517683982849\n",
      "epoch n°135 : train_loss = 2.2077178955078125, val_loss = 0.7992181181907654\n",
      "epoch n°136 : train_loss = 2.2142317295074463, val_loss = 0.7974523901939392\n",
      "epoch n°137 : train_loss = 2.215043544769287, val_loss = 0.7971364855766296\n",
      "epoch n°138 : train_loss = 2.201972246170044, val_loss = 0.7995455265045166\n",
      "epoch n°139 : train_loss = 2.209148406982422, val_loss = 0.8001552820205688\n",
      "epoch n°140 : train_loss = 2.208500623703003, val_loss = 0.7968130707740784\n",
      "epoch n°141 : train_loss = 2.204355478286743, val_loss = 0.799710214138031\n",
      "epoch n°142 : train_loss = 2.207681655883789, val_loss = 0.8038397431373596\n",
      "epoch n°143 : train_loss = 2.2159974575042725, val_loss = 0.7999386787414551\n",
      "epoch n°144 : train_loss = 2.2028045654296875, val_loss = 0.7976064085960388\n",
      "epoch n°145 : train_loss = 2.214820384979248, val_loss = 0.8012231588363647\n",
      "epoch n°146 : train_loss = 2.206725835800171, val_loss = 0.7963830828666687\n",
      "epoch n°147 : train_loss = 2.209954023361206, val_loss = 0.7967730164527893\n",
      "epoch n°148 : train_loss = 2.2028563022613525, val_loss = 0.8027286529541016\n",
      "epoch n°149 : train_loss = 2.210707902908325, val_loss = 0.7970153093338013\n",
      "epoch n°150 : train_loss = 2.206362724304199, val_loss = 0.8026981353759766\n",
      "epoch n°151 : train_loss = 2.2006142139434814, val_loss = 0.7982950806617737\n",
      "epoch n°152 : train_loss = 2.1947848796844482, val_loss = 0.8044912219047546\n",
      "epoch n°153 : train_loss = 2.2022042274475098, val_loss = 0.8001561164855957\n",
      "epoch n°154 : train_loss = 2.1979708671569824, val_loss = 0.8017065525054932\n",
      "epoch n°155 : train_loss = 2.208386182785034, val_loss = 0.7991758584976196\n",
      "epoch n°156 : train_loss = 2.1972203254699707, val_loss = 0.8013902902603149\n",
      "epoch n°157 : train_loss = 2.2012674808502197, val_loss = 0.7975757718086243\n",
      "epoch n°158 : train_loss = 2.204620838165283, val_loss = 0.7951323986053467\n",
      "epoch n°159 : train_loss = 2.1985201835632324, val_loss = 0.797404944896698\n",
      "epoch n°160 : train_loss = 2.1922359466552734, val_loss = 0.7963797450065613\n",
      "epoch n°161 : train_loss = 2.197801113128662, val_loss = 0.7991794347763062\n",
      "epoch n°162 : train_loss = 2.1889867782592773, val_loss = 0.7991619110107422\n",
      "epoch n°163 : train_loss = 2.1910758018493652, val_loss = 0.7979001402854919\n",
      "epoch n°164 : train_loss = 2.1906423568725586, val_loss = 0.7974624633789062\n",
      "epoch n°165 : train_loss = 2.202679395675659, val_loss = 0.7996089458465576\n",
      "epoch n°166 : train_loss = 2.190554618835449, val_loss = 0.7973535060882568\n",
      "epoch n°167 : train_loss = 2.1880502700805664, val_loss = 0.7983593940734863\n",
      "epoch n°168 : train_loss = 2.1861746311187744, val_loss = 0.8008278012275696\n",
      "epoch n°169 : train_loss = 2.2019851207733154, val_loss = 0.7966735363006592\n",
      "epoch n°170 : train_loss = 2.1908767223358154, val_loss = 0.7983477115631104\n",
      "epoch n°171 : train_loss = 2.1942999362945557, val_loss = 0.8010423183441162\n",
      "epoch n°172 : train_loss = 2.190308094024658, val_loss = 0.7976933717727661\n",
      "epoch n°173 : train_loss = 2.189570665359497, val_loss = 0.7956530451774597\n",
      "epoch n°174 : train_loss = 2.191068410873413, val_loss = 0.7965525388717651\n",
      "epoch n°175 : train_loss = 2.1899001598358154, val_loss = 0.7944663763046265\n",
      "epoch n°176 : train_loss = 2.186251163482666, val_loss = 0.7952037453651428\n",
      "epoch n°177 : train_loss = 2.195688486099243, val_loss = 0.7973583936691284\n",
      "epoch n°178 : train_loss = 2.1919384002685547, val_loss = 0.7976928353309631\n",
      "epoch n°179 : train_loss = 2.183238983154297, val_loss = 0.7960276007652283\n",
      "epoch n°180 : train_loss = 2.1813831329345703, val_loss = 0.7974488735198975\n",
      "epoch n°181 : train_loss = 2.1853933334350586, val_loss = 0.7964222431182861\n",
      "epoch n°182 : train_loss = 2.178558826446533, val_loss = 0.7969908714294434\n",
      "epoch n°183 : train_loss = 2.1794509887695312, val_loss = 0.8027172088623047\n",
      "epoch n°184 : train_loss = 2.1912472248077393, val_loss = 0.7970194220542908\n",
      "epoch n°185 : train_loss = 2.1721184253692627, val_loss = 0.7992632985115051\n",
      "epoch n°186 : train_loss = 2.1890921592712402, val_loss = 0.796417236328125\n",
      "epoch n°187 : train_loss = 2.185647487640381, val_loss = 0.7971518635749817\n",
      "epoch n°188 : train_loss = 2.182147979736328, val_loss = 0.8004534244537354\n",
      "epoch n°189 : train_loss = 2.1811399459838867, val_loss = 0.7963579893112183\n",
      "epoch n°190 : train_loss = 2.171948194503784, val_loss = 0.7982909679412842\n",
      "epoch n°191 : train_loss = 2.1749625205993652, val_loss = 0.7942078113555908\n",
      "epoch n°192 : train_loss = 2.180299758911133, val_loss = 0.7930662035942078\n",
      "epoch n°193 : train_loss = 2.1791892051696777, val_loss = 0.7962667346000671\n",
      "epoch n°194 : train_loss = 2.1722874641418457, val_loss = 0.7961652874946594\n",
      "epoch n°195 : train_loss = 2.1833877563476562, val_loss = 0.7969894409179688\n",
      "epoch n°196 : train_loss = 2.180492639541626, val_loss = 0.794666588306427\n",
      "epoch n°197 : train_loss = 2.1726419925689697, val_loss = 0.7971888184547424\n",
      "epoch n°198 : train_loss = 2.181316375732422, val_loss = 0.794447660446167\n",
      "epoch n°199 : train_loss = 2.181457281112671, val_loss = 0.7970656752586365\n",
      "epoch n°200 : train_loss = 2.174679756164551, val_loss = 0.7998536229133606\n",
      "epoch n°201 : train_loss = 2.173893928527832, val_loss = 0.7942938208580017\n",
      "epoch n°202 : train_loss = 2.172530174255371, val_loss = 0.7968101501464844\n",
      "epoch n°203 : train_loss = 2.1847760677337646, val_loss = 0.8020145297050476\n",
      "epoch n°204 : train_loss = 2.1686651706695557, val_loss = 0.800853967666626\n",
      "epoch n°205 : train_loss = 2.1706700325012207, val_loss = 0.7966259717941284\n",
      "epoch n°206 : train_loss = 2.1694483757019043, val_loss = 0.7945753931999207\n",
      "epoch n°207 : train_loss = 2.177309036254883, val_loss = 0.7985960841178894\n",
      "epoch n°208 : train_loss = 2.163949489593506, val_loss = 0.7956603169441223\n",
      "epoch n°209 : train_loss = 2.1687569618225098, val_loss = 0.7996333837509155\n",
      "epoch n°210 : train_loss = 2.1721351146698, val_loss = 0.7967490553855896\n",
      "epoch n°211 : train_loss = 2.174590826034546, val_loss = 0.7927212119102478\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,4,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_4L_fixed.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_4L_fixed.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_4L_fixed_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e0e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = get_params(32,5,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_5L_fixed.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f15a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_5L_fixed.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_5L_fixed_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_6L_fixed.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209065c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_fixed.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_fixed_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55973a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6829f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77cf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
