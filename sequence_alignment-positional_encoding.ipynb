{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=1400000,verbose=False):\n",
    "        \n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto'): \n",
    "        #for each sample:\n",
    "        #sample i, j st i != j the two sequences to compare\n",
    "        #sample k the sequence position of the prediction\n",
    "        #compute the file positions of the 25 + 1 AA\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        precomputed_pos = []\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        if type(sample_size) != int:\n",
    "            coef = round((n**2 * l)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1):\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_i != '-' and aa_j != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = []\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "                    \n",
    "            try:   \n",
    "                k = np.random.choice(possible_k)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            pos_ij = (k + precomputed_pos)/align_l\n",
    "            pos.append(pos_ij)\n",
    "            \n",
    "            window_i = ''\n",
    "            window_j = ''\n",
    "            \n",
    "            for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                if w < 0 or w >= align_l: #case of the edges\n",
    "                    window_i += 'Z'\n",
    "                    window_j += 'Z'\n",
    "                else:\n",
    "                    window_i += seq_i[w]\n",
    "                    window_j += seq_j[w]\n",
    "        \n",
    "            y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "            X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "            \n",
    "            X.append(X_i)\n",
    "            y.append(y_j)\n",
    "            PIDs.append(PID_ij)\n",
    "            local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                         + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "            \n",
    "            loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                         + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "            try:\n",
    "                tmp = local_PID_ij/loc_comp\n",
    "            except:\n",
    "                tmp = 0\n",
    "                \n",
    "            local_PIDs.append(tmp)\n",
    "            pfreqs.append(self.aa_freqs[idx])\n",
    "            p_i_freqs = self.p_aa_freqs[idx][i]\n",
    "            p_j_freqs = self.p_aa_freqs[idx][j]\n",
    "            \n",
    "            local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "            \n",
    "            assert y_j < 20\n",
    "            assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache()   \n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        out = X.float(),torch.tensor(y).long(),torch.tensor(PIDs),torch.tensor(local_PIDs),pfreqs,local_pfreqs,pos\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[4] for item in batch],dim=0)\n",
    "    lpfreqs = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[6] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID,pfreqs,lpfreqs, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb41b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.reshape((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,seq_len=30,clf_dims=[256,64],cont_size=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        blocks = []\n",
    "        for n_h, h_d,w,d in zip(num_heads,head_dims,wide_factors,drops):\n",
    "            blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.clf = Classifier_Head(in_features,clf_dims,out_size=out_size,seq_len=seq_len)\n",
    "        \n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "    \n",
    "    def to_input(self,x,PID,pfreqs,lpfreqs,pos):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "\n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        X = torch.cat([seq1,seq2],dim=1)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        lpf = F.pad(lpfreqs,pad=pad, mode='constant', value=0)\n",
    "\n",
    "        X_input = torch.cat([X,pf,lpf,pid],dim=1)\n",
    "        \n",
    "        return X_input\n",
    "    \n",
    "    def forward(self,x,PID,pfreqs,lpfreqs,pos):\n",
    "        X_input = self.to_input(x,PID,pfreqs,lpfreqs,pos)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        out = self.clf(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N=10):\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                lPID = lPID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True):\n",
    "    \n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    LossMut = nn.BCELoss(reduction='sum')\n",
    "    EvalLoss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pfreqs,lpfreqs,pos in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            lpfreqs = lpfreqs.to(device)\n",
    "            X_idx = torch.argmax(X[:,6],dim=1)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pfreqs,lpfreqs,pos)\n",
    "            y_true = (X_idx == y).float().unsqueeze(1)  #0 if a mutation happens else 1 \n",
    "            y_pred = F.softmax(y_hat,dim=1)\n",
    "            y_pred = y_pred.gather(1,X_idx.view(-1,1)) #the Xth component of y_hat should be predicting ^\n",
    "\n",
    "            if use_mut:\n",
    "                l = (Loss(y_hat,y) + LossMut(y_pred,y_true))/311\n",
    "            else:\n",
    "                l = Loss(y_hat,y)/311\n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "794c385e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AttNet                                        [1, 20]                   400\n",
       "├─Sequential: 1-1                             [1, 32]                   --\n",
       "│    └─Linear: 2-1                            [1, 32]                   64\n",
       "│    └─Sigmoid: 2-2                           [1, 32]                   --\n",
       "├─Sequential: 1-2                             [1, 30, 32]               --\n",
       "│    └─Block: 2-3                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-1                     [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-2                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-3                    [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-4                   [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-5                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-6                    [1, 30, 32]               64\n",
       "│    └─Block: 2-4                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-7                     [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-8                     [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-9                    [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-10                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-11                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-12                   [1, 30, 32]               64\n",
       "│    └─Block: 2-5                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-13                    [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-14                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-15                   [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-16                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-17                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-18                   [1, 30, 32]               64\n",
       "│    └─Block: 2-6                             [1, 30, 32]               --\n",
       "│    │    └─AttBlock: 3-19                    [1, 30, 32]               295,968\n",
       "│    │    └─DropPath: 3-20                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-21                   [1, 30, 32]               64\n",
       "│    │    └─FeedFoward: 3-22                  [1, 30, 32]               8,352\n",
       "│    │    └─DropPath: 3-23                    [1, 30, 32]               --\n",
       "│    │    └─LayerNorm: 3-24                   [1, 30, 32]               64\n",
       "├─Classifier_Head: 1-3                        [1, 20]                   --\n",
       "│    └─Sequential: 2-7                        [1, 20]                   --\n",
       "│    │    └─Linear: 3-25                      [1, 1024]                 984,064\n",
       "│    │    └─GELU: 3-26                        [1, 1024]                 --\n",
       "│    │    └─Dropout: 3-27                     [1, 1024]                 --\n",
       "│    │    └─Linear: 3-28                      [1, 256]                  262,400\n",
       "│    │    └─GELU: 3-29                        [1, 256]                  --\n",
       "│    │    └─Dropout: 3-30                     [1, 256]                  --\n",
       "│    │    └─Linear: 3-31                      [1, 64]                   16,448\n",
       "│    │    └─GELU: 3-32                        [1, 64]                   --\n",
       "│    │    └─Dropout: 3-33                     [1, 64]                   --\n",
       "│    │    └─Linear: 3-34                      [1, 20]                   1,300\n",
       "===============================================================================================\n",
       "Total params: 2,482,468\n",
       "Trainable params: 2,482,468\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.43\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.99\n",
       "Params size (MB): 5.72\n",
       "Estimated Total Size (MB): 6.71\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,4,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model,[(1,25,21),(1,),(1,20),(1,2,20),(1,25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.7526731491088867, val_loss = 0.8555530905723572\n",
      "epoch n°1 : train_loss = 2.4633779525756836, val_loss = 0.8299611210823059\n",
      "epoch n°2 : train_loss = 2.392967939376831, val_loss = 0.8254174590110779\n",
      "epoch n°3 : train_loss = 2.3815677165985107, val_loss = 0.8267021179199219\n",
      "epoch n°4 : train_loss = 2.354008674621582, val_loss = 0.8183674216270447\n",
      "epoch n°5 : train_loss = 2.340519666671753, val_loss = 0.8137349486351013\n",
      "epoch n°6 : train_loss = 2.3231418132781982, val_loss = 0.8129883408546448\n",
      "epoch n°7 : train_loss = 2.3210830688476562, val_loss = 0.8112228512763977\n",
      "epoch n°8 : train_loss = 2.3135526180267334, val_loss = 0.814944863319397\n",
      "epoch n°9 : train_loss = 2.308492422103882, val_loss = 0.8133566379547119\n",
      "epoch n°10 : train_loss = 2.3069562911987305, val_loss = 0.8099209070205688\n",
      "epoch n°11 : train_loss = 2.2959043979644775, val_loss = 0.8058611154556274\n",
      "epoch n°12 : train_loss = 2.298063039779663, val_loss = 0.8016940951347351\n",
      "epoch n°13 : train_loss = 2.290904998779297, val_loss = 0.805152416229248\n",
      "epoch n°14 : train_loss = 2.2922985553741455, val_loss = 0.8088434934616089\n",
      "epoch n°15 : train_loss = 2.281280279159546, val_loss = 0.8079959750175476\n",
      "epoch n°16 : train_loss = 2.2998032569885254, val_loss = 0.8087665438652039\n",
      "epoch n°17 : train_loss = 2.305220127105713, val_loss = 0.810353696346283\n",
      "epoch n°18 : train_loss = 2.299053192138672, val_loss = 0.8106412291526794\n",
      "epoch n°19 : train_loss = 2.285470485687256, val_loss = 0.8121878504753113\n",
      "epoch n°20 : train_loss = 2.292931079864502, val_loss = 0.8099760413169861\n",
      "epoch n°21 : train_loss = 2.2955498695373535, val_loss = 0.8091307282447815\n",
      "epoch n°22 : train_loss = 2.2929553985595703, val_loss = 0.8085408210754395\n",
      "epoch n°23 : train_loss = 2.2743983268737793, val_loss = 0.8162325024604797\n",
      "epoch n°24 : train_loss = 2.2782084941864014, val_loss = 0.8086501359939575\n",
      "epoch n°25 : train_loss = 2.2759811878204346, val_loss = 0.8041095733642578\n",
      "epoch n°26 : train_loss = 2.265179395675659, val_loss = 0.8068150877952576\n",
      "epoch n°27 : train_loss = 2.2617733478546143, val_loss = 0.8014929890632629\n",
      "epoch n°28 : train_loss = 2.2641549110412598, val_loss = 0.8043931126594543\n",
      "epoch n°29 : train_loss = 2.2654850482940674, val_loss = 0.8045690655708313\n",
      "epoch n°30 : train_loss = 2.262833833694458, val_loss = 0.80521160364151\n",
      "epoch n°31 : train_loss = 2.2646384239196777, val_loss = 0.8054623007774353\n",
      "epoch n°32 : train_loss = 2.2601447105407715, val_loss = 0.7994859218597412\n",
      "epoch n°33 : train_loss = 2.2465386390686035, val_loss = 0.7999362349510193\n",
      "epoch n°34 : train_loss = 2.259585380554199, val_loss = 0.8024024963378906\n",
      "epoch n°35 : train_loss = 2.243818759918213, val_loss = 0.8033786416053772\n",
      "epoch n°36 : train_loss = 2.2482471466064453, val_loss = 0.7990663647651672\n",
      "epoch n°37 : train_loss = 2.242974281311035, val_loss = 0.8024117350578308\n",
      "epoch n°38 : train_loss = 2.2349092960357666, val_loss = 0.8014545440673828\n",
      "epoch n°39 : train_loss = 2.241335153579712, val_loss = 0.7991170883178711\n",
      "epoch n°40 : train_loss = 2.2438313961029053, val_loss = 0.8009947538375854\n",
      "epoch n°41 : train_loss = 2.243072032928467, val_loss = 0.7979642152786255\n",
      "epoch n°42 : train_loss = 2.2378761768341064, val_loss = 0.798431932926178\n",
      "epoch n°43 : train_loss = 2.2412607669830322, val_loss = 0.797275185585022\n",
      "epoch n°44 : train_loss = 2.241692543029785, val_loss = 0.8000003695487976\n",
      "epoch n°45 : train_loss = 2.2396066188812256, val_loss = 0.7992863655090332\n",
      "epoch n°46 : train_loss = 2.235062837600708, val_loss = 0.799161970615387\n",
      "epoch n°47 : train_loss = 2.2366411685943604, val_loss = 0.795459508895874\n",
      "epoch n°48 : train_loss = 2.2550177574157715, val_loss = 0.8103844523429871\n",
      "epoch n°49 : train_loss = 2.2564449310302734, val_loss = 0.8022474050521851\n",
      "epoch n°50 : train_loss = 2.2610843181610107, val_loss = 0.8085155487060547\n",
      "epoch n°51 : train_loss = 2.257166862487793, val_loss = 0.8021203875541687\n",
      "epoch n°52 : train_loss = 2.2474827766418457, val_loss = 0.8067834973335266\n",
      "epoch n°53 : train_loss = 2.262566566467285, val_loss = 0.802346408367157\n",
      "epoch n°54 : train_loss = 2.253570318222046, val_loss = 0.8012716770172119\n",
      "epoch n°55 : train_loss = 2.2506277561187744, val_loss = 0.8026980757713318\n",
      "epoch n°56 : train_loss = 2.244847059249878, val_loss = 0.8034628033638\n",
      "epoch n°57 : train_loss = 2.254392147064209, val_loss = 0.8021346926689148\n",
      "epoch n°58 : train_loss = 2.2502691745758057, val_loss = 0.803679347038269\n",
      "epoch n°59 : train_loss = 2.2508840560913086, val_loss = 0.8014612197875977\n",
      "epoch n°60 : train_loss = 2.2474353313446045, val_loss = 0.8015256524085999\n",
      "epoch n°61 : train_loss = 2.2384698390960693, val_loss = 0.7992841601371765\n",
      "epoch n°62 : train_loss = 2.24289608001709, val_loss = 0.8012349009513855\n",
      "epoch n°63 : train_loss = 2.2470476627349854, val_loss = 0.7991334199905396\n",
      "epoch n°64 : train_loss = 2.237567663192749, val_loss = 0.7987527847290039\n",
      "epoch n°65 : train_loss = 2.237400531768799, val_loss = 0.8076136708259583\n",
      "epoch n°66 : train_loss = 2.2403805255889893, val_loss = 0.8016397953033447\n",
      "epoch n°67 : train_loss = 2.2369134426116943, val_loss = 0.8009904623031616\n",
      "epoch n°68 : train_loss = 2.2378244400024414, val_loss = 0.8029469847679138\n",
      "epoch n°69 : train_loss = 2.232042074203491, val_loss = 0.7990001440048218\n",
      "epoch n°70 : train_loss = 2.2295939922332764, val_loss = 0.7992565035820007\n",
      "epoch n°71 : train_loss = 2.226170063018799, val_loss = 0.7986935377120972\n",
      "epoch n°72 : train_loss = 2.231031894683838, val_loss = 0.7976086139678955\n",
      "epoch n°73 : train_loss = 2.227651834487915, val_loss = 0.8003266453742981\n",
      "epoch n°74 : train_loss = 2.22507643699646, val_loss = 0.7986553311347961\n",
      "epoch n°75 : train_loss = 2.22967267036438, val_loss = 0.8010433912277222\n",
      "epoch n°76 : train_loss = 2.218841314315796, val_loss = 0.7989183664321899\n",
      "epoch n°77 : train_loss = 2.2311172485351562, val_loss = 0.7995331883430481\n",
      "epoch n°78 : train_loss = 2.2200684547424316, val_loss = 0.7955055832862854\n",
      "epoch n°79 : train_loss = 2.218768835067749, val_loss = 0.8005192875862122\n",
      "epoch n°80 : train_loss = 2.21700382232666, val_loss = 0.7965344786643982\n",
      "epoch n°81 : train_loss = 2.2219951152801514, val_loss = 0.7985809445381165\n",
      "epoch n°82 : train_loss = 2.2205400466918945, val_loss = 0.8010574579238892\n",
      "epoch n°83 : train_loss = 2.212803840637207, val_loss = 0.7976163029670715\n",
      "epoch n°84 : train_loss = 2.21010422706604, val_loss = 0.8000858426094055\n",
      "epoch n°85 : train_loss = 2.213831663131714, val_loss = 0.7973248362541199\n",
      "epoch n°86 : train_loss = 2.203338384628296, val_loss = 0.7954027652740479\n",
      "epoch n°87 : train_loss = 2.2022526264190674, val_loss = 0.795664370059967\n",
      "epoch n°88 : train_loss = 2.2073781490325928, val_loss = 0.7983412146568298\n",
      "epoch n°89 : train_loss = 2.2079124450683594, val_loss = 0.7978342771530151\n",
      "epoch n°90 : train_loss = 2.2099008560180664, val_loss = 0.7949645519256592\n",
      "epoch n°91 : train_loss = 2.208911657333374, val_loss = 0.8003556728363037\n",
      "epoch n°92 : train_loss = 2.2088444232940674, val_loss = 0.8040790557861328\n",
      "epoch n°93 : train_loss = 2.2103099822998047, val_loss = 0.7954605221748352\n",
      "epoch n°94 : train_loss = 2.207824945449829, val_loss = 0.8011152148246765\n",
      "epoch n°95 : train_loss = 2.207585334777832, val_loss = 0.7987829446792603\n",
      "epoch n°96 : train_loss = 2.197535753250122, val_loss = 0.798536479473114\n",
      "epoch n°97 : train_loss = 2.204749345779419, val_loss = 0.7970018982887268\n",
      "epoch n°98 : train_loss = 2.207151412963867, val_loss = 0.7957236766815186\n",
      "epoch n°99 : train_loss = 2.204831600189209, val_loss = 0.7961917519569397\n",
      "epoch n°100 : train_loss = 2.2030398845672607, val_loss = 0.7959733009338379\n",
      "epoch n°101 : train_loss = 2.204833745956421, val_loss = 0.7932539582252502\n",
      "epoch n°102 : train_loss = 2.1946775913238525, val_loss = 0.7946186661720276\n",
      "epoch n°103 : train_loss = 2.1960396766662598, val_loss = 0.7948843836784363\n",
      "epoch n°104 : train_loss = 2.20127010345459, val_loss = 0.7966799139976501\n",
      "epoch n°105 : train_loss = 2.1971614360809326, val_loss = 0.7976881861686707\n",
      "epoch n°106 : train_loss = 2.1988446712493896, val_loss = 0.7953768968582153\n",
      "epoch n°107 : train_loss = 2.2014849185943604, val_loss = 0.8003882765769958\n",
      "epoch n°108 : train_loss = 2.199755907058716, val_loss = 0.7954079508781433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 2.2049288749694824, val_loss = 0.7974545955657959\n",
      "epoch n°110 : train_loss = 2.2015814781188965, val_loss = 0.7968394756317139\n",
      "epoch n°111 : train_loss = 2.207148551940918, val_loss = 0.7970711588859558\n",
      "epoch n°112 : train_loss = 2.2146387100219727, val_loss = 0.7971051931381226\n",
      "epoch n°113 : train_loss = 2.2138705253601074, val_loss = 0.7997382283210754\n",
      "epoch n°114 : train_loss = 2.2214601039886475, val_loss = 0.7964487075805664\n",
      "epoch n°115 : train_loss = 2.2278459072113037, val_loss = 0.8000537157058716\n",
      "epoch n°116 : train_loss = 2.2227394580841064, val_loss = 0.8015356063842773\n",
      "epoch n°117 : train_loss = 2.221604108810425, val_loss = 0.7987731099128723\n",
      "epoch n°118 : train_loss = 2.2170045375823975, val_loss = 0.8020251989364624\n",
      "epoch n°119 : train_loss = 2.221485137939453, val_loss = 0.7968968749046326\n",
      "epoch n°120 : train_loss = 2.2180426120758057, val_loss = 0.8020554184913635\n",
      "epoch n°121 : train_loss = 2.2243571281433105, val_loss = 0.8015269041061401\n",
      "epoch n°122 : train_loss = 2.2062606811523438, val_loss = 0.8031604886054993\n",
      "epoch n°123 : train_loss = 2.218376636505127, val_loss = 0.803686261177063\n",
      "epoch n°124 : train_loss = 2.220004081726074, val_loss = 0.7990726828575134\n",
      "epoch n°125 : train_loss = 2.2210240364074707, val_loss = 0.7987784743309021\n",
      "epoch n°126 : train_loss = 2.2167935371398926, val_loss = 0.800115704536438\n",
      "epoch n°127 : train_loss = 2.2186837196350098, val_loss = 0.7998020052909851\n",
      "epoch n°128 : train_loss = 2.2157773971557617, val_loss = 0.8023763298988342\n",
      "epoch n°129 : train_loss = 2.2201201915740967, val_loss = 0.8046939969062805\n",
      "epoch n°130 : train_loss = 2.2146222591400146, val_loss = 0.7962997555732727\n",
      "epoch n°131 : train_loss = 2.2079315185546875, val_loss = 0.7974384427070618\n",
      "epoch n°132 : train_loss = 2.217439889907837, val_loss = 0.800255298614502\n",
      "epoch n°133 : train_loss = 2.2129788398742676, val_loss = 0.7994307279586792\n",
      "epoch n°134 : train_loss = 2.215970754623413, val_loss = 0.7990997433662415\n",
      "epoch n°135 : train_loss = 2.2086565494537354, val_loss = 0.8003004789352417\n",
      "epoch n°136 : train_loss = 2.2234787940979004, val_loss = 0.7998338341712952\n",
      "epoch n°137 : train_loss = 2.2072227001190186, val_loss = 0.7975637912750244\n",
      "epoch n°138 : train_loss = 2.2068490982055664, val_loss = 0.7987689971923828\n",
      "epoch n°139 : train_loss = 2.20894455909729, val_loss = 0.7996749877929688\n",
      "epoch n°140 : train_loss = 2.2124388217926025, val_loss = 0.7958769202232361\n",
      "epoch n°141 : train_loss = 2.207758903503418, val_loss = 0.7970314025878906\n",
      "epoch n°142 : train_loss = 2.2079854011535645, val_loss = 0.7970215678215027\n",
      "epoch n°143 : train_loss = 2.213632822036743, val_loss = 0.8001994490623474\n",
      "epoch n°144 : train_loss = 2.207216501235962, val_loss = 0.8037223219871521\n",
      "epoch n°145 : train_loss = 2.2085447311401367, val_loss = 0.7983968257904053\n",
      "epoch n°146 : train_loss = 2.207524299621582, val_loss = 0.797468900680542\n",
      "epoch n°147 : train_loss = 2.2016634941101074, val_loss = 0.7973510026931763\n",
      "epoch n°148 : train_loss = 2.212451934814453, val_loss = 0.7998467683792114\n",
      "epoch n°149 : train_loss = 2.19415020942688, val_loss = 0.7989713549613953\n",
      "epoch n°150 : train_loss = 2.2037887573242188, val_loss = 0.7963248491287231\n",
      "epoch n°151 : train_loss = 2.2165441513061523, val_loss = 0.7974257469177246\n",
      "epoch n°152 : train_loss = 2.203944444656372, val_loss = 0.8008252382278442\n",
      "epoch n°153 : train_loss = 2.2037065029144287, val_loss = 0.798194944858551\n",
      "epoch n°154 : train_loss = 2.1920166015625, val_loss = 0.7967846393585205\n",
      "epoch n°155 : train_loss = 2.197072744369507, val_loss = 0.7987015843391418\n",
      "epoch n°156 : train_loss = 2.2034361362457275, val_loss = 0.7979316115379333\n",
      "epoch n°157 : train_loss = 2.1954383850097656, val_loss = 0.8013807535171509\n",
      "epoch n°158 : train_loss = 2.2023959159851074, val_loss = 0.7998523712158203\n",
      "epoch n°159 : train_loss = 2.193922758102417, val_loss = 0.7939144968986511\n",
      "epoch n°160 : train_loss = 2.19842267036438, val_loss = 0.7955448031425476\n",
      "epoch n°161 : train_loss = 2.1981875896453857, val_loss = 0.8005712032318115\n",
      "epoch n°162 : train_loss = 2.1842634677886963, val_loss = 0.7987251281738281\n",
      "epoch n°163 : train_loss = 2.1874024868011475, val_loss = 0.7978376746177673\n",
      "epoch n°164 : train_loss = 2.1945247650146484, val_loss = 0.7987944483757019\n",
      "epoch n°165 : train_loss = 2.192373752593994, val_loss = 0.7958373427391052\n",
      "epoch n°166 : train_loss = 2.1944189071655273, val_loss = 0.7968373894691467\n",
      "epoch n°167 : train_loss = 2.195298433303833, val_loss = 0.7982163429260254\n",
      "epoch n°168 : train_loss = 2.1948556900024414, val_loss = 0.794725239276886\n",
      "epoch n°169 : train_loss = 2.1895453929901123, val_loss = 0.8000080585479736\n",
      "epoch n°170 : train_loss = 2.195446014404297, val_loss = 0.7978864908218384\n",
      "epoch n°171 : train_loss = 2.190218687057495, val_loss = 0.795833170413971\n",
      "epoch n°172 : train_loss = 2.198040723800659, val_loss = 0.8026876449584961\n",
      "epoch n°173 : train_loss = 2.188293695449829, val_loss = 0.7935169339179993\n",
      "epoch n°174 : train_loss = 2.1804184913635254, val_loss = 0.7975926399230957\n",
      "epoch n°175 : train_loss = 2.182562828063965, val_loss = 0.7945066094398499\n",
      "epoch n°176 : train_loss = 2.1761128902435303, val_loss = 0.7984804511070251\n",
      "epoch n°177 : train_loss = 2.187263250350952, val_loss = 0.7934541702270508\n",
      "epoch n°178 : train_loss = 2.184523344039917, val_loss = 0.8006417155265808\n",
      "epoch n°179 : train_loss = 2.1932990550994873, val_loss = 0.795159101486206\n",
      "epoch n°180 : train_loss = 2.185871124267578, val_loss = 0.7956365942955017\n",
      "epoch n°181 : train_loss = 2.180363416671753, val_loss = 0.7970072031021118\n",
      "epoch n°182 : train_loss = 2.180666923522949, val_loss = 0.7989902496337891\n",
      "epoch n°183 : train_loss = 2.185241222381592, val_loss = 0.7948919534683228\n",
      "epoch n°184 : train_loss = 2.174180507659912, val_loss = 0.7968537211418152\n",
      "epoch n°185 : train_loss = 2.1849844455718994, val_loss = 0.7975037693977356\n",
      "epoch n°186 : train_loss = 2.183676242828369, val_loss = 0.7970384359359741\n",
      "epoch n°187 : train_loss = 2.1834402084350586, val_loss = 0.7985689043998718\n",
      "epoch n°188 : train_loss = 2.182290554046631, val_loss = 0.7945634722709656\n",
      "epoch n°189 : train_loss = 2.1743102073669434, val_loss = 0.7989301085472107\n",
      "epoch n°190 : train_loss = 2.1813838481903076, val_loss = 0.7960166335105896\n",
      "epoch n°191 : train_loss = 2.187302827835083, val_loss = 0.7921397089958191\n",
      "epoch n°192 : train_loss = 2.176844596862793, val_loss = 0.7941970825195312\n",
      "epoch n°193 : train_loss = 2.172522783279419, val_loss = 0.7947592735290527\n",
      "epoch n°194 : train_loss = 2.166558265686035, val_loss = 0.7986869812011719\n",
      "epoch n°195 : train_loss = 2.1728265285491943, val_loss = 0.7983100414276123\n",
      "epoch n°196 : train_loss = 2.179168462753296, val_loss = 0.8007673025131226\n",
      "epoch n°197 : train_loss = 2.178046703338623, val_loss = 0.7963425517082214\n",
      "epoch n°198 : train_loss = 2.168109178543091, val_loss = 0.7947540879249573\n",
      "epoch n°199 : train_loss = 2.1597650051116943, val_loss = 0.7960147261619568\n",
      "epoch n°200 : train_loss = 2.1677417755126953, val_loss = 0.7969757318496704\n",
      "epoch n°201 : train_loss = 2.174103021621704, val_loss = 0.7944111824035645\n",
      "epoch n°202 : train_loss = 2.16544508934021, val_loss = 0.7908408045768738\n",
      "epoch n°203 : train_loss = 2.1770291328430176, val_loss = 0.7971850633621216\n",
      "epoch n°204 : train_loss = 2.1739561557769775, val_loss = 0.7981933355331421\n",
      "epoch n°205 : train_loss = 2.176424264907837, val_loss = 0.7935177087783813\n",
      "epoch n°206 : train_loss = 2.170506000518799, val_loss = 0.8011504411697388\n",
      "epoch n°207 : train_loss = 2.1775498390197754, val_loss = 0.7957550287246704\n",
      "epoch n°208 : train_loss = 2.166512966156006, val_loss = 0.793641209602356\n",
      "epoch n°209 : train_loss = 2.178145408630371, val_loss = 0.7962242960929871\n",
      "epoch n°210 : train_loss = 2.17055606842041, val_loss = 0.7940361499786377\n",
      "epoch n°211 : train_loss = 2.167975902557373, val_loss = 0.7929506897926331\n",
      "epoch n°212 : train_loss = 2.1649913787841797, val_loss = 0.79615718126297\n",
      "epoch n°213 : train_loss = 2.165842056274414, val_loss = 0.7993385791778564\n",
      "epoch n°214 : train_loss = 2.170194625854492, val_loss = 0.795453667640686\n",
      "epoch n°215 : train_loss = 2.1742796897888184, val_loss = 0.7925786972045898\n",
      "epoch n°216 : train_loss = 2.161668300628662, val_loss = 0.7952303886413574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°217 : train_loss = 2.1599349975585938, val_loss = 0.7956164479255676\n",
      "epoch n°218 : train_loss = 2.1675987243652344, val_loss = 0.793487548828125\n",
      "epoch n°219 : train_loss = 2.1683197021484375, val_loss = 0.7920657992362976\n",
      "epoch n°220 : train_loss = 2.1651599407196045, val_loss = 0.7942768335342407\n",
      "epoch n°221 : train_loss = 2.16426157951355, val_loss = 0.7937719821929932\n",
      "epoch n°222 : train_loss = 2.168322801589966, val_loss = 0.7956324219703674\n",
      "epoch n°223 : train_loss = 2.165876626968384, val_loss = 0.7932578921318054\n",
      "epoch n°224 : train_loss = 2.166597843170166, val_loss = 0.7945222854614258\n",
      "epoch n°225 : train_loss = 2.1617558002471924, val_loss = 0.7944337725639343\n",
      "epoch n°226 : train_loss = 2.1660780906677246, val_loss = 0.7927371859550476\n",
      "epoch n°227 : train_loss = 2.1703999042510986, val_loss = 0.7980520129203796\n",
      "epoch n°228 : train_loss = 2.1620302200317383, val_loss = 0.7928245067596436\n",
      "epoch n°229 : train_loss = 2.1668448448181152, val_loss = 0.7943317890167236\n",
      "epoch n°230 : train_loss = 2.158195734024048, val_loss = 0.7924462556838989\n",
      "epoch n°231 : train_loss = 2.160352945327759, val_loss = 0.7940990924835205\n",
      "epoch n°232 : train_loss = 2.1655478477478027, val_loss = 0.7942239046096802\n",
      "epoch n°233 : train_loss = 2.1702513694763184, val_loss = 0.8008352518081665\n",
      "epoch n°234 : train_loss = 2.1704509258270264, val_loss = 0.7958194017410278\n",
      "epoch n°235 : train_loss = 2.164539337158203, val_loss = 0.79532790184021\n",
      "epoch n°236 : train_loss = 2.1599972248077393, val_loss = 0.7972529530525208\n",
      "epoch n°237 : train_loss = 2.160994052886963, val_loss = 0.7942821383476257\n",
      "epoch n°238 : train_loss = 2.1621763706207275, val_loss = 0.7927915453910828\n",
      "epoch n°239 : train_loss = 2.1708500385284424, val_loss = 0.797471284866333\n",
      "epoch n°240 : train_loss = 2.174328565597534, val_loss = 0.7997312545776367\n",
      "epoch n°241 : train_loss = 2.18332576751709, val_loss = 0.7969303131103516\n",
      "epoch n°242 : train_loss = 2.1930837631225586, val_loss = 0.7965751886367798\n",
      "epoch n°243 : train_loss = 2.1905558109283447, val_loss = 0.7930024862289429\n",
      "epoch n°244 : train_loss = 2.183729887008667, val_loss = 0.798717737197876\n",
      "epoch n°245 : train_loss = 2.18066668510437, val_loss = 0.8002286553382874\n",
      "epoch n°246 : train_loss = 2.1887295246124268, val_loss = 0.7991369962692261\n",
      "epoch n°247 : train_loss = 2.18213152885437, val_loss = 0.7993520498275757\n",
      "epoch n°248 : train_loss = 2.1847710609436035, val_loss = 0.7985881567001343\n",
      "epoch n°249 : train_loss = 2.1846535205841064, val_loss = 0.795421838760376\n",
      "epoch n°250 : train_loss = 2.1955204010009766, val_loss = 0.8025717735290527\n",
      "epoch n°251 : train_loss = 2.1848065853118896, val_loss = 0.7974698543548584\n",
      "epoch n°252 : train_loss = 2.1797163486480713, val_loss = 0.7983064651489258\n",
      "epoch n°253 : train_loss = 2.1852192878723145, val_loss = 0.7955262064933777\n",
      "epoch n°254 : train_loss = 2.1897175312042236, val_loss = 0.8012092113494873\n",
      "epoch n°255 : train_loss = 2.1800928115844727, val_loss = 0.7940340638160706\n",
      "epoch n°256 : train_loss = 2.1997783184051514, val_loss = 0.8013233542442322\n",
      "epoch n°257 : train_loss = 2.2044219970703125, val_loss = 0.7976861596107483\n",
      "epoch n°258 : train_loss = 2.1917028427124023, val_loss = 0.7971369624137878\n",
      "epoch n°259 : train_loss = 2.1904940605163574, val_loss = 0.7999165058135986\n",
      "epoch n°260 : train_loss = 2.1851751804351807, val_loss = 0.7999997138977051\n",
      "epoch n°261 : train_loss = 2.1864306926727295, val_loss = 0.7957208156585693\n",
      "epoch n°262 : train_loss = 2.1936306953430176, val_loss = 0.7971417903900146\n",
      "epoch n°263 : train_loss = 2.1897292137145996, val_loss = 0.7945456504821777\n",
      "epoch n°264 : train_loss = 2.1950082778930664, val_loss = 0.7978277206420898\n",
      "epoch n°265 : train_loss = 2.1939446926116943, val_loss = 0.7984195351600647\n",
      "epoch n°266 : train_loss = 2.185983896255493, val_loss = 0.798608124256134\n",
      "epoch n°267 : train_loss = 2.1866743564605713, val_loss = 0.7948058843612671\n",
      "epoch n°268 : train_loss = 2.192075490951538, val_loss = 0.8002179861068726\n",
      "epoch n°269 : train_loss = 2.1803877353668213, val_loss = 0.7979827523231506\n",
      "epoch n°270 : train_loss = 2.1745264530181885, val_loss = 0.7984347343444824\n",
      "epoch n°271 : train_loss = 2.1827964782714844, val_loss = 0.7958833575248718\n",
      "epoch n°272 : train_loss = 2.1835758686065674, val_loss = 0.7951294779777527\n",
      "epoch n°273 : train_loss = 2.18967604637146, val_loss = 0.7950695157051086\n",
      "epoch n°274 : train_loss = 2.178804636001587, val_loss = 0.7983345985412598\n",
      "epoch n°275 : train_loss = 2.180044174194336, val_loss = 0.8015756607055664\n",
      "epoch n°276 : train_loss = 2.183326244354248, val_loss = 0.795272171497345\n",
      "epoch n°277 : train_loss = 2.1757125854492188, val_loss = 0.7966046333312988\n",
      "epoch n°278 : train_loss = 2.1735363006591797, val_loss = 0.7967109680175781\n",
      "epoch n°279 : train_loss = 2.1849310398101807, val_loss = 0.7991596460342407\n",
      "epoch n°280 : train_loss = 2.1805193424224854, val_loss = 0.7974884510040283\n",
      "epoch n°281 : train_loss = 2.1753673553466797, val_loss = 0.795124888420105\n",
      "epoch n°282 : train_loss = 2.1871860027313232, val_loss = 0.7962719798088074\n",
      "epoch n°283 : train_loss = 2.182297468185425, val_loss = 0.8005484938621521\n",
      "epoch n°284 : train_loss = 2.1839962005615234, val_loss = 0.8005736470222473\n",
      "epoch n°285 : train_loss = 2.1785593032836914, val_loss = 0.7956863641738892\n",
      "epoch n°286 : train_loss = 2.1813879013061523, val_loss = 0.7999665141105652\n",
      "epoch n°287 : train_loss = 2.1829519271850586, val_loss = 0.7945839762687683\n",
      "epoch n°288 : train_loss = 2.1767759323120117, val_loss = 0.7998751997947693\n",
      "epoch n°289 : train_loss = 2.1806013584136963, val_loss = 0.7988272309303284\n",
      "epoch n°290 : train_loss = 2.1714131832122803, val_loss = 0.80201655626297\n",
      "epoch n°291 : train_loss = 2.1779165267944336, val_loss = 0.7960570454597473\n",
      "epoch n°292 : train_loss = 2.1646652221679688, val_loss = 0.7965006232261658\n",
      "epoch n°293 : train_loss = 2.1817896366119385, val_loss = 0.7958061695098877\n",
      "epoch n°294 : train_loss = 2.178881883621216, val_loss = 0.796302318572998\n",
      "epoch n°295 : train_loss = 2.172760248184204, val_loss = 0.7992324233055115\n",
      "epoch n°296 : train_loss = 2.17179274559021, val_loss = 0.7988497614860535\n",
      "epoch n°297 : train_loss = 2.1714909076690674, val_loss = 0.7948665022850037\n",
      "epoch n°298 : train_loss = 2.184589385986328, val_loss = 0.7940834760665894\n",
      "epoch n°299 : train_loss = 2.1739883422851562, val_loss = 0.7971817851066589\n",
      "epoch n°300 : train_loss = 2.164799451828003, val_loss = 0.8004574179649353\n",
      "epoch n°301 : train_loss = 2.177631139755249, val_loss = 0.7949509024620056\n",
      "epoch n°302 : train_loss = 2.1692402362823486, val_loss = 0.7986974716186523\n",
      "epoch n°303 : train_loss = 2.1779396533966064, val_loss = 0.7966220378875732\n",
      "epoch n°304 : train_loss = 2.1714842319488525, val_loss = 0.7968965172767639\n",
      "epoch n°305 : train_loss = 2.1709954738616943, val_loss = 0.7966302037239075\n",
      "epoch n°306 : train_loss = 2.1677498817443848, val_loss = 0.796716570854187\n",
      "epoch n°307 : train_loss = 2.1653976440429688, val_loss = 0.7913409471511841\n",
      "epoch n°308 : train_loss = 2.1666815280914307, val_loss = 0.795647382736206\n",
      "epoch n°309 : train_loss = 2.170305013656616, val_loss = 0.7972573041915894\n",
      "epoch n°310 : train_loss = 2.1664927005767822, val_loss = 0.7963435649871826\n",
      "epoch n°311 : train_loss = 2.1659746170043945, val_loss = 0.7938324213027954\n",
      "epoch n°312 : train_loss = 2.158860206604004, val_loss = 0.7956174612045288\n",
      "epoch n°313 : train_loss = 2.1681509017944336, val_loss = 0.7987930774688721\n",
      "epoch n°314 : train_loss = 2.179227590560913, val_loss = 0.7970327734947205\n",
      "epoch n°315 : train_loss = 2.1642441749572754, val_loss = 0.7976920008659363\n",
      "epoch n°316 : train_loss = 2.1617836952209473, val_loss = 0.7919207811355591\n",
      "epoch n°317 : train_loss = 2.161102533340454, val_loss = 0.796698272228241\n",
      "epoch n°318 : train_loss = 2.1668808460235596, val_loss = 0.7970102429389954\n",
      "epoch n°319 : train_loss = 2.162386894226074, val_loss = 0.7975265979766846\n",
      "epoch n°320 : train_loss = 2.159524917602539, val_loss = 0.7959454655647278\n",
      "epoch n°321 : train_loss = 2.1628122329711914, val_loss = 0.7963795065879822\n",
      "epoch n°322 : train_loss = 2.168144941329956, val_loss = 0.7963590025901794\n",
      "epoch n°323 : train_loss = 2.163193941116333, val_loss = 0.7997217774391174\n",
      "epoch n°324 : train_loss = 2.164156198501587, val_loss = 0.7994018793106079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°325 : train_loss = 2.1665468215942383, val_loss = 0.7958833575248718\n",
      "epoch n°326 : train_loss = 2.165904998779297, val_loss = 0.7965208888053894\n",
      "epoch n°327 : train_loss = 2.1603548526763916, val_loss = 0.794757068157196\n",
      "epoch n°328 : train_loss = 2.158196449279785, val_loss = 0.8015500903129578\n",
      "epoch n°329 : train_loss = 2.168900728225708, val_loss = 0.795966386795044\n",
      "epoch n°330 : train_loss = 2.1663596630096436, val_loss = 0.7930715084075928\n",
      "epoch n°331 : train_loss = 2.1627871990203857, val_loss = 0.7949982285499573\n",
      "epoch n°332 : train_loss = 2.1571617126464844, val_loss = 0.7983756065368652\n",
      "epoch n°333 : train_loss = 2.157670021057129, val_loss = 0.7993775010108948\n",
      "epoch n°334 : train_loss = 2.1526927947998047, val_loss = 0.7998030185699463\n",
      "epoch n°335 : train_loss = 2.1635732650756836, val_loss = 0.7967163920402527\n",
      "epoch n°336 : train_loss = 2.1598992347717285, val_loss = 0.8029744625091553\n",
      "epoch n°337 : train_loss = 2.1608405113220215, val_loss = 0.7991942763328552\n",
      "epoch n°338 : train_loss = 2.1560144424438477, val_loss = 0.7908215522766113\n",
      "epoch n°339 : train_loss = 2.166555881500244, val_loss = 0.7957388162612915\n",
      "epoch n°340 : train_loss = 2.16497540473938, val_loss = 0.7981331944465637\n",
      "epoch n°341 : train_loss = 2.162332773208618, val_loss = 0.7948766350746155\n",
      "epoch n°342 : train_loss = 2.1656806468963623, val_loss = 0.7943940162658691\n",
      "epoch n°343 : train_loss = 2.1530368328094482, val_loss = 0.7964318990707397\n",
      "epoch n°344 : train_loss = 2.1520776748657227, val_loss = 0.7925745844841003\n",
      "epoch n°345 : train_loss = 2.163227081298828, val_loss = 0.7970622181892395\n",
      "epoch n°346 : train_loss = 2.158801794052124, val_loss = 0.7977741360664368\n",
      "epoch n°347 : train_loss = 2.144984006881714, val_loss = 0.7955861687660217\n",
      "epoch n°348 : train_loss = 2.1537888050079346, val_loss = 0.7922722697257996\n",
      "epoch n°349 : train_loss = 2.1573078632354736, val_loss = 0.7909941673278809\n",
      "epoch n°350 : train_loss = 2.1535167694091797, val_loss = 0.7944703698158264\n",
      "epoch n°351 : train_loss = 2.1545164585113525, val_loss = 0.7974117994308472\n",
      "epoch n°352 : train_loss = 2.167665481567383, val_loss = 0.795741081237793\n",
      "epoch n°353 : train_loss = 2.15085768699646, val_loss = 0.7979729771614075\n",
      "epoch n°354 : train_loss = 2.157569646835327, val_loss = 0.7943652868270874\n",
      "epoch n°355 : train_loss = 2.149813413619995, val_loss = 0.7967658042907715\n",
      "epoch n°356 : train_loss = 2.139539957046509, val_loss = 0.7958203554153442\n",
      "epoch n°357 : train_loss = 2.1504316329956055, val_loss = 0.7977147102355957\n",
      "epoch n°358 : train_loss = 2.1523473262786865, val_loss = 0.7947776913642883\n",
      "epoch n°359 : train_loss = 2.1531264781951904, val_loss = 0.7961426377296448\n",
      "epoch n°360 : train_loss = 2.157737970352173, val_loss = 0.8003295660018921\n",
      "epoch n°361 : train_loss = 2.1533873081207275, val_loss = 0.7949550747871399\n",
      "epoch n°362 : train_loss = 2.153890371322632, val_loss = 0.8012321591377258\n",
      "epoch n°363 : train_loss = 2.1421539783477783, val_loss = 0.7945308089256287\n",
      "epoch n°364 : train_loss = 2.156651020050049, val_loss = 0.7968414425849915\n",
      "epoch n°365 : train_loss = 2.1466243267059326, val_loss = 0.7961082458496094\n",
      "epoch n°366 : train_loss = 2.1459197998046875, val_loss = 0.7962285280227661\n",
      "epoch n°367 : train_loss = 2.148711681365967, val_loss = 0.795480489730835\n",
      "epoch n°368 : train_loss = 2.146106243133545, val_loss = 0.7959221005439758\n",
      "epoch n°369 : train_loss = 2.140460252761841, val_loss = 0.7964784502983093\n",
      "epoch n°370 : train_loss = 2.153129816055298, val_loss = 0.7965795397758484\n",
      "epoch n°371 : train_loss = 2.1422276496887207, val_loss = 0.7982214093208313\n",
      "epoch n°372 : train_loss = 2.1447062492370605, val_loss = 0.798503577709198\n",
      "epoch n°373 : train_loss = 2.149749279022217, val_loss = 0.793365478515625\n",
      "epoch n°374 : train_loss = 2.1408684253692627, val_loss = 0.7910194396972656\n",
      "epoch n°375 : train_loss = 2.1475210189819336, val_loss = 0.7936851382255554\n",
      "epoch n°376 : train_loss = 2.146820306777954, val_loss = 0.7957070469856262\n",
      "epoch n°377 : train_loss = 2.1394121646881104, val_loss = 0.7956984639167786\n",
      "epoch n°378 : train_loss = 2.138632297515869, val_loss = 0.7945884466171265\n",
      "epoch n°379 : train_loss = 2.1426122188568115, val_loss = 0.798518180847168\n",
      "epoch n°380 : train_loss = 2.1441397666931152, val_loss = 0.7955535054206848\n",
      "epoch n°381 : train_loss = 2.144425392150879, val_loss = 0.7899559140205383\n",
      "epoch n°382 : train_loss = 2.137216329574585, val_loss = 0.7941880822181702\n",
      "epoch n°383 : train_loss = 2.1413514614105225, val_loss = 0.7962797284126282\n",
      "epoch n°384 : train_loss = 2.1405110359191895, val_loss = 0.7981297969818115\n",
      "epoch n°385 : train_loss = 2.1368188858032227, val_loss = 0.7939934730529785\n",
      "epoch n°386 : train_loss = 2.137171745300293, val_loss = 0.7972440123558044\n",
      "epoch n°387 : train_loss = 2.1436309814453125, val_loss = 0.7957385182380676\n",
      "epoch n°388 : train_loss = 2.133554458618164, val_loss = 0.7957249283790588\n",
      "epoch n°389 : train_loss = 2.1395320892333984, val_loss = 0.7925418019294739\n",
      "epoch n°390 : train_loss = 2.1437878608703613, val_loss = 0.7952391505241394\n",
      "epoch n°391 : train_loss = 2.1419479846954346, val_loss = 0.789617657661438\n",
      "epoch n°392 : train_loss = 2.1290578842163086, val_loss = 0.7938874959945679\n",
      "epoch n°393 : train_loss = 2.1368234157562256, val_loss = 0.7918118238449097\n",
      "epoch n°394 : train_loss = 2.1429803371429443, val_loss = 0.794998824596405\n",
      "epoch n°395 : train_loss = 2.140784978866577, val_loss = 0.7966862320899963\n",
      "epoch n°396 : train_loss = 2.1350274085998535, val_loss = 0.7924386858940125\n",
      "epoch n°397 : train_loss = 2.1357457637786865, val_loss = 0.7989867329597473\n",
      "epoch n°398 : train_loss = 2.1426138877868652, val_loss = 0.7926502823829651\n",
      "epoch n°399 : train_loss = 2.1310267448425293, val_loss = 0.7916426062583923\n",
      "epoch n°400 : train_loss = 2.1423418521881104, val_loss = 0.7911113500595093\n",
      "epoch n°401 : train_loss = 2.1438491344451904, val_loss = 0.7923992872238159\n",
      "epoch n°402 : train_loss = 2.142512321472168, val_loss = 0.7926949858665466\n",
      "epoch n°403 : train_loss = 2.1391990184783936, val_loss = 0.793127715587616\n",
      "epoch n°404 : train_loss = 2.1334307193756104, val_loss = 0.7976294159889221\n",
      "epoch n°405 : train_loss = 2.1229233741760254, val_loss = 0.7904890179634094\n",
      "epoch n°406 : train_loss = 2.135145664215088, val_loss = 0.7928575873374939\n",
      "epoch n°407 : train_loss = 2.133124828338623, val_loss = 0.7931086421012878\n",
      "epoch n°408 : train_loss = 2.1359918117523193, val_loss = 0.7932767868041992\n",
      "epoch n°409 : train_loss = 2.1400554180145264, val_loss = 0.7914532423019409\n",
      "epoch n°410 : train_loss = 2.1401238441467285, val_loss = 0.7958771586418152\n",
      "epoch n°411 : train_loss = 2.135768413543701, val_loss = 0.7925684452056885\n",
      "epoch n°412 : train_loss = 2.129312515258789, val_loss = 0.7948547005653381\n",
      "epoch n°413 : train_loss = 2.1417644023895264, val_loss = 0.7944900989532471\n",
      "epoch n°414 : train_loss = 2.1251895427703857, val_loss = 0.7983021140098572\n",
      "epoch n°415 : train_loss = 2.129732370376587, val_loss = 0.7961007952690125\n",
      "epoch n°416 : train_loss = 2.138686180114746, val_loss = 0.7966278791427612\n",
      "epoch n°417 : train_loss = 2.133244514465332, val_loss = 0.7963303327560425\n",
      "epoch n°418 : train_loss = 2.143125057220459, val_loss = 0.7974584102630615\n",
      "epoch n°419 : train_loss = 2.1312551498413086, val_loss = 0.7974757552146912\n",
      "epoch n°420 : train_loss = 2.1273159980773926, val_loss = 0.7942779660224915\n",
      "epoch n°421 : train_loss = 2.126904249191284, val_loss = 0.7942348718643188\n",
      "epoch n°422 : train_loss = 2.1344549655914307, val_loss = 0.7948796153068542\n",
      "epoch n°423 : train_loss = 2.132415533065796, val_loss = 0.7992961406707764\n",
      "epoch n°424 : train_loss = 2.138047695159912, val_loss = 0.7974795699119568\n",
      "epoch n°425 : train_loss = 2.1226885318756104, val_loss = 0.7956966757774353\n",
      "epoch n°426 : train_loss = 2.1362226009368896, val_loss = 0.7935113906860352\n",
      "epoch n°427 : train_loss = 2.1330409049987793, val_loss = 0.7910986542701721\n",
      "epoch n°428 : train_loss = 2.1222739219665527, val_loss = 0.7938403487205505\n",
      "epoch n°429 : train_loss = 2.1314632892608643, val_loss = 0.7945395708084106\n",
      "epoch n°430 : train_loss = 2.131347894668579, val_loss = 0.794580340385437\n",
      "epoch n°431 : train_loss = 2.1266283988952637, val_loss = 0.797363817691803\n",
      "epoch n°432 : train_loss = 2.131693124771118, val_loss = 0.7919393181800842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°433 : train_loss = 2.130415439605713, val_loss = 0.7924270033836365\n",
      "epoch n°434 : train_loss = 2.1277623176574707, val_loss = 0.7986264824867249\n",
      "epoch n°435 : train_loss = 2.1249375343322754, val_loss = 0.7926645278930664\n",
      "epoch n°436 : train_loss = 2.1253819465637207, val_loss = 0.79542475938797\n",
      "epoch n°437 : train_loss = 2.1261792182922363, val_loss = 0.792827844619751\n",
      "epoch n°438 : train_loss = 2.125342845916748, val_loss = 0.7938708066940308\n",
      "epoch n°439 : train_loss = 2.1240503787994385, val_loss = 0.795914888381958\n",
      "epoch n°440 : train_loss = 2.1275899410247803, val_loss = 0.7911152839660645\n",
      "epoch n°441 : train_loss = 2.138592004776001, val_loss = 0.7918905019760132\n",
      "epoch n°442 : train_loss = 2.1133108139038086, val_loss = 0.7916736006736755\n",
      "epoch n°443 : train_loss = 2.115070104598999, val_loss = 0.7915818095207214\n",
      "epoch n°444 : train_loss = 2.1236679553985596, val_loss = 0.7949360013008118\n",
      "epoch n°445 : train_loss = 2.1324400901794434, val_loss = 0.7943419218063354\n",
      "epoch n°446 : train_loss = 2.132636308670044, val_loss = 0.791483461856842\n",
      "epoch n°447 : train_loss = 2.129613161087036, val_loss = 0.7916945815086365\n",
      "epoch n°448 : train_loss = 2.1236400604248047, val_loss = 0.7944794297218323\n",
      "epoch n°449 : train_loss = 2.1243977546691895, val_loss = 0.7949780821800232\n",
      "epoch n°450 : train_loss = 2.122366428375244, val_loss = 0.7925148010253906\n",
      "epoch n°451 : train_loss = 2.1285109519958496, val_loss = 0.7943728566169739\n",
      "epoch n°452 : train_loss = 2.121225357055664, val_loss = 0.7937771677970886\n",
      "epoch n°453 : train_loss = 2.119457960128784, val_loss = 0.7953421473503113\n",
      "epoch n°454 : train_loss = 2.122763156890869, val_loss = 0.7927881479263306\n",
      "epoch n°455 : train_loss = 2.1219446659088135, val_loss = 0.7903757095336914\n",
      "epoch n°456 : train_loss = 2.1248719692230225, val_loss = 0.7957279682159424\n",
      "epoch n°457 : train_loss = 2.1214840412139893, val_loss = 0.7969752550125122\n",
      "epoch n°458 : train_loss = 2.1255264282226562, val_loss = 0.7943692207336426\n",
      "epoch n°459 : train_loss = 2.1336443424224854, val_loss = 0.7927334904670715\n",
      "epoch n°460 : train_loss = 2.1214869022369385, val_loss = 0.7928482890129089\n",
      "epoch n°461 : train_loss = 2.128648281097412, val_loss = 0.7927318811416626\n",
      "epoch n°462 : train_loss = 2.1221585273742676, val_loss = 0.7958927154541016\n",
      "epoch n°463 : train_loss = 2.1234893798828125, val_loss = 0.790686845779419\n",
      "epoch n°464 : train_loss = 2.1271097660064697, val_loss = 0.7959086894989014\n",
      "epoch n°465 : train_loss = 2.120250940322876, val_loss = 0.7948445081710815\n",
      "epoch n°466 : train_loss = 2.1216602325439453, val_loss = 0.7947261333465576\n",
      "epoch n°467 : train_loss = 2.116036891937256, val_loss = 0.7915638089179993\n",
      "epoch n°468 : train_loss = 2.1222143173217773, val_loss = 0.7945994734764099\n",
      "epoch n°469 : train_loss = 2.1224472522735596, val_loss = 0.7943230271339417\n",
      "epoch n°470 : train_loss = 2.117065668106079, val_loss = 0.7909867763519287\n",
      "epoch n°471 : train_loss = 2.119741439819336, val_loss = 0.7945155501365662\n",
      "epoch n°472 : train_loss = 2.1339502334594727, val_loss = 0.794916033744812\n",
      "epoch n°473 : train_loss = 2.122523069381714, val_loss = 0.7917510867118835\n",
      "epoch n°474 : train_loss = 2.126811981201172, val_loss = 0.7969653606414795\n",
      "epoch n°475 : train_loss = 2.119053840637207, val_loss = 0.7915783524513245\n",
      "epoch n°476 : train_loss = 2.1246511936187744, val_loss = 0.7921218276023865\n",
      "epoch n°477 : train_loss = 2.1263086795806885, val_loss = 0.7949334383010864\n",
      "epoch n°478 : train_loss = 2.120147228240967, val_loss = 0.7941490411758423\n",
      "epoch n°479 : train_loss = 2.117814779281616, val_loss = 0.7919769883155823\n",
      "epoch n°480 : train_loss = 2.1219241619110107, val_loss = 0.7928876280784607\n",
      "epoch n°481 : train_loss = 2.1215884685516357, val_loss = 0.7903642058372498\n",
      "epoch n°482 : train_loss = 2.1252901554107666, val_loss = 0.7900995016098022\n",
      "epoch n°483 : train_loss = 2.126621961593628, val_loss = 0.7967183589935303\n",
      "epoch n°484 : train_loss = 2.1274843215942383, val_loss = 0.7952933311462402\n",
      "epoch n°485 : train_loss = 2.1269290447235107, val_loss = 0.7928979396820068\n",
      "epoch n°486 : train_loss = 2.116128444671631, val_loss = 0.79740971326828\n",
      "epoch n°487 : train_loss = 2.120964527130127, val_loss = 0.7886465191841125\n",
      "epoch n°488 : train_loss = 2.127202033996582, val_loss = 0.7912057638168335\n",
      "epoch n°489 : train_loss = 2.1227681636810303, val_loss = 0.7969093918800354\n",
      "epoch n°490 : train_loss = 2.121617078781128, val_loss = 0.7930280566215515\n",
      "epoch n°491 : train_loss = 2.1238534450531006, val_loss = 0.7937965989112854\n",
      "epoch n°492 : train_loss = 2.129973888397217, val_loss = 0.789720356464386\n",
      "epoch n°493 : train_loss = 2.124460458755493, val_loss = 0.7956122159957886\n",
      "epoch n°494 : train_loss = 2.1315104961395264, val_loss = 0.7945773005485535\n",
      "epoch n°495 : train_loss = 2.125976800918579, val_loss = 0.7936569452285767\n",
      "epoch n°496 : train_loss = 2.1434860229492188, val_loss = 0.8019099831581116\n",
      "epoch n°497 : train_loss = 2.149557590484619, val_loss = 0.7962115406990051\n",
      "epoch n°498 : train_loss = 2.1458981037139893, val_loss = 0.7972835898399353\n",
      "epoch n°499 : train_loss = 2.1339917182922363, val_loss = 0.8001937866210938\n",
      "epoch n°500 : train_loss = 2.150613784790039, val_loss = 0.7939467430114746\n",
      "epoch n°501 : train_loss = 2.139747142791748, val_loss = 0.8001836538314819\n",
      "epoch n°502 : train_loss = 2.150294303894043, val_loss = 0.7972168326377869\n",
      "epoch n°503 : train_loss = 2.1523869037628174, val_loss = 0.7953565716743469\n",
      "epoch n°504 : train_loss = 2.1454896926879883, val_loss = 0.7940072417259216\n",
      "epoch n°505 : train_loss = 2.154294729232788, val_loss = 0.8003284931182861\n",
      "epoch n°506 : train_loss = 2.1573705673217773, val_loss = 0.7967967391014099\n",
      "epoch n°507 : train_loss = 2.14791202545166, val_loss = 0.7941710948944092\n",
      "epoch n°508 : train_loss = 2.1420881748199463, val_loss = 0.7939830422401428\n",
      "epoch n°509 : train_loss = 2.156409978866577, val_loss = 0.795668363571167\n",
      "epoch n°510 : train_loss = 2.151714324951172, val_loss = 0.7951239943504333\n",
      "epoch n°511 : train_loss = 2.14481258392334, val_loss = 0.7974005937576294\n",
      "epoch n°512 : train_loss = 2.1420440673828125, val_loss = 0.7965284585952759\n",
      "epoch n°513 : train_loss = 2.15187931060791, val_loss = 0.7983772158622742\n",
      "epoch n°514 : train_loss = 2.154801845550537, val_loss = 0.7970750331878662\n",
      "epoch n°515 : train_loss = 2.14414644241333, val_loss = 0.7993934750556946\n",
      "epoch n°516 : train_loss = 2.1589982509613037, val_loss = 0.8002161383628845\n",
      "epoch n°517 : train_loss = 2.1585888862609863, val_loss = 0.8002917170524597\n",
      "epoch n°518 : train_loss = 2.145967483520508, val_loss = 0.795862078666687\n",
      "epoch n°519 : train_loss = 2.160404920578003, val_loss = 0.8003511428833008\n",
      "epoch n°520 : train_loss = 2.144753932952881, val_loss = 0.7977569699287415\n",
      "epoch n°521 : train_loss = 2.151249408721924, val_loss = 0.8042197823524475\n",
      "epoch n°522 : train_loss = 2.149172067642212, val_loss = 0.7954400777816772\n",
      "epoch n°523 : train_loss = 2.1490540504455566, val_loss = 0.7940179705619812\n",
      "epoch n°524 : train_loss = 2.1508688926696777, val_loss = 0.8015629649162292\n",
      "epoch n°525 : train_loss = 2.1481142044067383, val_loss = 0.7961235642433167\n",
      "epoch n°526 : train_loss = 2.1574273109436035, val_loss = 0.7966175079345703\n",
      "epoch n°527 : train_loss = 2.153904914855957, val_loss = 0.7994226217269897\n",
      "epoch n°528 : train_loss = 2.1477837562561035, val_loss = 0.7952442765235901\n",
      "epoch n°529 : train_loss = 2.1452479362487793, val_loss = 0.7882830500602722\n",
      "epoch n°530 : train_loss = 2.1486976146698, val_loss = 0.7974085807800293\n",
      "epoch n°531 : train_loss = 2.1519007682800293, val_loss = 0.7964538931846619\n",
      "epoch n°532 : train_loss = 2.159496307373047, val_loss = 0.7959951758384705\n",
      "epoch n°533 : train_loss = 2.148733377456665, val_loss = 0.795791745185852\n",
      "epoch n°534 : train_loss = 2.140230417251587, val_loss = 0.7942654490470886\n",
      "epoch n°535 : train_loss = 2.1485960483551025, val_loss = 0.7981900572776794\n",
      "epoch n°536 : train_loss = 2.1519126892089844, val_loss = 0.797066867351532\n",
      "epoch n°537 : train_loss = 2.1415843963623047, val_loss = 0.7944395542144775\n",
      "epoch n°538 : train_loss = 2.1527657508850098, val_loss = 0.794252336025238\n",
      "epoch n°539 : train_loss = 2.1452035903930664, val_loss = 0.7893880605697632\n",
      "epoch n°540 : train_loss = 2.1495115756988525, val_loss = 0.796044111251831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°541 : train_loss = 2.1444623470306396, val_loss = 0.7980580925941467\n",
      "epoch n°542 : train_loss = 2.1470799446105957, val_loss = 0.7939302921295166\n",
      "epoch n°543 : train_loss = 2.1537892818450928, val_loss = 0.7951680421829224\n",
      "epoch n°544 : train_loss = 2.142354965209961, val_loss = 0.796725332736969\n",
      "epoch n°545 : train_loss = 2.1495919227600098, val_loss = 0.7971682548522949\n",
      "epoch n°546 : train_loss = 2.1460869312286377, val_loss = 0.8001255393028259\n",
      "epoch n°547 : train_loss = 2.1458301544189453, val_loss = 0.7972053289413452\n",
      "epoch n°548 : train_loss = 2.1493961811065674, val_loss = 0.7990917563438416\n",
      "epoch n°549 : train_loss = 2.147158622741699, val_loss = 0.7938517332077026\n",
      "epoch n°550 : train_loss = 2.150017738342285, val_loss = 0.7981680631637573\n",
      "epoch n°551 : train_loss = 2.143214464187622, val_loss = 0.797930896282196\n",
      "epoch n°552 : train_loss = 2.136986017227173, val_loss = 0.7975918054580688\n",
      "epoch n°553 : train_loss = 2.140195846557617, val_loss = 0.7960596084594727\n",
      "epoch n°554 : train_loss = 2.1467690467834473, val_loss = 0.7938739061355591\n",
      "epoch n°555 : train_loss = 2.1499109268188477, val_loss = 0.7973514795303345\n",
      "epoch n°556 : train_loss = 2.1437535285949707, val_loss = 0.7965491414070129\n",
      "epoch n°557 : train_loss = 2.1542840003967285, val_loss = 0.7963758707046509\n",
      "epoch n°558 : train_loss = 2.1474828720092773, val_loss = 0.7951348423957825\n",
      "epoch n°559 : train_loss = 2.151257276535034, val_loss = 0.7949916124343872\n",
      "epoch n°560 : train_loss = 2.1432178020477295, val_loss = 0.8011172413825989\n",
      "epoch n°561 : train_loss = 2.144940137863159, val_loss = 0.7963055372238159\n",
      "epoch n°562 : train_loss = 2.144442319869995, val_loss = 0.7943593263626099\n",
      "epoch n°563 : train_loss = 2.144906759262085, val_loss = 0.7977880239486694\n",
      "epoch n°564 : train_loss = 2.1502137184143066, val_loss = 0.7957613468170166\n",
      "epoch n°565 : train_loss = 2.1438851356506348, val_loss = 0.796119213104248\n",
      "epoch n°566 : train_loss = 2.146573305130005, val_loss = 0.8000457882881165\n",
      "epoch n°567 : train_loss = 2.1454334259033203, val_loss = 0.7946920394897461\n",
      "epoch n°568 : train_loss = 2.142238140106201, val_loss = 0.8009234070777893\n",
      "epoch n°569 : train_loss = 2.1415727138519287, val_loss = 0.793451726436615\n",
      "epoch n°570 : train_loss = 2.138838529586792, val_loss = 0.7954061031341553\n",
      "epoch n°571 : train_loss = 2.141187906265259, val_loss = 0.7961223125457764\n",
      "epoch n°572 : train_loss = 2.13997745513916, val_loss = 0.7941861748695374\n",
      "epoch n°573 : train_loss = 2.142942190170288, val_loss = 0.7990866303443909\n",
      "epoch n°574 : train_loss = 2.1464521884918213, val_loss = 0.7991495132446289\n",
      "epoch n°575 : train_loss = 2.144392490386963, val_loss = 0.8005905747413635\n",
      "epoch n°576 : train_loss = 2.1422669887542725, val_loss = 0.7977169156074524\n",
      "epoch n°577 : train_loss = 2.1438167095184326, val_loss = 0.7975334525108337\n",
      "epoch n°578 : train_loss = 2.1415514945983887, val_loss = 0.7951300740242004\n",
      "epoch n°579 : train_loss = 2.1461637020111084, val_loss = 0.7969292998313904\n",
      "epoch n°580 : train_loss = 2.1451516151428223, val_loss = 0.7924013137817383\n",
      "epoch n°581 : train_loss = 2.145998954772949, val_loss = 0.8004003167152405\n",
      "epoch n°582 : train_loss = 2.1391329765319824, val_loss = 0.7971991896629333\n",
      "epoch n°583 : train_loss = 2.137075901031494, val_loss = 0.7974582314491272\n",
      "epoch n°584 : train_loss = 2.13702654838562, val_loss = 0.8023211359977722\n",
      "epoch n°585 : train_loss = 2.1396608352661133, val_loss = 0.7947080135345459\n",
      "epoch n°586 : train_loss = 2.1444497108459473, val_loss = 0.7953062057495117\n",
      "epoch n°587 : train_loss = 2.1358327865600586, val_loss = 0.796589195728302\n",
      "epoch n°588 : train_loss = 2.136321783065796, val_loss = 0.7966543436050415\n",
      "epoch n°589 : train_loss = 2.1391706466674805, val_loss = 0.7932360768318176\n",
      "epoch n°590 : train_loss = 2.136272430419922, val_loss = 0.7933295369148254\n",
      "epoch n°591 : train_loss = 2.1425952911376953, val_loss = 0.8003048300743103\n",
      "epoch n°592 : train_loss = 2.140730381011963, val_loss = 0.7926642894744873\n",
      "epoch n°593 : train_loss = 2.14178729057312, val_loss = 0.7944765686988831\n",
      "epoch n°594 : train_loss = 2.136026382446289, val_loss = 0.8004528284072876\n",
      "epoch n°595 : train_loss = 2.1439290046691895, val_loss = 0.7981668710708618\n",
      "epoch n°596 : train_loss = 2.1397459506988525, val_loss = 0.7965641021728516\n",
      "epoch n°597 : train_loss = 2.1389596462249756, val_loss = 0.7973738312721252\n",
      "epoch n°598 : train_loss = 2.1342179775238037, val_loss = 0.7959679961204529\n",
      "epoch n°599 : train_loss = 2.133928060531616, val_loss = 0.7946669459342957\n",
      "epoch n°600 : train_loss = 2.133173942565918, val_loss = 0.8028528094291687\n",
      "epoch n°601 : train_loss = 2.13911509513855, val_loss = 0.7931753993034363\n",
      "epoch n°602 : train_loss = 2.1316604614257812, val_loss = 0.7955641746520996\n",
      "epoch n°603 : train_loss = 2.142482042312622, val_loss = 0.7939538359642029\n",
      "epoch n°604 : train_loss = 2.143188714981079, val_loss = 0.7933034300804138\n",
      "epoch n°605 : train_loss = 2.126338243484497, val_loss = 0.7938704490661621\n",
      "epoch n°606 : train_loss = 2.1303834915161133, val_loss = 0.7965263724327087\n",
      "epoch n°607 : train_loss = 2.1364872455596924, val_loss = 0.796153724193573\n",
      "epoch n°608 : train_loss = 2.12551212310791, val_loss = 0.7981017827987671\n",
      "epoch n°609 : train_loss = 2.1335535049438477, val_loss = 0.7992095351219177\n",
      "epoch n°610 : train_loss = 2.1358604431152344, val_loss = 0.7922109365463257\n",
      "epoch n°611 : train_loss = 2.1305599212646484, val_loss = 0.7983455061912537\n",
      "epoch n°612 : train_loss = 2.130537509918213, val_loss = 0.8000177145004272\n",
      "epoch n°613 : train_loss = 2.141977310180664, val_loss = 0.7918950319290161\n",
      "epoch n°614 : train_loss = 2.1308212280273438, val_loss = 0.7967171669006348\n",
      "epoch n°615 : train_loss = 2.129713773727417, val_loss = 0.7985812425613403\n",
      "epoch n°616 : train_loss = 2.126497983932495, val_loss = 0.7961346507072449\n",
      "epoch n°617 : train_loss = 2.1362171173095703, val_loss = 0.7965924143791199\n",
      "epoch n°618 : train_loss = 2.1430251598358154, val_loss = 0.7987412810325623\n",
      "epoch n°619 : train_loss = 2.1272847652435303, val_loss = 0.7939890027046204\n",
      "epoch n°620 : train_loss = 2.1314620971679688, val_loss = 0.7941815853118896\n",
      "epoch n°621 : train_loss = 2.134768486022949, val_loss = 0.7955000400543213\n",
      "epoch n°622 : train_loss = 2.133863925933838, val_loss = 0.7971469163894653\n",
      "epoch n°623 : train_loss = 2.1294639110565186, val_loss = 0.7954652309417725\n",
      "epoch n°624 : train_loss = 2.1262426376342773, val_loss = 0.7934916019439697\n",
      "epoch n°625 : train_loss = 2.137587785720825, val_loss = 0.792427659034729\n",
      "epoch n°626 : train_loss = 2.140533924102783, val_loss = 0.7944470047950745\n",
      "epoch n°627 : train_loss = 2.1292901039123535, val_loss = 0.7990365028381348\n",
      "epoch n°628 : train_loss = 2.1329164505004883, val_loss = 0.7973262071609497\n",
      "epoch n°629 : train_loss = 2.1338765621185303, val_loss = 0.7975397109985352\n",
      "epoch n°630 : train_loss = 2.132927417755127, val_loss = 0.7923588752746582\n",
      "epoch n°631 : train_loss = 2.1345348358154297, val_loss = 0.7944889068603516\n",
      "epoch n°632 : train_loss = 2.125471830368042, val_loss = 0.7927904725074768\n",
      "epoch n°633 : train_loss = 2.1272480487823486, val_loss = 0.7886788845062256\n",
      "epoch n°634 : train_loss = 2.1298604011535645, val_loss = 0.7941691875457764\n",
      "epoch n°635 : train_loss = 2.12579607963562, val_loss = 0.7985948920249939\n",
      "epoch n°636 : train_loss = 2.121739149093628, val_loss = 0.7940770387649536\n",
      "epoch n°637 : train_loss = 2.1357367038726807, val_loss = 0.7966912388801575\n",
      "epoch n°638 : train_loss = 2.130246639251709, val_loss = 0.7929828763008118\n",
      "epoch n°639 : train_loss = 2.1315348148345947, val_loss = 0.7986063957214355\n",
      "epoch n°640 : train_loss = 2.1265697479248047, val_loss = 0.7968040108680725\n",
      "epoch n°641 : train_loss = 2.126728057861328, val_loss = 0.7927898168563843\n",
      "epoch n°642 : train_loss = 2.120270013809204, val_loss = 0.7951905727386475\n",
      "epoch n°643 : train_loss = 2.1183743476867676, val_loss = 0.7959636449813843\n",
      "epoch n°644 : train_loss = 2.129791736602783, val_loss = 0.797156035900116\n",
      "epoch n°645 : train_loss = 2.127078056335449, val_loss = 0.797934889793396\n",
      "epoch n°646 : train_loss = 2.1293394565582275, val_loss = 0.7948171496391296\n",
      "epoch n°647 : train_loss = 2.1252732276916504, val_loss = 0.7961951494216919\n",
      "epoch n°648 : train_loss = 2.1321046352386475, val_loss = 0.7955279350280762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°649 : train_loss = 2.121217966079712, val_loss = 0.7977476119995117\n",
      "epoch n°650 : train_loss = 2.1244940757751465, val_loss = 0.7940459251403809\n",
      "epoch n°651 : train_loss = 2.1248114109039307, val_loss = 0.7982465028762817\n",
      "epoch n°652 : train_loss = 2.1178629398345947, val_loss = 0.796455442905426\n",
      "epoch n°653 : train_loss = 2.1297900676727295, val_loss = 0.7965042591094971\n",
      "epoch n°654 : train_loss = 2.1286370754241943, val_loss = 0.7946485280990601\n",
      "epoch n°655 : train_loss = 2.1237380504608154, val_loss = 0.7962120175361633\n",
      "epoch n°656 : train_loss = 2.123152494430542, val_loss = 0.7973523736000061\n",
      "epoch n°657 : train_loss = 2.1242990493774414, val_loss = 0.7968031167984009\n",
      "epoch n°658 : train_loss = 2.1255571842193604, val_loss = 0.7967371940612793\n",
      "epoch n°659 : train_loss = 2.130291700363159, val_loss = 0.7962353825569153\n",
      "epoch n°660 : train_loss = 2.1286797523498535, val_loss = 0.7983272671699524\n",
      "epoch n°661 : train_loss = 2.1327245235443115, val_loss = 0.7968908548355103\n",
      "epoch n°662 : train_loss = 2.1282269954681396, val_loss = 0.7924763560295105\n",
      "epoch n°663 : train_loss = 2.1160311698913574, val_loss = 0.7932906746864319\n",
      "epoch n°664 : train_loss = 2.1246986389160156, val_loss = 0.7918073534965515\n",
      "epoch n°665 : train_loss = 2.1219985485076904, val_loss = 0.7962601780891418\n",
      "epoch n°666 : train_loss = 2.1199357509613037, val_loss = 0.7953590154647827\n",
      "epoch n°667 : train_loss = 2.1160097122192383, val_loss = 0.79768306016922\n",
      "epoch n°668 : train_loss = 2.122933864593506, val_loss = 0.7946393489837646\n",
      "epoch n°669 : train_loss = 2.1163365840911865, val_loss = 0.7932313680648804\n",
      "epoch n°670 : train_loss = 2.107069969177246, val_loss = 0.7972224950790405\n",
      "epoch n°671 : train_loss = 2.1253299713134766, val_loss = 0.7972155213356018\n",
      "epoch n°672 : train_loss = 2.1197307109832764, val_loss = 0.7992106080055237\n",
      "epoch n°673 : train_loss = 2.126396417617798, val_loss = 0.796804666519165\n",
      "epoch n°674 : train_loss = 2.1248104572296143, val_loss = 0.793081521987915\n",
      "epoch n°675 : train_loss = 2.1243345737457275, val_loss = 0.7978640198707581\n",
      "epoch n°676 : train_loss = 2.1245949268341064, val_loss = 0.8010861277580261\n",
      "epoch n°677 : train_loss = 2.1195108890533447, val_loss = 0.7914046049118042\n",
      "epoch n°678 : train_loss = 2.1177546977996826, val_loss = 0.7983994483947754\n",
      "epoch n°679 : train_loss = 2.117465019226074, val_loss = 0.7940887212753296\n",
      "epoch n°680 : train_loss = 2.1167211532592773, val_loss = 0.7960272431373596\n",
      "epoch n°681 : train_loss = 2.123880386352539, val_loss = 0.7906531095504761\n",
      "epoch n°682 : train_loss = 2.113680601119995, val_loss = 0.7940264940261841\n",
      "epoch n°683 : train_loss = 2.1219396591186523, val_loss = 0.7927660942077637\n",
      "epoch n°684 : train_loss = 2.1236298084259033, val_loss = 0.7961721420288086\n",
      "epoch n°685 : train_loss = 2.1233747005462646, val_loss = 0.7997133135795593\n",
      "epoch n°686 : train_loss = 2.1193301677703857, val_loss = 0.7915834784507751\n",
      "epoch n°687 : train_loss = 2.119691848754883, val_loss = 0.795282781124115\n",
      "epoch n°688 : train_loss = 2.120283365249634, val_loss = 0.796509861946106\n",
      "epoch n°689 : train_loss = 2.1263954639434814, val_loss = 0.7934216856956482\n",
      "epoch n°690 : train_loss = 2.1167452335357666, val_loss = 0.7969322204589844\n",
      "epoch n°691 : train_loss = 2.1191694736480713, val_loss = 0.7919510006904602\n",
      "epoch n°692 : train_loss = 2.1064066886901855, val_loss = 0.7935172319412231\n",
      "epoch n°693 : train_loss = 2.1168627738952637, val_loss = 0.7949734926223755\n",
      "epoch n°694 : train_loss = 2.1301324367523193, val_loss = 0.7970654964447021\n",
      "epoch n°695 : train_loss = 2.117544174194336, val_loss = 0.795242965221405\n",
      "epoch n°696 : train_loss = 2.1193161010742188, val_loss = 0.7946631908416748\n",
      "epoch n°697 : train_loss = 2.122485399246216, val_loss = 0.799306333065033\n",
      "epoch n°698 : train_loss = 2.125600814819336, val_loss = 0.7950928807258606\n",
      "epoch n°699 : train_loss = 2.103325605392456, val_loss = 0.7921258211135864\n",
      "epoch n°700 : train_loss = 2.119626760482788, val_loss = 0.7963956594467163\n",
      "epoch n°701 : train_loss = 2.1063833236694336, val_loss = 0.7961697578430176\n",
      "epoch n°702 : train_loss = 2.124567985534668, val_loss = 0.7962577939033508\n",
      "epoch n°703 : train_loss = 2.1177358627319336, val_loss = 0.7944460511207581\n",
      "epoch n°704 : train_loss = 2.1105332374572754, val_loss = 0.7932927012443542\n",
      "epoch n°705 : train_loss = 2.114872455596924, val_loss = 0.7985018491744995\n",
      "epoch n°706 : train_loss = 2.1113312244415283, val_loss = 0.7937679886817932\n",
      "epoch n°707 : train_loss = 2.11491322517395, val_loss = 0.7965446710586548\n",
      "epoch n°708 : train_loss = 2.111673355102539, val_loss = 0.7985118627548218\n",
      "epoch n°709 : train_loss = 2.1113874912261963, val_loss = 0.798298180103302\n",
      "epoch n°710 : train_loss = 2.125171422958374, val_loss = 0.7981573939323425\n",
      "epoch n°711 : train_loss = 2.1167187690734863, val_loss = 0.7925110459327698\n",
      "epoch n°712 : train_loss = 2.1138978004455566, val_loss = 0.7973899245262146\n",
      "epoch n°713 : train_loss = 2.1094679832458496, val_loss = 0.7936150431632996\n",
      "epoch n°714 : train_loss = 2.113964319229126, val_loss = 0.7968742847442627\n",
      "epoch n°715 : train_loss = 2.113736391067505, val_loss = 0.7982720732688904\n",
      "epoch n°716 : train_loss = 2.1134660243988037, val_loss = 0.7958076000213623\n",
      "epoch n°717 : train_loss = 2.1145708560943604, val_loss = 0.7950029969215393\n",
      "epoch n°718 : train_loss = 2.10693097114563, val_loss = 0.7968796491622925\n",
      "epoch n°719 : train_loss = 2.10899019241333, val_loss = 0.7924631834030151\n",
      "epoch n°720 : train_loss = 2.1102523803710938, val_loss = 0.7937953472137451\n",
      "epoch n°721 : train_loss = 2.1127970218658447, val_loss = 0.7946511507034302\n",
      "epoch n°722 : train_loss = 2.111708879470825, val_loss = 0.7957231998443604\n",
      "epoch n°723 : train_loss = 2.12066912651062, val_loss = 0.795037031173706\n",
      "epoch n°724 : train_loss = 2.114027976989746, val_loss = 0.7944536805152893\n",
      "epoch n°725 : train_loss = 2.110323905944824, val_loss = 0.7974953055381775\n",
      "epoch n°726 : train_loss = 2.1166796684265137, val_loss = 0.7971704602241516\n",
      "epoch n°727 : train_loss = 2.1140761375427246, val_loss = 0.7915696501731873\n",
      "epoch n°728 : train_loss = 2.122731924057007, val_loss = 0.7952292561531067\n",
      "epoch n°729 : train_loss = 2.1127846240997314, val_loss = 0.7979302406311035\n",
      "epoch n°730 : train_loss = 2.1180403232574463, val_loss = 0.7978084683418274\n",
      "epoch n°731 : train_loss = 2.1108486652374268, val_loss = 0.7961403131484985\n",
      "epoch n°732 : train_loss = 2.1031928062438965, val_loss = 0.795466423034668\n",
      "epoch n°733 : train_loss = 2.106976270675659, val_loss = 0.7929916381835938\n",
      "epoch n°734 : train_loss = 2.120131015777588, val_loss = 0.7973392605781555\n",
      "epoch n°735 : train_loss = 2.111210823059082, val_loss = 0.7911236882209778\n",
      "epoch n°736 : train_loss = 2.1121180057525635, val_loss = 0.7942288517951965\n",
      "epoch n°737 : train_loss = 2.109813928604126, val_loss = 0.7945987582206726\n",
      "epoch n°738 : train_loss = 2.1164791584014893, val_loss = 0.7950983643531799\n",
      "epoch n°739 : train_loss = 2.102991819381714, val_loss = 0.7946812510490417\n",
      "epoch n°740 : train_loss = 2.1156165599823, val_loss = 0.7944993376731873\n",
      "epoch n°741 : train_loss = 2.110388994216919, val_loss = 0.7941709160804749\n",
      "epoch n°742 : train_loss = 2.105030059814453, val_loss = 0.7977656722068787\n",
      "epoch n°743 : train_loss = 2.1122148036956787, val_loss = 0.7997825741767883\n",
      "epoch n°744 : train_loss = 2.1054351329803467, val_loss = 0.8005452156066895\n",
      "epoch n°745 : train_loss = 2.101212978363037, val_loss = 0.793157160282135\n",
      "epoch n°746 : train_loss = 2.112355947494507, val_loss = 0.793562114238739\n",
      "epoch n°747 : train_loss = 2.108229398727417, val_loss = 0.7955136299133301\n",
      "epoch n°748 : train_loss = 2.112509250640869, val_loss = 0.7962305545806885\n",
      "epoch n°749 : train_loss = 2.102745294570923, val_loss = 0.7933773398399353\n",
      "epoch n°750 : train_loss = 2.1179611682891846, val_loss = 0.795048713684082\n",
      "epoch n°751 : train_loss = 2.105397939682007, val_loss = 0.7956773638725281\n",
      "epoch n°752 : train_loss = 2.106551170349121, val_loss = 0.7975929379463196\n",
      "epoch n°753 : train_loss = 2.1135880947113037, val_loss = 0.7943556904792786\n",
      "epoch n°754 : train_loss = 2.097318172454834, val_loss = 0.7994877696037292\n",
      "epoch n°755 : train_loss = 2.11246919631958, val_loss = 0.7994633913040161\n",
      "epoch n°756 : train_loss = 2.1118741035461426, val_loss = 0.7937272787094116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°757 : train_loss = 2.1102240085601807, val_loss = 0.7939848899841309\n",
      "epoch n°758 : train_loss = 2.103158950805664, val_loss = 0.7903590202331543\n",
      "epoch n°759 : train_loss = 2.11055064201355, val_loss = 0.7997207641601562\n",
      "epoch n°760 : train_loss = 2.104032516479492, val_loss = 0.7933548092842102\n",
      "epoch n°761 : train_loss = 2.1034817695617676, val_loss = 0.7982746958732605\n",
      "epoch n°762 : train_loss = 2.1103272438049316, val_loss = 0.7918920516967773\n",
      "epoch n°763 : train_loss = 2.097743034362793, val_loss = 0.7946990728378296\n",
      "epoch n°764 : train_loss = 2.1051902770996094, val_loss = 0.7975489497184753\n",
      "epoch n°765 : train_loss = 2.102043390274048, val_loss = 0.7921027541160583\n",
      "epoch n°766 : train_loss = 2.105410575866699, val_loss = 0.794224202632904\n",
      "epoch n°767 : train_loss = 2.100878953933716, val_loss = 0.7965028285980225\n",
      "epoch n°768 : train_loss = 2.107762336730957, val_loss = 0.7953813076019287\n",
      "epoch n°769 : train_loss = 2.1029186248779297, val_loss = 0.7898544073104858\n",
      "epoch n°770 : train_loss = 2.104811906814575, val_loss = 0.7918927073478699\n",
      "epoch n°771 : train_loss = 2.104912757873535, val_loss = 0.7954031229019165\n",
      "epoch n°772 : train_loss = 2.1117489337921143, val_loss = 0.791582465171814\n",
      "epoch n°773 : train_loss = 2.0968427658081055, val_loss = 0.7952309250831604\n",
      "epoch n°774 : train_loss = 2.100052833557129, val_loss = 0.7958060503005981\n",
      "epoch n°775 : train_loss = 2.107565402984619, val_loss = 0.7944643497467041\n",
      "epoch n°776 : train_loss = 2.0976548194885254, val_loss = 0.796275794506073\n",
      "epoch n°777 : train_loss = 2.098156690597534, val_loss = 0.7896594405174255\n",
      "epoch n°778 : train_loss = 2.1080970764160156, val_loss = 0.7987380027770996\n",
      "epoch n°779 : train_loss = 2.0985121726989746, val_loss = 0.792900562286377\n",
      "epoch n°780 : train_loss = 2.090982437133789, val_loss = 0.7972803115844727\n",
      "epoch n°781 : train_loss = 2.1032819747924805, val_loss = 0.7937855124473572\n",
      "epoch n°782 : train_loss = 2.1075589656829834, val_loss = 0.7951042056083679\n",
      "epoch n°783 : train_loss = 2.1113805770874023, val_loss = 0.7899530529975891\n",
      "epoch n°784 : train_loss = 2.1104226112365723, val_loss = 0.7961570024490356\n",
      "epoch n°785 : train_loss = 2.0981791019439697, val_loss = 0.7911182641983032\n",
      "epoch n°786 : train_loss = 2.1031479835510254, val_loss = 0.79196697473526\n",
      "epoch n°787 : train_loss = 2.1036038398742676, val_loss = 0.7911232709884644\n",
      "epoch n°788 : train_loss = 2.0988705158233643, val_loss = 0.7959843277931213\n",
      "epoch n°789 : train_loss = 2.100282907485962, val_loss = 0.7960428595542908\n",
      "epoch n°790 : train_loss = 2.100311279296875, val_loss = 0.7944112420082092\n",
      "epoch n°791 : train_loss = 2.100827693939209, val_loss = 0.792888879776001\n",
      "epoch n°792 : train_loss = 2.1046016216278076, val_loss = 0.7923032641410828\n",
      "epoch n°793 : train_loss = 2.098074436187744, val_loss = 0.7937870621681213\n",
      "epoch n°794 : train_loss = 2.1053085327148438, val_loss = 0.7942842245101929\n",
      "epoch n°795 : train_loss = 2.106053113937378, val_loss = 0.7936097383499146\n",
      "epoch n°796 : train_loss = 2.099630355834961, val_loss = 0.7969601154327393\n",
      "epoch n°797 : train_loss = 2.0985896587371826, val_loss = 0.7943159937858582\n",
      "epoch n°798 : train_loss = 2.0980446338653564, val_loss = 0.7930805087089539\n",
      "epoch n°799 : train_loss = 2.0995874404907227, val_loss = 0.7927502989768982\n",
      "epoch n°800 : train_loss = 2.1046860218048096, val_loss = 0.793290376663208\n",
      "epoch n°801 : train_loss = 2.094536066055298, val_loss = 0.7911022901535034\n",
      "epoch n°802 : train_loss = 2.0951249599456787, val_loss = 0.792766809463501\n",
      "epoch n°803 : train_loss = 2.0957062244415283, val_loss = 0.7917781472206116\n",
      "epoch n°804 : train_loss = 2.0922138690948486, val_loss = 0.7958555221557617\n",
      "epoch n°805 : train_loss = 2.097046136856079, val_loss = 0.7923850417137146\n",
      "epoch n°806 : train_loss = 2.091573476791382, val_loss = 0.792963445186615\n",
      "epoch n°807 : train_loss = 2.0977680683135986, val_loss = 0.7919439673423767\n",
      "epoch n°808 : train_loss = 2.0921337604522705, val_loss = 0.7904828190803528\n",
      "epoch n°809 : train_loss = 2.0975592136383057, val_loss = 0.7967371344566345\n",
      "epoch n°810 : train_loss = 2.0955708026885986, val_loss = 0.7952107787132263\n",
      "epoch n°811 : train_loss = 2.099710702896118, val_loss = 0.7932480573654175\n",
      "epoch n°812 : train_loss = 2.0939862728118896, val_loss = 0.7934852242469788\n",
      "epoch n°813 : train_loss = 2.096198558807373, val_loss = 0.7916629910469055\n",
      "epoch n°814 : train_loss = 2.095027208328247, val_loss = 0.7976483106613159\n",
      "epoch n°815 : train_loss = 2.101836919784546, val_loss = 0.8015363216400146\n",
      "epoch n°816 : train_loss = 2.1005544662475586, val_loss = 0.7924197316169739\n",
      "epoch n°817 : train_loss = 2.090977668762207, val_loss = 0.7963079214096069\n",
      "epoch n°818 : train_loss = 2.0929479598999023, val_loss = 0.7954378724098206\n",
      "epoch n°819 : train_loss = 2.0896878242492676, val_loss = 0.7964489459991455\n",
      "epoch n°820 : train_loss = 2.094618082046509, val_loss = 0.7946755290031433\n",
      "epoch n°821 : train_loss = 2.0916004180908203, val_loss = 0.7946721315383911\n",
      "epoch n°822 : train_loss = 2.088606357574463, val_loss = 0.7985712885856628\n",
      "epoch n°823 : train_loss = 2.101263999938965, val_loss = 0.7949262261390686\n",
      "epoch n°824 : train_loss = 2.0949630737304688, val_loss = 0.7952328324317932\n",
      "epoch n°825 : train_loss = 2.09957218170166, val_loss = 0.7930174469947815\n",
      "epoch n°826 : train_loss = 2.0974838733673096, val_loss = 0.7900359034538269\n",
      "epoch n°827 : train_loss = 2.0884199142456055, val_loss = 0.7933379411697388\n",
      "epoch n°828 : train_loss = 2.0863888263702393, val_loss = 0.7977427244186401\n",
      "epoch n°829 : train_loss = 2.094409942626953, val_loss = 0.793535590171814\n",
      "epoch n°830 : train_loss = 2.093749523162842, val_loss = 0.7962443232536316\n",
      "epoch n°831 : train_loss = 2.095494270324707, val_loss = 0.7879321575164795\n",
      "epoch n°832 : train_loss = 2.089165210723877, val_loss = 0.7878182530403137\n",
      "epoch n°833 : train_loss = 2.0942471027374268, val_loss = 0.7942623496055603\n",
      "epoch n°834 : train_loss = 2.0778181552886963, val_loss = 0.7958272099494934\n",
      "epoch n°835 : train_loss = 2.0906853675842285, val_loss = 0.793243944644928\n",
      "epoch n°836 : train_loss = 2.095982074737549, val_loss = 0.7917108535766602\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,4,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2,eta_min=3e-6)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_4L_bis.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=1008,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON TRAIN DATA : \n",
      "score_train = 0.743149995803833\n",
      "\n",
      "EVALUATING ON TEST DATA : \n",
      "score_test = 0.8088610172271729\n",
      "\n",
      "EVALUATING ON VAL DATA : \n",
      "score_val = 0.7928915619850159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = \"models/stateATT_4L_bis.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22da8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON TRAIN DATA : \n",
      "score_train = 0.7424394488334656\n",
      "\n",
      "EVALUATING ON TEST DATA : \n",
      "score_test = 0.8099067807197571\n",
      "\n",
      "EVALUATING ON VAL DATA : \n",
      "score_val = 0.7926374673843384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = \"models/stateATT_4L_bis_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "846e0e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = get_params(32,5,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2,eta_min=3e-6)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_5L_bis.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=1008,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f15a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e964674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_5L_bis.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_5L_bis_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2,eta_min=3e-6)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_6L_bis.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=1008,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209065c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_bis.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_bis_best.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55973a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a442d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
