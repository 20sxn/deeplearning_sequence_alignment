{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') #to avoid issues in the dataloading\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1378df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_SIZE = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20891a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 4\n",
    "rep = torch.tensor([4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0, 0, 0, 0, 0, 0, 0,0, 0])\n",
    "#max 8 \n",
    "rep = torch.tensor([8, 8, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 5, 7, 8, 8, 8, 8, 8])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0.2, 0.5, 0.3, 0.9, 0.8, 0.5, 0.9, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=2000,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by precomputing a bunch of data on the sequence families\n",
    "        \"\"\"\n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto',rep=rep,rand=rand): \n",
    "        \"\"\"\n",
    "        input idx of the family of the sample\n",
    "        return a Tensor containing several samples from the family corresponding to the index\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        bin_n = len(rep) #for biasing the sampling\n",
    "        \n",
    "        precomputed_pos = [] #positions of the amino-acids\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        #sampling more for big families and long sequences\n",
    "        if type(sample_size) != int:\n",
    "            sample_s = min(n * l,25_000)\n",
    "            coef = round((sample_s)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1): #computing PID and removing aligned '-' ##might need to compute the actual column num\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_j != '-' and aa_i != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0 #case 0/0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = [] #possible position to take\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "            \n",
    "            # biasing for more diverse PID  \n",
    "            bin_idx = int(PID_ij//(1/bin_n))\n",
    "            rep_number = rep[bin_idx].clone()\n",
    "            if torch.rand(1) < rand[bin_idx]:\n",
    "                rep_number+=1\n",
    "            \n",
    "            for _ in range(rep_number):\n",
    "                try:   \n",
    "                    k = np.random.choice(possible_k)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                #adding to the output\n",
    "                lengths.append(align_l)\n",
    "                pos_ij = (k + precomputed_pos)\n",
    "                pos.append(pos_ij)\n",
    "                \n",
    "                #computing the windows\n",
    "                window_i = ''\n",
    "                window_j = ''\n",
    "                for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                    if w < 0 or w >= align_l: #case of the edges\n",
    "                        window_i += 'Z'\n",
    "                        window_j += 'Z'\n",
    "                    else:\n",
    "                        window_i += seq_i[w]\n",
    "                        window_j += seq_j[w]\n",
    "\n",
    "                y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "                X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "\n",
    "                X.append(X_i)\n",
    "                y.append(y_j)\n",
    "                PIDs.append(PID_ij)\n",
    "                #computing the local PID\n",
    "                local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "\n",
    "                loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "                try:\n",
    "                    tmp = local_PID_ij/loc_comp  \n",
    "                except:\n",
    "                    tmp = 0 #case 0/0\n",
    "\n",
    "                local_PIDs.append(tmp)\n",
    "\n",
    "                assert y_j < 20\n",
    "                assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache() #clearing the cache\n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        out = X,y.long(),PIDs,local_PIDs,pos,lengths\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run This for new data\n",
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6,4000)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=2000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=2000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "#To load datasets with all features computed\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_dataset.cont_size = CONT_SIZE\n",
    "test_dataset.cont_size = CONT_SIZE\n",
    "val_dataset.cont_size = CONT_SIZE\n",
    "\n",
    "train_dataset.div = 2000\n",
    "test_dataset.div = 2000\n",
    "val_dataset.div = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Transforms a list of tensors to a batch tensor\n",
    "    \"\"\"\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[4] for item in batch],dim=0)\n",
    "    length = torch.cat([item[5] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID, pos, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06957f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w_tgt = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "\n",
    "        self.K_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w_src = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att_tgt = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        \n",
    "        Q_tgt = self.Q_w_tgt(tgt)\n",
    "        K_src = self.K_w_src(src)\n",
    "        V_src = self.V_w_src(src)\n",
    "        \n",
    "        out,_ = self.att_tgt(Q_tgt,K_src,V_src,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dccfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttQBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w_Q = nn.Linear(20,num_heads*head_dims,bias=False)\n",
    "\n",
    "        self.K_w_x = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w_x = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att_tgt = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "\n",
    "        self.lin = nn.Linear(num_heads*head_dims,20)\n",
    "        \n",
    "    def forward(self,x,Q):\n",
    "        \n",
    "        Q_Q = self.Q_w_Q(Q)\n",
    "        K_x = self.K_w_x(x)\n",
    "        V_x = self.V_w_x(x)\n",
    "        \n",
    "        out,_ = self.att_tgt(Q_Q,K_x,V_x,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb64bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastCrossAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        src,tgt = x[:,0],x[:,1]\n",
    "        Q = self.Q_w(tgt)\n",
    "        K = self.K_w(src)\n",
    "        V = self.V_w(src)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1da4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Row-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*n,l,d))).view(b,n,l,d)\n",
    "        return out\n",
    "    \n",
    "class ColAttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Column-wise Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = self.att_block(x.view((b*l,n,d))).view((b,n,l,d))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d39d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward2D(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or 2*in_features\n",
    "        hidden_dim = wide_factor * 2*in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(2*in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        b,n,l,d = x.shape\n",
    "        out = x.view(b,l,n*d)\n",
    "        out = self.lin1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        return out.view(b,n,l,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(from the timm library)\n",
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a6c6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.row_att_block = RowAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.cross_att_block = CrossAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff2d = FeedFoward2D(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.norm3 = nn.LayerNorm(in_features)\n",
    "        self.norm4 = nn.LayerNorm(in_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.row_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        out[:,1] = out[:,1] + self.drop_path(self.cross_att_block(out))\n",
    "        out = self.norm3(out)\n",
    "        out = out + self.drop_path(self.ff2d(out))\n",
    "        out = self.norm4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd38d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Last Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.row_att_block = RowAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.cross_att_block = CrossAttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff2 = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "        self.norm3 = nn.LayerNorm(in_features)\n",
    "        self.norm4 = nn.LayerNorm(in_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.row_att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        out = out[:,1] + self.drop_path(self.cross_att_block(out))\n",
    "        out = self.norm3(out)\n",
    "        out = out + self.drop_path(self.ff2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20059eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549ec3ee",
   "metadata": {},
   "source": [
    "ADD Linear + Act d-wise before clf_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5815535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_block(nn.Module):\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        self.cross_att_block = CrossAttQBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.act = torch.nn.Softplus()\n",
    "    def forward(self,x,Q):\n",
    "        Q = Q + self.cross_att_block(x,Q)\n",
    "        \n",
    "        Q = self.act(Q) #so that every element of Q is >=0\n",
    "        \n",
    "        PI = torch.diagonal(Q,dim1=-2,dim2=-1).clone() #makes a copy of the diagonal\n",
    "        PI = F.normalize(PI,p=1,dim=1)  # and normalize it to make probabilities\n",
    "        \n",
    "        #pointer manipulation\n",
    "        tmp = torch.diagonal(Q,dim1=-2,dim2=-1) \n",
    "        tmp -= tmp #set Q diagonal to 0\n",
    "        tmp -= torch.sum(Q,dim=2) #sum over rows = 0\n",
    "        \n",
    "        mu = -(PI*torch.diagonal(Q,dim1=-2,dim2=-1)).sum(dim=1).view(-1,1,1) #normalization factor\n",
    "        Q = Q/mu\n",
    "        return Q\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier Head of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        layers.append(nn.Softplus())\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    \"\"\"\n",
    "    Returns the initialization parameters of the Transformer\n",
    "    \"\"\"\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "085855a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-like neural net\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,num_seq=2,seq_len=2*CONT_SIZE+2,clf_dims=[256,64],cont_size=CONT_SIZE):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        blocks = []\n",
    "        r = min(len(num_heads),len(head_dims),len(wide_factors),len(drops))\n",
    "        for i,(n_h, h_d,w,d) in enumerate(zip(num_heads,head_dims,wide_factors,drops)):\n",
    "            if i < (r-1):\n",
    "                blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "            else:\n",
    "                blocks.append(LastBlock(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.dist = Classifier_Head(in_features,clf_dims,out_size=1,seq_len=seq_len)\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,2*in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "        \n",
    "        self.Qp = nn.Parameter(torch.randn(20,20))\n",
    "        self.Q_b = Q_block(in_features)\n",
    "        \n",
    "    def to_input(self,x,PID,pos,length):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1)\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(32**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(32**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "        \n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        \n",
    "        out = torch.stack([seq1,seq2],dim=1)\n",
    "        \n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        pid = rearrange(pid,\"b 1 (n e) -> b n 1 e\",n=2)\n",
    "        \n",
    "        out = torch.cat([out,pid],dim=2)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def forward(self,x,PID,pos,length):\n",
    "        X_input = self.to_input(x,PID,pos,length)\n",
    "        b = X_input.shape[0]\n",
    "        Q = repeat(self.Qp,\"c d -> b c d\",b=b)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        t = self.dist(features)\n",
    "        Q = self.Q_b(features,Q)\n",
    "        x_aa = repeat(x[:,31,:20],'b d -> b d e',e=20)\n",
    "        y_pred = (torch.matrix_exp(Q*t.view(-1,1,1))*x_aa).sum(dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Used for checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "3b97a85f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "AttNet                                             [1, 20]                   800\n",
       "├─Sequential: 1-1                                  [1, 64]                   --\n",
       "│    └─Linear: 2-1                                 [1, 64]                   128\n",
       "│    └─Sigmoid: 2-2                                [1, 64]                   --\n",
       "├─Sequential: 1-2                                  [1, 64, 32]               --\n",
       "│    └─Block: 2-3                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-1                       [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-2                          [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-3                         [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward: 3-4                        [1, 2, 64, 32]            8,352\n",
       "│    │    └─DropPath: 3-5                          [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-6                         [1, 2, 64, 32]            64\n",
       "│    │    └─CrossAttBlock: 3-7                     [1, 64, 32]               295,968\n",
       "│    │    └─DropPath: 3-8                          [1, 64, 32]               --\n",
       "│    │    └─LayerNorm: 3-9                         [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-10                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-11                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-12                        [1, 2, 64, 32]            64\n",
       "│    └─Block: 2-4                                  [1, 2, 64, 32]            --\n",
       "│    │    └─RowAttBlock: 3-13                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-14                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-15                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward: 3-16                       [1, 2, 64, 32]            8,352\n",
       "│    │    └─DropPath: 3-17                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-18                        [1, 2, 64, 32]            64\n",
       "│    │    └─CrossAttBlock: 3-19                    [1, 64, 32]               295,968\n",
       "│    │    └─DropPath: 3-20                         [1, 64, 32]               --\n",
       "│    │    └─LayerNorm: 3-21                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward2D: 3-22                     [1, 2, 64, 32]            33,088\n",
       "│    │    └─DropPath: 3-23                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-24                        [1, 2, 64, 32]            64\n",
       "│    └─LastBlock: 2-5                              [1, 64, 32]               64\n",
       "│    │    └─RowAttBlock: 3-25                      [1, 2, 64, 32]            295,968\n",
       "│    │    └─DropPath: 3-26                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-27                        [1, 2, 64, 32]            64\n",
       "│    │    └─FeedFoward: 3-28                       [1, 2, 64, 32]            8,352\n",
       "│    │    └─DropPath: 3-29                         [1, 2, 64, 32]            --\n",
       "│    │    └─LayerNorm: 3-30                        [1, 2, 64, 32]            64\n",
       "│    │    └─CrossAttBlock: 3-31                    [1, 64, 32]               295,968\n",
       "│    │    └─DropPath: 3-32                         [1, 64, 32]               --\n",
       "│    │    └─LayerNorm: 3-33                        [1, 64, 32]               64\n",
       "│    │    └─FeedFoward: 3-34                       [1, 64, 32]               8,352\n",
       "│    │    └─DropPath: 3-35                         [1, 64, 32]               --\n",
       "├─Classifier_Head: 1-3                             [1, 1]                    --\n",
       "│    └─Sequential: 2-6                             [1, 1]                    --\n",
       "│    │    └─Linear: 3-36                           [1, 512]                  1,049,088\n",
       "│    │    └─GELU: 3-37                             [1, 512]                  --\n",
       "│    │    └─Dropout: 3-38                          [1, 512]                  --\n",
       "│    │    └─Linear: 3-39                           [1, 128]                  65,664\n",
       "│    │    └─GELU: 3-40                             [1, 128]                  --\n",
       "│    │    └─Dropout: 3-41                          [1, 128]                  --\n",
       "│    │    └─Linear: 3-42                           [1, 32]                   4,128\n",
       "│    │    └─GELU: 3-43                             [1, 32]                   --\n",
       "│    │    └─Dropout: 3-44                          [1, 32]                   --\n",
       "│    │    └─Linear: 3-45                           [1, 1]                    33\n",
       "│    │    └─Softplus: 3-46                         [1, 1]                    --\n",
       "├─Q_block: 1-4                                     [1, 20, 20]               --\n",
       "│    └─CrossAttQBlock: 2-7                         [1, 20, 20]               --\n",
       "│    │    └─Linear: 3-47                           [1, 20, 192]              3,840\n",
       "│    │    └─Linear: 3-48                           [1, 64, 192]              6,144\n",
       "│    │    └─Linear: 3-49                           [1, 64, 192]              6,144\n",
       "│    │    └─MultiheadAttention: 3-50               [1, 20, 192]              148,224\n",
       "│    │    └─Linear: 3-51                           [1, 20, 20]               3,860\n",
       "│    └─Softplus: 2-8                               [1, 20, 20]               --\n",
       "│    └─Softplus: 2-9                               [1, 20]                   --\n",
       "====================================================================================================\n",
       "Total params: 3,164,213\n",
       "Trainable params: 3,164,213\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.53\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 5.17\n",
       "Params size (MB): 5.74\n",
       "Estimated Total Size (MB): 10.92\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,3,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32])\n",
    "torchinfo.summary(model,[(1,4*CONT_SIZE+1,21),(1,),(1,4*CONT_SIZE+1),(1,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID, pos,length in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos, length in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0982994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4d60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_acc_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "                \n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID,lPID,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Accuracy wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True,cont_size=CONT_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    \"\"\"\n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.NLLLoss(reduction='sum')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pos,length in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            length = length.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pos,length)\n",
    "            l = Loss(torch.log(y_hat),y)/440 \n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID, pos,length in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pos,length)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a1e75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm \"models/state_Q_mat.pth\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.3159232139587402, val_loss = 0.7510738372802734\n",
      "epoch n°1 : train_loss = 2.1603641510009766, val_loss = 0.7385901212692261\n",
      "epoch n°2 : train_loss = 2.1158864498138428, val_loss = 0.7316387295722961\n",
      "epoch n°3 : train_loss = 2.110595941543579, val_loss = 0.7268059849739075\n",
      "epoch n°4 : train_loss = 2.095601797103882, val_loss = 0.7316327691078186\n",
      "epoch n°5 : train_loss = 2.0964279174804688, val_loss = 0.7285582423210144\n",
      "epoch n°6 : train_loss = 2.0779104232788086, val_loss = 0.7288389801979065\n",
      "epoch n°7 : train_loss = 2.087352752685547, val_loss = 0.7311847805976868\n",
      "epoch n°8 : train_loss = 2.077056407928467, val_loss = 0.7228968143463135\n",
      "epoch n°9 : train_loss = 2.076732873916626, val_loss = 0.7345440983772278\n",
      "epoch n°10 : train_loss = 2.063638925552368, val_loss = 0.7324970364570618\n",
      "epoch n°11 : train_loss = 2.0678870677948, val_loss = 0.7178199291229248\n",
      "epoch n°12 : train_loss = 2.0693018436431885, val_loss = 0.7304043769836426\n",
      "epoch n°13 : train_loss = 2.0696938037872314, val_loss = 0.7229526042938232\n",
      "epoch n°14 : train_loss = 2.076563835144043, val_loss = 0.7259109020233154\n",
      "epoch n°15 : train_loss = 2.0643088817596436, val_loss = 0.7229968905448914\n",
      "epoch n°16 : train_loss = 2.0834169387817383, val_loss = 0.7193483710289001\n",
      "epoch n°17 : train_loss = 2.083327054977417, val_loss = 0.7199599742889404\n",
      "epoch n°18 : train_loss = 2.077885627746582, val_loss = 0.7255952954292297\n",
      "epoch n°19 : train_loss = 2.0763397216796875, val_loss = 0.726077139377594\n",
      "epoch n°20 : train_loss = 2.080256223678589, val_loss = 0.7176699638366699\n",
      "epoch n°21 : train_loss = 2.073765754699707, val_loss = 0.7249993085861206\n",
      "epoch n°22 : train_loss = 2.079986810684204, val_loss = 0.7252301573753357\n",
      "epoch n°23 : train_loss = 2.0684311389923096, val_loss = 0.7235502600669861\n",
      "epoch n°24 : train_loss = 2.0679574012756348, val_loss = 0.7214715480804443\n",
      "epoch n°25 : train_loss = 2.0652384757995605, val_loss = 0.7186974883079529\n",
      "epoch n°26 : train_loss = 2.0617809295654297, val_loss = 0.7193273305892944\n",
      "epoch n°27 : train_loss = 2.0723631381988525, val_loss = 0.7247118949890137\n",
      "epoch n°28 : train_loss = 2.0620903968811035, val_loss = 0.7266533374786377\n",
      "epoch n°29 : train_loss = 2.065011501312256, val_loss = 0.7192992568016052\n",
      "epoch n°30 : train_loss = 2.0727102756500244, val_loss = 0.7155417799949646\n",
      "epoch n°31 : train_loss = 2.0518062114715576, val_loss = 0.7287147045135498\n",
      "epoch n°32 : train_loss = 2.058201551437378, val_loss = 0.7198328971862793\n",
      "epoch n°33 : train_loss = 2.0538668632507324, val_loss = 0.717901885509491\n",
      "epoch n°34 : train_loss = 2.0643575191497803, val_loss = 0.7213431596755981\n",
      "epoch n°35 : train_loss = 2.061208724975586, val_loss = 0.7190917134284973\n",
      "epoch n°36 : train_loss = 2.0576462745666504, val_loss = 0.7184611558914185\n",
      "epoch n°37 : train_loss = 2.0632166862487793, val_loss = 0.7149137854576111\n",
      "epoch n°38 : train_loss = 2.0612921714782715, val_loss = 0.7162598371505737\n",
      "epoch n°39 : train_loss = 2.0500195026397705, val_loss = 0.7169053554534912\n",
      "epoch n°40 : train_loss = 2.0512101650238037, val_loss = 0.717956006526947\n",
      "epoch n°41 : train_loss = 2.050755739212036, val_loss = 0.723392128944397\n",
      "epoch n°42 : train_loss = 2.0643298625946045, val_loss = 0.7244186997413635\n",
      "epoch n°43 : train_loss = 2.0510001182556152, val_loss = 0.7244934439659119\n",
      "epoch n°44 : train_loss = 2.0501151084899902, val_loss = 0.7155473232269287\n",
      "epoch n°45 : train_loss = 2.050121784210205, val_loss = 0.7175251841545105\n",
      "epoch n°46 : train_loss = 2.056668281555176, val_loss = 0.7292360067367554\n",
      "epoch n°47 : train_loss = 2.045180559158325, val_loss = 0.7199021577835083\n",
      "epoch n°48 : train_loss = 2.0492606163024902, val_loss = 0.7197533249855042\n",
      "epoch n°49 : train_loss = 2.0580506324768066, val_loss = 0.7232772707939148\n",
      "epoch n°50 : train_loss = 2.0559442043304443, val_loss = 0.7154645323753357\n",
      "epoch n°51 : train_loss = 2.0702884197235107, val_loss = 0.7225814461708069\n",
      "epoch n°52 : train_loss = 2.0628538131713867, val_loss = 0.7185850739479065\n",
      "epoch n°53 : train_loss = 2.056441068649292, val_loss = 0.7111698389053345\n",
      "epoch n°54 : train_loss = 2.0535812377929688, val_loss = 0.7212877869606018\n",
      "epoch n°55 : train_loss = 2.053816795349121, val_loss = 0.720501720905304\n",
      "epoch n°56 : train_loss = 2.0575499534606934, val_loss = 0.7245098948478699\n",
      "epoch n°57 : train_loss = 2.047683000564575, val_loss = 0.7107425332069397\n",
      "epoch n°58 : train_loss = 2.0488860607147217, val_loss = 0.7228241562843323\n",
      "epoch n°59 : train_loss = 2.049421787261963, val_loss = 0.723125696182251\n",
      "epoch n°60 : train_loss = 2.0504233837127686, val_loss = 0.7200153470039368\n",
      "epoch n°61 : train_loss = 2.045146942138672, val_loss = 0.7188683152198792\n",
      "epoch n°62 : train_loss = 2.050462007522583, val_loss = 0.7111048698425293\n",
      "epoch n°63 : train_loss = 2.0386719703674316, val_loss = 0.714140772819519\n",
      "epoch n°64 : train_loss = 2.0566978454589844, val_loss = 0.7168834805488586\n",
      "epoch n°65 : train_loss = 2.0452160835266113, val_loss = 0.7188395857810974\n",
      "epoch n°66 : train_loss = 2.0395753383636475, val_loss = 0.7118716835975647\n",
      "epoch n°67 : train_loss = 2.045663356781006, val_loss = 0.7174897193908691\n",
      "epoch n°68 : train_loss = 2.045649528503418, val_loss = 0.7180930376052856\n",
      "epoch n°69 : train_loss = 2.0428566932678223, val_loss = 0.7153968214988708\n",
      "epoch n°70 : train_loss = 2.0417768955230713, val_loss = 0.7155153155326843\n",
      "epoch n°71 : train_loss = 2.037855625152588, val_loss = 0.7125462889671326\n",
      "epoch n°72 : train_loss = 2.0417897701263428, val_loss = 0.714627742767334\n",
      "epoch n°73 : train_loss = 2.028085708618164, val_loss = 0.7077983617782593\n",
      "epoch n°74 : train_loss = 2.039459228515625, val_loss = 0.7124781012535095\n",
      "epoch n°75 : train_loss = 2.033320188522339, val_loss = 0.7087774276733398\n",
      "epoch n°76 : train_loss = 2.043868064880371, val_loss = 0.7057746052742004\n",
      "epoch n°77 : train_loss = 2.0352346897125244, val_loss = 0.714484691619873\n",
      "epoch n°78 : train_loss = 2.030677556991577, val_loss = 0.7102246880531311\n",
      "epoch n°79 : train_loss = 2.032443046569824, val_loss = 0.7150446772575378\n",
      "epoch n°80 : train_loss = 2.038627862930298, val_loss = 0.7133444547653198\n",
      "epoch n°81 : train_loss = 2.0361642837524414, val_loss = 0.7134919166564941\n",
      "epoch n°82 : train_loss = 2.036329984664917, val_loss = 0.7088270783424377\n",
      "epoch n°83 : train_loss = 2.021138906478882, val_loss = 0.7052604556083679\n",
      "epoch n°84 : train_loss = 2.0248258113861084, val_loss = 0.7057639360427856\n",
      "epoch n°85 : train_loss = 2.0198585987091064, val_loss = 0.7046020030975342\n",
      "epoch n°86 : train_loss = 2.032907009124756, val_loss = 0.7088673114776611\n",
      "epoch n°87 : train_loss = 2.032238721847534, val_loss = 0.7046974301338196\n",
      "epoch n°88 : train_loss = 2.039980888366699, val_loss = 0.7042414546012878\n",
      "epoch n°89 : train_loss = 2.0272107124328613, val_loss = 0.7152169346809387\n",
      "epoch n°90 : train_loss = 2.0365965366363525, val_loss = 0.7158094048500061\n",
      "epoch n°91 : train_loss = 2.040104866027832, val_loss = 0.7146400213241577\n",
      "epoch n°92 : train_loss = 2.0263636112213135, val_loss = 0.7115592360496521\n",
      "epoch n°93 : train_loss = 2.0240516662597656, val_loss = 0.712801456451416\n",
      "epoch n°94 : train_loss = 2.023864269256592, val_loss = 0.7176545262336731\n",
      "epoch n°95 : train_loss = 2.019639253616333, val_loss = 0.7055513858795166\n",
      "epoch n°96 : train_loss = 2.0343008041381836, val_loss = 0.7094860076904297\n",
      "epoch n°97 : train_loss = 2.0224063396453857, val_loss = 0.7061076760292053\n",
      "epoch n°98 : train_loss = 2.022942304611206, val_loss = 0.706676185131073\n",
      "epoch n°99 : train_loss = 2.0358965396881104, val_loss = 0.7082776427268982\n",
      "epoch n°100 : train_loss = 2.0279541015625, val_loss = 0.7059025764465332\n",
      "epoch n°101 : train_loss = 2.0145795345306396, val_loss = 0.7044850587844849\n",
      "epoch n°102 : train_loss = 2.016439199447632, val_loss = 0.7058548331260681\n",
      "epoch n°103 : train_loss = 2.028228282928467, val_loss = 0.7127484083175659\n",
      "epoch n°104 : train_loss = 2.010874032974243, val_loss = 0.7063136100769043\n",
      "epoch n°105 : train_loss = 2.0241854190826416, val_loss = 0.7095084190368652\n",
      "epoch n°106 : train_loss = 2.022757053375244, val_loss = 0.7070611715316772\n",
      "epoch n°107 : train_loss = 2.0228941440582275, val_loss = 0.7133896350860596\n",
      "epoch n°108 : train_loss = 2.02683162689209, val_loss = 0.7075634598731995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 2.0168397426605225, val_loss = 0.7029383182525635\n",
      "epoch n°110 : train_loss = 2.0184459686279297, val_loss = 0.7088371515274048\n",
      "epoch n°111 : train_loss = 2.022366762161255, val_loss = 0.7085182070732117\n",
      "epoch n°112 : train_loss = 2.0330209732055664, val_loss = 0.7122586965560913\n",
      "epoch n°113 : train_loss = 2.046315908432007, val_loss = 0.7105733752250671\n",
      "epoch n°114 : train_loss = 2.03391170501709, val_loss = 0.7129029631614685\n",
      "epoch n°115 : train_loss = 2.026707887649536, val_loss = 0.7070639133453369\n",
      "epoch n°116 : train_loss = 2.0427279472351074, val_loss = 0.7137728333473206\n",
      "epoch n°117 : train_loss = 2.035778522491455, val_loss = 0.7101156711578369\n",
      "epoch n°118 : train_loss = 2.0336146354675293, val_loss = 0.7106419205665588\n",
      "epoch n°119 : train_loss = 2.031162738800049, val_loss = 0.71473228931427\n",
      "epoch n°120 : train_loss = 2.0402016639709473, val_loss = 0.7076798677444458\n",
      "epoch n°121 : train_loss = 2.03039813041687, val_loss = 0.7104827165603638\n",
      "epoch n°122 : train_loss = 2.030503034591675, val_loss = 0.7116031646728516\n",
      "epoch n°123 : train_loss = 2.0421128273010254, val_loss = 0.7133142948150635\n",
      "epoch n°124 : train_loss = 2.033581495285034, val_loss = 0.7180503010749817\n",
      "epoch n°125 : train_loss = 2.0330889225006104, val_loss = 0.70714271068573\n",
      "epoch n°126 : train_loss = 2.0289642810821533, val_loss = 0.7151363492012024\n",
      "epoch n°127 : train_loss = 2.025696039199829, val_loss = 0.7114382386207581\n",
      "epoch n°128 : train_loss = 2.027963638305664, val_loss = 0.7140780687332153\n",
      "epoch n°129 : train_loss = 2.032021999359131, val_loss = 0.7089136838912964\n",
      "epoch n°130 : train_loss = 2.046088218688965, val_loss = 0.7120989561080933\n",
      "epoch n°131 : train_loss = 2.0339648723602295, val_loss = 0.7098345160484314\n",
      "epoch n°132 : train_loss = 2.0339066982269287, val_loss = 0.705843448638916\n",
      "epoch n°133 : train_loss = 2.042391300201416, val_loss = 0.7077348828315735\n",
      "epoch n°134 : train_loss = 2.0292654037475586, val_loss = 0.7087532877922058\n",
      "epoch n°135 : train_loss = 2.027597665786743, val_loss = 0.7209599018096924\n",
      "epoch n°136 : train_loss = 2.021056890487671, val_loss = 0.7117611169815063\n",
      "epoch n°137 : train_loss = 2.022447109222412, val_loss = 0.7094159722328186\n",
      "epoch n°138 : train_loss = 2.0407490730285645, val_loss = 0.7093545794487\n",
      "epoch n°139 : train_loss = 2.030592679977417, val_loss = 0.707493245601654\n",
      "epoch n°140 : train_loss = 2.030600070953369, val_loss = 0.7113673090934753\n",
      "epoch n°141 : train_loss = 2.0219507217407227, val_loss = 0.7148711681365967\n",
      "epoch n°142 : train_loss = 2.029252529144287, val_loss = 0.7078725695610046\n",
      "epoch n°143 : train_loss = 2.0280508995056152, val_loss = 0.7069042921066284\n",
      "epoch n°144 : train_loss = 2.0214569568634033, val_loss = 0.7175610661506653\n",
      "epoch n°145 : train_loss = 2.022696018218994, val_loss = 0.7104476094245911\n",
      "epoch n°146 : train_loss = 2.023214817047119, val_loss = 0.7081491947174072\n",
      "epoch n°147 : train_loss = 2.031859874725342, val_loss = 0.7079111337661743\n",
      "epoch n°148 : train_loss = 2.035148859024048, val_loss = 0.7121346592903137\n",
      "epoch n°149 : train_loss = 2.0156707763671875, val_loss = 0.7127238512039185\n",
      "epoch n°150 : train_loss = 2.0299673080444336, val_loss = 0.7079015374183655\n",
      "epoch n°151 : train_loss = 2.026479959487915, val_loss = 0.7103551626205444\n",
      "epoch n°152 : train_loss = 2.0342001914978027, val_loss = 0.7117179036140442\n",
      "epoch n°153 : train_loss = 2.0258660316467285, val_loss = 0.7059106826782227\n",
      "epoch n°154 : train_loss = 2.0360512733459473, val_loss = 0.7172316312789917\n",
      "epoch n°155 : train_loss = 2.0283219814300537, val_loss = 0.6986632347106934\n",
      "epoch n°156 : train_loss = 2.0275940895080566, val_loss = 0.712877631187439\n",
      "epoch n°157 : train_loss = 2.0274055004119873, val_loss = 0.7069873213768005\n",
      "epoch n°158 : train_loss = 2.0276200771331787, val_loss = 0.7094720005989075\n",
      "epoch n°159 : train_loss = 2.026154041290283, val_loss = 0.7099087834358215\n",
      "epoch n°160 : train_loss = 2.0225727558135986, val_loss = 0.7153371572494507\n",
      "epoch n°161 : train_loss = 2.0233843326568604, val_loss = 0.7107043266296387\n",
      "epoch n°162 : train_loss = 2.028532028198242, val_loss = 0.7091347575187683\n",
      "epoch n°163 : train_loss = 2.0275826454162598, val_loss = 0.7106310129165649\n",
      "epoch n°164 : train_loss = 2.0231852531433105, val_loss = 0.7075334191322327\n",
      "epoch n°165 : train_loss = 2.01617693901062, val_loss = 0.715176522731781\n",
      "epoch n°166 : train_loss = 2.0261948108673096, val_loss = 0.7086828947067261\n",
      "epoch n°167 : train_loss = 2.018336296081543, val_loss = 0.7099325656890869\n",
      "epoch n°168 : train_loss = 2.0256381034851074, val_loss = 0.7112352252006531\n",
      "epoch n°169 : train_loss = 2.026137113571167, val_loss = 0.7103837132453918\n",
      "epoch n°170 : train_loss = 2.0179901123046875, val_loss = 0.709246039390564\n",
      "epoch n°171 : train_loss = 2.021955728530884, val_loss = 0.7099295854568481\n",
      "epoch n°172 : train_loss = 2.0223660469055176, val_loss = 0.7098474502563477\n",
      "epoch n°173 : train_loss = 2.024660110473633, val_loss = 0.7023333311080933\n",
      "epoch n°174 : train_loss = 2.0155482292175293, val_loss = 0.7118536829948425\n",
      "epoch n°175 : train_loss = 2.013162136077881, val_loss = 0.7117695808410645\n",
      "epoch n°176 : train_loss = 2.0040628910064697, val_loss = 0.7115955352783203\n",
      "epoch n°177 : train_loss = 2.02227783203125, val_loss = 0.7091506123542786\n",
      "epoch n°178 : train_loss = 2.005314588546753, val_loss = 0.7010986804962158\n",
      "epoch n°179 : train_loss = 2.0122060775756836, val_loss = 0.708004891872406\n",
      "epoch n°180 : train_loss = 2.0284945964813232, val_loss = 0.7069535255432129\n",
      "epoch n°181 : train_loss = 2.026627779006958, val_loss = 0.7122597098350525\n",
      "epoch n°182 : train_loss = 2.0146968364715576, val_loss = 0.7064221501350403\n",
      "epoch n°183 : train_loss = 2.0222864151000977, val_loss = 0.705902636051178\n",
      "epoch n°184 : train_loss = 2.0178751945495605, val_loss = 0.7082916498184204\n",
      "epoch n°185 : train_loss = 2.0142414569854736, val_loss = 0.7122209668159485\n",
      "epoch n°186 : train_loss = 2.002202272415161, val_loss = 0.711712121963501\n",
      "epoch n°187 : train_loss = 2.0075104236602783, val_loss = 0.7103383541107178\n",
      "epoch n°188 : train_loss = 2.0099000930786133, val_loss = 0.7016591429710388\n",
      "epoch n°189 : train_loss = 2.01413631439209, val_loss = 0.7119002938270569\n",
      "epoch n°190 : train_loss = 2.0177364349365234, val_loss = 0.7099230289459229\n",
      "epoch n°191 : train_loss = 2.0032029151916504, val_loss = 0.7115029096603394\n",
      "epoch n°192 : train_loss = 2.0105555057525635, val_loss = 0.7122488617897034\n",
      "epoch n°193 : train_loss = 2.0191056728363037, val_loss = 0.7028918266296387\n",
      "epoch n°194 : train_loss = 2.020833969116211, val_loss = 0.7123599052429199\n",
      "epoch n°195 : train_loss = 2.0191314220428467, val_loss = 0.7054973840713501\n",
      "epoch n°196 : train_loss = 2.009953260421753, val_loss = 0.7091347575187683\n",
      "epoch n°197 : train_loss = 2.0227465629577637, val_loss = 0.6994970440864563\n",
      "epoch n°198 : train_loss = 2.0214362144470215, val_loss = 0.7069936990737915\n",
      "epoch n°199 : train_loss = 2.015857696533203, val_loss = 0.7113980650901794\n",
      "epoch n°200 : train_loss = 2.001027822494507, val_loss = 0.7052433490753174\n",
      "epoch n°201 : train_loss = 2.0049524307250977, val_loss = 0.7039946913719177\n",
      "epoch n°202 : train_loss = 2.0088672637939453, val_loss = 0.7094444632530212\n",
      "epoch n°203 : train_loss = 2.003516674041748, val_loss = 0.7049581408500671\n",
      "epoch n°204 : train_loss = 2.013754367828369, val_loss = 0.7097168564796448\n",
      "epoch n°205 : train_loss = 2.017033338546753, val_loss = 0.7031245827674866\n",
      "epoch n°206 : train_loss = 2.0239923000335693, val_loss = 0.7060456275939941\n",
      "epoch n°207 : train_loss = 2.008108139038086, val_loss = 0.7045904994010925\n",
      "epoch n°208 : train_loss = 2.010331153869629, val_loss = 0.7054452896118164\n",
      "epoch n°209 : train_loss = 2.021392822265625, val_loss = 0.7132430076599121\n",
      "epoch n°210 : train_loss = 2.0069496631622314, val_loss = 0.7091521620750427\n",
      "epoch n°211 : train_loss = 2.0020992755889893, val_loss = 0.7049768567085266\n",
      "epoch n°212 : train_loss = 2.012942314147949, val_loss = 0.704602062702179\n",
      "epoch n°213 : train_loss = 2.0174336433410645, val_loss = 0.7025020122528076\n",
      "epoch n°214 : train_loss = 2.020620822906494, val_loss = 0.7136020660400391\n",
      "epoch n°215 : train_loss = 2.018657922744751, val_loss = 0.7065633535385132\n",
      "epoch n°216 : train_loss = 2.020660161972046, val_loss = 0.7130797505378723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°217 : train_loss = 2.008204698562622, val_loss = 0.7127100825309753\n",
      "epoch n°218 : train_loss = 2.0108134746551514, val_loss = 0.7073202729225159\n",
      "epoch n°219 : train_loss = 2.019810676574707, val_loss = 0.7042711973190308\n",
      "epoch n°220 : train_loss = 2.002593517303467, val_loss = 0.7081671953201294\n",
      "epoch n°221 : train_loss = 1.9981244802474976, val_loss = 0.7090455889701843\n",
      "epoch n°222 : train_loss = 2.0195183753967285, val_loss = 0.7052385807037354\n",
      "epoch n°223 : train_loss = 2.006972551345825, val_loss = 0.7122553586959839\n",
      "epoch n°224 : train_loss = 2.011356830596924, val_loss = 0.7090211510658264\n",
      "epoch n°225 : train_loss = 2.003905773162842, val_loss = 0.702796220779419\n",
      "epoch n°226 : train_loss = 2.0080318450927734, val_loss = 0.7093393802642822\n",
      "epoch n°227 : train_loss = 2.011131763458252, val_loss = 0.7031147480010986\n",
      "epoch n°228 : train_loss = 1.9994009733200073, val_loss = 0.7014696002006531\n",
      "epoch n°229 : train_loss = 1.9979100227355957, val_loss = 0.7091823220252991\n",
      "epoch n°230 : train_loss = 2.0061419010162354, val_loss = 0.7071672677993774\n",
      "epoch n°231 : train_loss = 2.014592170715332, val_loss = 0.7042799592018127\n",
      "epoch n°232 : train_loss = 2.0158703327178955, val_loss = 0.7059267163276672\n",
      "epoch n°233 : train_loss = 2.0075807571411133, val_loss = 0.7098099589347839\n",
      "epoch n°234 : train_loss = 2.016291856765747, val_loss = 0.71055006980896\n",
      "epoch n°235 : train_loss = 2.009420394897461, val_loss = 0.7095418572425842\n",
      "epoch n°236 : train_loss = 2.0077013969421387, val_loss = 0.7053800225257874\n",
      "epoch n°237 : train_loss = 2.016460418701172, val_loss = 0.7091089487075806\n",
      "epoch n°238 : train_loss = 2.0205941200256348, val_loss = 0.7038516998291016\n",
      "epoch n°239 : train_loss = 2.013030529022217, val_loss = 0.7036077380180359\n",
      "epoch n°240 : train_loss = 2.0127928256988525, val_loss = 0.7100801467895508\n",
      "epoch n°241 : train_loss = 2.021266222000122, val_loss = 0.7132412195205688\n",
      "epoch n°242 : train_loss = 2.022505521774292, val_loss = 0.7096923589706421\n",
      "epoch n°243 : train_loss = 2.029313564300537, val_loss = 0.7000576853752136\n",
      "epoch n°244 : train_loss = 2.014101028442383, val_loss = 0.7047297954559326\n",
      "epoch n°245 : train_loss = 2.0182137489318848, val_loss = 0.7046006917953491\n",
      "epoch n°246 : train_loss = 2.0154995918273926, val_loss = 0.716432511806488\n",
      "epoch n°247 : train_loss = 2.021404504776001, val_loss = 0.7079481482505798\n",
      "epoch n°248 : train_loss = 2.0189590454101562, val_loss = 0.716057300567627\n",
      "epoch n°249 : train_loss = 2.0259742736816406, val_loss = 0.7049666047096252\n",
      "epoch n°250 : train_loss = 2.0233404636383057, val_loss = 0.7131837606430054\n",
      "epoch n°251 : train_loss = 2.0243444442749023, val_loss = 0.7071374654769897\n",
      "epoch n°252 : train_loss = 2.0224573612213135, val_loss = 0.7035008072853088\n",
      "epoch n°253 : train_loss = 2.0215001106262207, val_loss = 0.7115203738212585\n",
      "epoch n°254 : train_loss = 2.0283010005950928, val_loss = 0.7101368308067322\n",
      "epoch n°255 : train_loss = 2.0321531295776367, val_loss = 0.7120141983032227\n",
      "epoch n°256 : train_loss = 2.0271809101104736, val_loss = 0.7088128924369812\n",
      "epoch n°257 : train_loss = 2.013838529586792, val_loss = 0.7078953385353088\n",
      "epoch n°258 : train_loss = 2.018317222595215, val_loss = 0.7096837759017944\n",
      "epoch n°259 : train_loss = 2.014078378677368, val_loss = 0.7138088941574097\n",
      "epoch n°260 : train_loss = 2.0251846313476562, val_loss = 0.70892733335495\n",
      "epoch n°261 : train_loss = 2.028629779815674, val_loss = 0.7143623232841492\n",
      "epoch n°262 : train_loss = 2.0268073081970215, val_loss = 0.7044781446456909\n",
      "epoch n°263 : train_loss = 2.0176892280578613, val_loss = 0.7102572321891785\n",
      "epoch n°264 : train_loss = 2.0265660285949707, val_loss = 0.7023366689682007\n",
      "epoch n°265 : train_loss = 2.0166542530059814, val_loss = 0.7060690522193909\n",
      "epoch n°266 : train_loss = 2.011558771133423, val_loss = 0.705271303653717\n",
      "epoch n°267 : train_loss = 2.027439832687378, val_loss = 0.714165210723877\n",
      "epoch n°268 : train_loss = 2.0271902084350586, val_loss = 0.7110884785652161\n",
      "epoch n°269 : train_loss = 2.017603874206543, val_loss = 0.7055031657218933\n",
      "epoch n°270 : train_loss = 2.0194575786590576, val_loss = 0.7091831564903259\n",
      "epoch n°271 : train_loss = 2.020392894744873, val_loss = 0.7102529406547546\n",
      "epoch n°272 : train_loss = 2.006988286972046, val_loss = 0.7049571871757507\n",
      "epoch n°273 : train_loss = 2.018308401107788, val_loss = 0.7098082304000854\n",
      "epoch n°274 : train_loss = 2.0125315189361572, val_loss = 0.7023307085037231\n",
      "epoch n°275 : train_loss = 2.016366481781006, val_loss = 0.7080643773078918\n",
      "epoch n°276 : train_loss = 2.025977611541748, val_loss = 0.7067124843597412\n",
      "epoch n°277 : train_loss = 2.009533166885376, val_loss = 0.7065915465354919\n",
      "epoch n°278 : train_loss = 2.010897636413574, val_loss = 0.7110525369644165\n",
      "epoch n°279 : train_loss = 2.029177665710449, val_loss = 0.7066769003868103\n",
      "epoch n°280 : train_loss = 2.0186715126037598, val_loss = 0.7069167494773865\n",
      "epoch n°281 : train_loss = 2.0283491611480713, val_loss = 0.7043677568435669\n",
      "epoch n°282 : train_loss = 2.0276575088500977, val_loss = 0.7117366790771484\n",
      "epoch n°283 : train_loss = 2.0181469917297363, val_loss = 0.7073225378990173\n",
      "epoch n°284 : train_loss = 2.015856981277466, val_loss = 0.7028602361679077\n",
      "epoch n°285 : train_loss = 2.0282881259918213, val_loss = 0.7089362740516663\n",
      "epoch n°286 : train_loss = 2.010334014892578, val_loss = 0.7131946086883545\n",
      "epoch n°287 : train_loss = 2.013624429702759, val_loss = 0.7093692421913147\n",
      "epoch n°288 : train_loss = 2.0145037174224854, val_loss = 0.7203063368797302\n",
      "epoch n°289 : train_loss = 2.0225677490234375, val_loss = 0.7040258049964905\n",
      "epoch n°290 : train_loss = 2.014072895050049, val_loss = 0.715720534324646\n",
      "epoch n°291 : train_loss = 2.012265205383301, val_loss = 0.7096211910247803\n",
      "epoch n°292 : train_loss = 2.0233912467956543, val_loss = 0.7112849950790405\n",
      "epoch n°293 : train_loss = 2.016969919204712, val_loss = 0.7071923613548279\n",
      "epoch n°294 : train_loss = 2.0146660804748535, val_loss = 0.708732008934021\n",
      "epoch n°295 : train_loss = 2.0229110717773438, val_loss = 0.7162865400314331\n",
      "epoch n°296 : train_loss = 2.0184175968170166, val_loss = 0.7039275169372559\n",
      "epoch n°297 : train_loss = 2.0083577632904053, val_loss = 0.7132415175437927\n",
      "epoch n°298 : train_loss = 2.0101988315582275, val_loss = 0.7106672525405884\n",
      "epoch n°299 : train_loss = 2.0035436153411865, val_loss = 0.7053881883621216\n",
      "epoch n°300 : train_loss = 2.019195079803467, val_loss = 0.7115340828895569\n",
      "epoch n°301 : train_loss = 2.017552375793457, val_loss = 0.7082191109657288\n",
      "epoch n°302 : train_loss = 2.020638942718506, val_loss = 0.7040841579437256\n",
      "epoch n°303 : train_loss = 2.0191192626953125, val_loss = 0.7092152833938599\n",
      "epoch n°304 : train_loss = 2.000300884246826, val_loss = 0.7103695273399353\n",
      "epoch n°305 : train_loss = 2.0073249340057373, val_loss = 0.7057621479034424\n",
      "epoch n°306 : train_loss = 2.0096752643585205, val_loss = 0.7088667750358582\n",
      "epoch n°307 : train_loss = 2.0108652114868164, val_loss = 0.701938807964325\n",
      "epoch n°308 : train_loss = 2.0137884616851807, val_loss = 0.7089595198631287\n",
      "epoch n°309 : train_loss = 1.998924970626831, val_loss = 0.7071341872215271\n",
      "epoch n°310 : train_loss = 2.0069305896759033, val_loss = 0.7110385894775391\n",
      "epoch n°311 : train_loss = 2.0128355026245117, val_loss = 0.7101809978485107\n",
      "epoch n°312 : train_loss = 2.008967876434326, val_loss = 0.7092387676239014\n",
      "epoch n°313 : train_loss = 2.0019540786743164, val_loss = 0.712293267250061\n",
      "epoch n°314 : train_loss = 2.012057304382324, val_loss = 0.7030746340751648\n",
      "epoch n°315 : train_loss = 2.0128185749053955, val_loss = 0.703476071357727\n",
      "epoch n°316 : train_loss = 2.0122768878936768, val_loss = 0.7059783935546875\n",
      "epoch n°317 : train_loss = 2.0187301635742188, val_loss = 0.7044864296913147\n",
      "epoch n°318 : train_loss = 2.020324230194092, val_loss = 0.7079064249992371\n",
      "epoch n°319 : train_loss = 2.017167568206787, val_loss = 0.6997721791267395\n",
      "epoch n°320 : train_loss = 2.016155481338501, val_loss = 0.710749626159668\n",
      "epoch n°321 : train_loss = 2.015672445297241, val_loss = 0.710705578327179\n",
      "epoch n°322 : train_loss = 2.012148380279541, val_loss = 0.7075698971748352\n",
      "epoch n°323 : train_loss = 2.0032730102539062, val_loss = 0.7127934098243713\n",
      "epoch n°324 : train_loss = 2.0095314979553223, val_loss = 0.7047447562217712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°325 : train_loss = 2.0173778533935547, val_loss = 0.7082457542419434\n",
      "epoch n°326 : train_loss = 2.0054099559783936, val_loss = 0.7068219184875488\n",
      "epoch n°327 : train_loss = 2.003990888595581, val_loss = 0.6984466910362244\n",
      "epoch n°328 : train_loss = 2.0054500102996826, val_loss = 0.7000658512115479\n",
      "epoch n°329 : train_loss = 2.0175623893737793, val_loss = 0.7091925144195557\n",
      "epoch n°330 : train_loss = 2.0095396041870117, val_loss = 0.709083080291748\n",
      "epoch n°331 : train_loss = 2.017418622970581, val_loss = 0.7038782835006714\n",
      "epoch n°332 : train_loss = 2.0106678009033203, val_loss = 0.707733154296875\n",
      "epoch n°333 : train_loss = 2.0025346279144287, val_loss = 0.7056586742401123\n",
      "epoch n°334 : train_loss = 2.0110068321228027, val_loss = 0.70474773645401\n",
      "epoch n°335 : train_loss = 2.0039525032043457, val_loss = 0.7054221630096436\n",
      "epoch n°336 : train_loss = 2.0091423988342285, val_loss = 0.7078304886817932\n",
      "epoch n°337 : train_loss = 1.9991958141326904, val_loss = 0.7131544947624207\n",
      "epoch n°338 : train_loss = 2.0156633853912354, val_loss = 0.7134120464324951\n",
      "epoch n°339 : train_loss = 1.996171474456787, val_loss = 0.7041173577308655\n",
      "epoch n°340 : train_loss = 1.998044490814209, val_loss = 0.7063671946525574\n",
      "epoch n°341 : train_loss = 2.0113120079040527, val_loss = 0.7083557844161987\n",
      "epoch n°342 : train_loss = 2.0011250972747803, val_loss = 0.7131623029708862\n",
      "epoch n°343 : train_loss = 1.9964274168014526, val_loss = 0.7039265036582947\n",
      "epoch n°344 : train_loss = 2.009218692779541, val_loss = 0.7128071784973145\n",
      "epoch n°345 : train_loss = 2.001803398132324, val_loss = 0.7098698616027832\n",
      "epoch n°346 : train_loss = 2.001230478286743, val_loss = 0.7036200165748596\n",
      "epoch n°347 : train_loss = 2.0181987285614014, val_loss = 0.7111515998840332\n",
      "epoch n°348 : train_loss = 2.001190662384033, val_loss = 0.7042264342308044\n",
      "epoch n°349 : train_loss = 2.0102999210357666, val_loss = 0.703002393245697\n",
      "epoch n°350 : train_loss = 2.006103754043579, val_loss = 0.7063825726509094\n",
      "epoch n°351 : train_loss = 2.008559465408325, val_loss = 0.7025904059410095\n",
      "epoch n°352 : train_loss = 2.000051259994507, val_loss = 0.7036277651786804\n",
      "epoch n°353 : train_loss = 2.006490707397461, val_loss = 0.7141218185424805\n",
      "epoch n°354 : train_loss = 2.0035014152526855, val_loss = 0.7096583247184753\n",
      "epoch n°355 : train_loss = 1.9997445344924927, val_loss = 0.7095754146575928\n",
      "epoch n°356 : train_loss = 2.0039336681365967, val_loss = 0.7140259146690369\n",
      "epoch n°357 : train_loss = 2.0081777572631836, val_loss = 0.710731565952301\n",
      "epoch n°358 : train_loss = 2.006077527999878, val_loss = 0.7034424543380737\n",
      "epoch n°359 : train_loss = 2.009936571121216, val_loss = 0.7029106616973877\n",
      "epoch n°360 : train_loss = 1.9956878423690796, val_loss = 0.7104543447494507\n",
      "epoch n°361 : train_loss = 2.0074944496154785, val_loss = 0.7073158621788025\n",
      "epoch n°362 : train_loss = 1.999306321144104, val_loss = 0.7141892313957214\n",
      "epoch n°363 : train_loss = 1.9982795715332031, val_loss = 0.7052830457687378\n",
      "epoch n°364 : train_loss = 2.0256035327911377, val_loss = 0.7006204128265381\n",
      "epoch n°365 : train_loss = 1.9982951879501343, val_loss = 0.7057719826698303\n",
      "epoch n°366 : train_loss = 2.0064570903778076, val_loss = 0.7003911733627319\n",
      "epoch n°367 : train_loss = 2.0024492740631104, val_loss = 0.7015622854232788\n",
      "epoch n°368 : train_loss = 2.0102524757385254, val_loss = 0.7134896516799927\n",
      "epoch n°369 : train_loss = 2.004896640777588, val_loss = 0.7083150148391724\n",
      "epoch n°370 : train_loss = 1.9958394765853882, val_loss = 0.6973870992660522\n",
      "epoch n°371 : train_loss = 1.9984210729599, val_loss = 0.7012231945991516\n",
      "epoch n°372 : train_loss = 2.003884792327881, val_loss = 0.7004267573356628\n",
      "epoch n°373 : train_loss = 2.002378225326538, val_loss = 0.712516725063324\n",
      "epoch n°374 : train_loss = 2.007498264312744, val_loss = 0.705335259437561\n",
      "epoch n°375 : train_loss = 2.011451482772827, val_loss = 0.703394889831543\n",
      "epoch n°376 : train_loss = 2.0101938247680664, val_loss = 0.7026621103286743\n",
      "epoch n°377 : train_loss = 1.9944807291030884, val_loss = 0.7084501385688782\n",
      "epoch n°378 : train_loss = 2.001274585723877, val_loss = 0.7093231081962585\n",
      "epoch n°379 : train_loss = 2.0023770332336426, val_loss = 0.7048801779747009\n",
      "epoch n°380 : train_loss = 2.0049123764038086, val_loss = 0.7026357054710388\n",
      "epoch n°381 : train_loss = 2.007258176803589, val_loss = 0.7123693823814392\n",
      "epoch n°382 : train_loss = 2.0092477798461914, val_loss = 0.7106930613517761\n",
      "epoch n°383 : train_loss = 1.9908078908920288, val_loss = 0.7074985504150391\n",
      "epoch n°384 : train_loss = 2.001044750213623, val_loss = 0.7138684391975403\n",
      "epoch n°385 : train_loss = 2.0032107830047607, val_loss = 0.6992104649543762\n",
      "epoch n°386 : train_loss = 2.011597156524658, val_loss = 0.7043097615242004\n",
      "epoch n°387 : train_loss = 2.0094974040985107, val_loss = 0.7063403725624084\n",
      "epoch n°388 : train_loss = 2.0018160343170166, val_loss = 0.7128804326057434\n",
      "epoch n°389 : train_loss = 2.001319169998169, val_loss = 0.7017953395843506\n",
      "epoch n°390 : train_loss = 2.0037152767181396, val_loss = 0.7068862915039062\n",
      "epoch n°391 : train_loss = 2.001760721206665, val_loss = 0.7070474028587341\n",
      "epoch n°392 : train_loss = 2.0059914588928223, val_loss = 0.707321047782898\n",
      "epoch n°393 : train_loss = 2.012690544128418, val_loss = 0.7070179581642151\n",
      "epoch n°394 : train_loss = 2.003472328186035, val_loss = 0.7050093412399292\n",
      "epoch n°395 : train_loss = 2.0129692554473877, val_loss = 0.70772784948349\n",
      "epoch n°396 : train_loss = 1.9888033866882324, val_loss = 0.7075957655906677\n",
      "epoch n°397 : train_loss = 1.999709129333496, val_loss = 0.7010149955749512\n",
      "epoch n°398 : train_loss = 2.0106897354125977, val_loss = 0.7048510909080505\n",
      "epoch n°399 : train_loss = 2.0009942054748535, val_loss = 0.7061246037483215\n",
      "epoch n°400 : train_loss = 2.002021074295044, val_loss = 0.704715371131897\n",
      "epoch n°401 : train_loss = 2.0043859481811523, val_loss = 0.7113935947418213\n",
      "epoch n°402 : train_loss = 1.9964791536331177, val_loss = 0.6983134150505066\n",
      "epoch n°403 : train_loss = 1.9986814260482788, val_loss = 0.7068817019462585\n",
      "epoch n°404 : train_loss = 1.991266131401062, val_loss = 0.7061207890510559\n",
      "epoch n°405 : train_loss = 1.9895637035369873, val_loss = 0.7119401693344116\n",
      "epoch n°406 : train_loss = 2.0048134326934814, val_loss = 0.7087228298187256\n",
      "epoch n°407 : train_loss = 2.0012872219085693, val_loss = 0.7102245092391968\n",
      "epoch n°408 : train_loss = 1.9993865489959717, val_loss = 0.7040907740592957\n",
      "epoch n°409 : train_loss = 2.002697706222534, val_loss = 0.7042906284332275\n",
      "epoch n°410 : train_loss = 1.994117021560669, val_loss = 0.7034860253334045\n",
      "epoch n°411 : train_loss = 2.0056190490722656, val_loss = 0.7061015963554382\n",
      "epoch n°412 : train_loss = 2.0102765560150146, val_loss = 0.707669734954834\n",
      "epoch n°413 : train_loss = 1.989940881729126, val_loss = 0.705808699131012\n",
      "epoch n°414 : train_loss = 2.002302646636963, val_loss = 0.7049950361251831\n",
      "epoch n°415 : train_loss = 1.9968247413635254, val_loss = 0.7072893381118774\n",
      "epoch n°416 : train_loss = 2.001647710800171, val_loss = 0.7092681527137756\n",
      "epoch n°417 : train_loss = 2.003000497817993, val_loss = 0.7161540985107422\n",
      "epoch n°418 : train_loss = 1.98550546169281, val_loss = 0.7031787037849426\n",
      "epoch n°419 : train_loss = 1.9876692295074463, val_loss = 0.7089682817459106\n",
      "epoch n°420 : train_loss = 1.9887547492980957, val_loss = 0.7028664350509644\n",
      "epoch n°421 : train_loss = 1.9977530241012573, val_loss = 0.7170270681381226\n",
      "epoch n°422 : train_loss = 1.9922493696212769, val_loss = 0.6985154747962952\n",
      "epoch n°423 : train_loss = 1.9998359680175781, val_loss = 0.7136799693107605\n",
      "epoch n°424 : train_loss = 1.9935566186904907, val_loss = 0.7044797539710999\n",
      "epoch n°425 : train_loss = 1.9980275630950928, val_loss = 0.709101676940918\n",
      "epoch n°426 : train_loss = 1.9870824813842773, val_loss = 0.7055450081825256\n",
      "epoch n°427 : train_loss = 1.9978069067001343, val_loss = 0.7035501599311829\n",
      "epoch n°428 : train_loss = 1.9900704622268677, val_loss = 0.696873128414154\n",
      "epoch n°429 : train_loss = 2.006783962249756, val_loss = 0.7103818655014038\n",
      "epoch n°430 : train_loss = 2.00003719329834, val_loss = 0.7036781311035156\n",
      "epoch n°431 : train_loss = 1.998992681503296, val_loss = 0.7044057846069336\n",
      "epoch n°432 : train_loss = 1.9955878257751465, val_loss = 0.7017109990119934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°433 : train_loss = 1.999412178993225, val_loss = 0.7063737511634827\n",
      "epoch n°434 : train_loss = 1.9939712285995483, val_loss = 0.7022276520729065\n",
      "epoch n°435 : train_loss = 1.9997683763504028, val_loss = 0.7098737359046936\n",
      "epoch n°436 : train_loss = 2.001476526260376, val_loss = 0.7071850895881653\n",
      "epoch n°437 : train_loss = 1.9904093742370605, val_loss = 0.7035548686981201\n",
      "epoch n°438 : train_loss = 2.0006015300750732, val_loss = 0.708502471446991\n",
      "epoch n°439 : train_loss = 1.9844670295715332, val_loss = 0.7050119638442993\n",
      "epoch n°440 : train_loss = 1.997742772102356, val_loss = 0.7037720680236816\n",
      "epoch n°441 : train_loss = 1.9955905675888062, val_loss = 0.7086261510848999\n",
      "epoch n°442 : train_loss = 2.0040245056152344, val_loss = 0.7018792629241943\n",
      "epoch n°443 : train_loss = 1.9915441274642944, val_loss = 0.7052223682403564\n",
      "epoch n°444 : train_loss = 2.0065224170684814, val_loss = 0.7030482292175293\n",
      "epoch n°445 : train_loss = 1.9894427061080933, val_loss = 0.707490086555481\n",
      "epoch n°446 : train_loss = 1.9897525310516357, val_loss = 0.7058067917823792\n",
      "epoch n°447 : train_loss = 2.002620220184326, val_loss = 0.709782600402832\n",
      "epoch n°448 : train_loss = 1.9921919107437134, val_loss = 0.6990064978599548\n",
      "epoch n°449 : train_loss = 1.9863630533218384, val_loss = 0.7156485915184021\n",
      "epoch n°450 : train_loss = 2.001479387283325, val_loss = 0.7030023336410522\n",
      "epoch n°451 : train_loss = 1.9990921020507812, val_loss = 0.7046211957931519\n",
      "epoch n°452 : train_loss = 1.9958789348602295, val_loss = 0.7132286429405212\n",
      "epoch n°453 : train_loss = 1.9965112209320068, val_loss = 0.7084419131278992\n",
      "epoch n°454 : train_loss = 1.9961814880371094, val_loss = 0.704639732837677\n",
      "epoch n°455 : train_loss = 2.000653028488159, val_loss = 0.7091372609138489\n",
      "epoch n°456 : train_loss = 1.9823381900787354, val_loss = 0.7032619118690491\n",
      "epoch n°457 : train_loss = 1.9928547143936157, val_loss = 0.7062967419624329\n",
      "epoch n°458 : train_loss = 1.9905052185058594, val_loss = 0.7008101344108582\n",
      "epoch n°459 : train_loss = 1.9926139116287231, val_loss = 0.699565052986145\n",
      "epoch n°460 : train_loss = 1.991113305091858, val_loss = 0.7079104781150818\n",
      "epoch n°461 : train_loss = 1.9880475997924805, val_loss = 0.7037261724472046\n",
      "epoch n°462 : train_loss = 1.9904156923294067, val_loss = 0.7035948634147644\n",
      "epoch n°463 : train_loss = 1.9910615682601929, val_loss = 0.7087342739105225\n",
      "epoch n°464 : train_loss = 1.9911673069000244, val_loss = 0.701719343662262\n",
      "epoch n°465 : train_loss = 1.9876787662506104, val_loss = 0.7084660530090332\n",
      "epoch n°466 : train_loss = 1.9932026863098145, val_loss = 0.7055745720863342\n",
      "epoch n°467 : train_loss = 2.0023140907287598, val_loss = 0.7056745886802673\n",
      "epoch n°468 : train_loss = 1.9956928491592407, val_loss = 0.7060829401016235\n",
      "epoch n°469 : train_loss = 1.9891059398651123, val_loss = 0.7070493698120117\n",
      "epoch n°470 : train_loss = 1.992043137550354, val_loss = 0.7055596709251404\n",
      "epoch n°471 : train_loss = 1.9958739280700684, val_loss = 0.7056292295455933\n",
      "epoch n°472 : train_loss = 1.9990326166152954, val_loss = 0.7044950723648071\n",
      "epoch n°473 : train_loss = 1.9885094165802002, val_loss = 0.7139601111412048\n",
      "epoch n°474 : train_loss = 1.9914804697036743, val_loss = 0.7039315700531006\n",
      "epoch n°475 : train_loss = 1.9946050643920898, val_loss = 0.7101141810417175\n",
      "epoch n°476 : train_loss = 1.9899758100509644, val_loss = 0.6981577277183533\n",
      "epoch n°477 : train_loss = 1.9930241107940674, val_loss = 0.7039185762405396\n",
      "epoch n°478 : train_loss = 1.9898147583007812, val_loss = 0.6994161605834961\n",
      "epoch n°479 : train_loss = 1.9985779523849487, val_loss = 0.7013644576072693\n",
      "epoch n°480 : train_loss = 1.9929019212722778, val_loss = 0.703925609588623\n",
      "epoch n°481 : train_loss = 1.9947816133499146, val_loss = 0.7056346535682678\n",
      "epoch n°482 : train_loss = 1.9979876279830933, val_loss = 0.700255274772644\n",
      "epoch n°483 : train_loss = 1.9977498054504395, val_loss = 0.6989245414733887\n",
      "epoch n°484 : train_loss = 1.9860237836837769, val_loss = 0.7011749148368835\n",
      "epoch n°485 : train_loss = 1.995047926902771, val_loss = 0.7036564946174622\n",
      "epoch n°486 : train_loss = 1.9893423318862915, val_loss = 0.7065813541412354\n",
      "epoch n°487 : train_loss = 1.9920971393585205, val_loss = 0.7051159143447876\n",
      "epoch n°488 : train_loss = 1.994901180267334, val_loss = 0.7100282907485962\n",
      "epoch n°489 : train_loss = 1.9923763275146484, val_loss = 0.7054066061973572\n",
      "epoch n°490 : train_loss = 1.9936307668685913, val_loss = 0.7001276016235352\n",
      "epoch n°491 : train_loss = 1.9936795234680176, val_loss = 0.7058522701263428\n",
      "epoch n°492 : train_loss = 1.9961615800857544, val_loss = 0.6997675895690918\n",
      "epoch n°493 : train_loss = 1.994715690612793, val_loss = 0.7004645466804504\n",
      "epoch n°494 : train_loss = 1.9922873973846436, val_loss = 0.7078832983970642\n",
      "epoch n°495 : train_loss = 1.99341881275177, val_loss = 0.6996995210647583\n",
      "epoch n°496 : train_loss = 2.0013694763183594, val_loss = 0.7079128623008728\n",
      "epoch n°497 : train_loss = 2.010793685913086, val_loss = 0.7067383527755737\n",
      "epoch n°498 : train_loss = 2.0036046504974365, val_loss = 0.7081714272499084\n",
      "epoch n°499 : train_loss = 2.0151519775390625, val_loss = 0.699407696723938\n",
      "epoch n°500 : train_loss = 2.0037074089050293, val_loss = 0.7080175280570984\n",
      "epoch n°501 : train_loss = 2.012524366378784, val_loss = 0.7093072533607483\n",
      "epoch n°502 : train_loss = 2.004472017288208, val_loss = 0.7135829329490662\n",
      "epoch n°503 : train_loss = 1.9967986345291138, val_loss = 0.7096200585365295\n",
      "epoch n°504 : train_loss = 2.0103957653045654, val_loss = 0.7036063075065613\n",
      "epoch n°505 : train_loss = 2.0080151557922363, val_loss = 0.7053606510162354\n",
      "epoch n°506 : train_loss = 2.0166122913360596, val_loss = 0.7094535231590271\n",
      "epoch n°507 : train_loss = 2.0250110626220703, val_loss = 0.7031716108322144\n",
      "epoch n°508 : train_loss = 2.017085313796997, val_loss = 0.7051161527633667\n",
      "epoch n°509 : train_loss = 2.0151829719543457, val_loss = 0.7073987722396851\n",
      "epoch n°510 : train_loss = 2.001923084259033, val_loss = 0.7067050337791443\n",
      "epoch n°511 : train_loss = 2.0006837844848633, val_loss = 0.7075355648994446\n",
      "epoch n°512 : train_loss = 2.001497268676758, val_loss = 0.7041040658950806\n",
      "epoch n°513 : train_loss = 2.005258083343506, val_loss = 0.706333339214325\n",
      "epoch n°514 : train_loss = 2.0000052452087402, val_loss = 0.7043699622154236\n",
      "epoch n°515 : train_loss = 2.0103728771209717, val_loss = 0.7103395462036133\n",
      "epoch n°516 : train_loss = 2.0122745037078857, val_loss = 0.7057183384895325\n",
      "epoch n°517 : train_loss = 2.021002769470215, val_loss = 0.7096053957939148\n",
      "epoch n°518 : train_loss = 2.0004045963287354, val_loss = 0.7093081474304199\n",
      "epoch n°519 : train_loss = 2.012740135192871, val_loss = 0.7112166881561279\n",
      "epoch n°520 : train_loss = 2.009286403656006, val_loss = 0.7019473314285278\n",
      "epoch n°521 : train_loss = 2.021026611328125, val_loss = 0.7129797339439392\n",
      "epoch n°522 : train_loss = 2.0142834186553955, val_loss = 0.7183611392974854\n",
      "epoch n°523 : train_loss = 2.001685857772827, val_loss = 0.7018340229988098\n",
      "epoch n°524 : train_loss = 2.005319118499756, val_loss = 0.7058097720146179\n",
      "epoch n°525 : train_loss = 2.0085861682891846, val_loss = 0.707298994064331\n",
      "epoch n°526 : train_loss = 2.0194199085235596, val_loss = 0.7099902629852295\n",
      "epoch n°527 : train_loss = 2.0078341960906982, val_loss = 0.7099779844284058\n",
      "epoch n°528 : train_loss = 2.010344982147217, val_loss = 0.7124708294868469\n",
      "epoch n°529 : train_loss = 2.0027077198028564, val_loss = 0.7124963402748108\n",
      "epoch n°530 : train_loss = 2.0033867359161377, val_loss = 0.7033309936523438\n",
      "epoch n°531 : train_loss = 2.0044469833374023, val_loss = 0.7155089974403381\n",
      "epoch n°532 : train_loss = 2.0105903148651123, val_loss = 0.7084723114967346\n",
      "epoch n°533 : train_loss = 2.001857280731201, val_loss = 0.7090437412261963\n",
      "epoch n°534 : train_loss = 2.010913610458374, val_loss = 0.7018404603004456\n",
      "epoch n°535 : train_loss = 2.020341396331787, val_loss = 0.7105519771575928\n",
      "epoch n°536 : train_loss = 2.0040900707244873, val_loss = 0.7037455439567566\n",
      "epoch n°537 : train_loss = 2.0078909397125244, val_loss = 0.7007877826690674\n",
      "epoch n°538 : train_loss = 2.01352858543396, val_loss = 0.7070830464363098\n",
      "epoch n°539 : train_loss = 2.0168473720550537, val_loss = 0.7070358395576477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°540 : train_loss = 2.0100595951080322, val_loss = 0.7091965675354004\n",
      "epoch n°541 : train_loss = 2.010633707046509, val_loss = 0.7091044783592224\n",
      "epoch n°542 : train_loss = 2.009326934814453, val_loss = 0.7089424133300781\n",
      "epoch n°543 : train_loss = 2.008157968521118, val_loss = 0.7042912244796753\n",
      "epoch n°544 : train_loss = 2.0011727809906006, val_loss = 0.7103451490402222\n",
      "epoch n°545 : train_loss = 2.0048413276672363, val_loss = 0.7112862467765808\n",
      "epoch n°546 : train_loss = 1.9974229335784912, val_loss = 0.7065048813819885\n",
      "epoch n°547 : train_loss = 2.004756212234497, val_loss = 0.7036749720573425\n",
      "epoch n°548 : train_loss = 2.0003609657287598, val_loss = 0.707796573638916\n",
      "epoch n°549 : train_loss = 2.0029067993164062, val_loss = 0.7111994028091431\n",
      "epoch n°550 : train_loss = 2.0053207874298096, val_loss = 0.7116884589195251\n",
      "epoch n°551 : train_loss = 2.007506847381592, val_loss = 0.7113655805587769\n",
      "epoch n°552 : train_loss = 1.9931135177612305, val_loss = 0.7042362689971924\n",
      "epoch n°553 : train_loss = 2.0052332878112793, val_loss = 0.7050427794456482\n",
      "epoch n°554 : train_loss = 2.0132553577423096, val_loss = 0.7072816491127014\n",
      "epoch n°555 : train_loss = 1.9946401119232178, val_loss = 0.7055663466453552\n",
      "epoch n°556 : train_loss = 2.022895336151123, val_loss = 0.7075154781341553\n",
      "epoch n°557 : train_loss = 2.0138707160949707, val_loss = 0.7074755430221558\n",
      "epoch n°558 : train_loss = 2.0113685131073, val_loss = 0.7073797583580017\n",
      "epoch n°559 : train_loss = 2.015817642211914, val_loss = 0.7033659815788269\n",
      "epoch n°560 : train_loss = 2.014313220977783, val_loss = 0.7024995684623718\n",
      "epoch n°561 : train_loss = 1.9913257360458374, val_loss = 0.7028706073760986\n",
      "epoch n°562 : train_loss = 2.011749029159546, val_loss = 0.7049850225448608\n",
      "epoch n°563 : train_loss = 1.9957298040390015, val_loss = 0.707100510597229\n",
      "epoch n°564 : train_loss = 2.0014712810516357, val_loss = 0.7076957821846008\n",
      "epoch n°565 : train_loss = 2.005993127822876, val_loss = 0.7114589810371399\n",
      "epoch n°566 : train_loss = 2.0007753372192383, val_loss = 0.7085270881652832\n",
      "epoch n°567 : train_loss = 2.009187936782837, val_loss = 0.7046592831611633\n",
      "epoch n°568 : train_loss = 2.007019519805908, val_loss = 0.704797089099884\n",
      "epoch n°569 : train_loss = 2.008657693862915, val_loss = 0.7004103064537048\n",
      "epoch n°570 : train_loss = 2.0062179565429688, val_loss = 0.7004804611206055\n",
      "epoch n°571 : train_loss = 2.007667064666748, val_loss = 0.7061427235603333\n",
      "epoch n°572 : train_loss = 1.9989662170410156, val_loss = 0.7030103802680969\n",
      "epoch n°573 : train_loss = 2.0102663040161133, val_loss = 0.7114541530609131\n",
      "epoch n°574 : train_loss = 1.9951891899108887, val_loss = 0.7009783983230591\n",
      "epoch n°575 : train_loss = 2.007446765899658, val_loss = 0.7063676118850708\n",
      "epoch n°576 : train_loss = 2.0065643787384033, val_loss = 0.7087888121604919\n",
      "epoch n°577 : train_loss = 2.0029242038726807, val_loss = 0.7127436995506287\n",
      "epoch n°578 : train_loss = 2.004671335220337, val_loss = 0.7014829516410828\n",
      "epoch n°579 : train_loss = 2.003506660461426, val_loss = 0.7061979174613953\n",
      "epoch n°580 : train_loss = 1.989283800125122, val_loss = 0.7035648822784424\n",
      "epoch n°581 : train_loss = 2.010951280593872, val_loss = 0.7104142904281616\n",
      "epoch n°582 : train_loss = 1.9972201585769653, val_loss = 0.7076376676559448\n",
      "epoch n°583 : train_loss = 2.0004308223724365, val_loss = 0.7074441313743591\n",
      "epoch n°584 : train_loss = 2.006467580795288, val_loss = 0.711078941822052\n",
      "epoch n°585 : train_loss = 2.009622812271118, val_loss = 0.7148070931434631\n",
      "epoch n°586 : train_loss = 2.006556272506714, val_loss = 0.7016110420227051\n",
      "epoch n°587 : train_loss = 1.9984568357467651, val_loss = 0.7109473347663879\n",
      "epoch n°588 : train_loss = 2.0001397132873535, val_loss = 0.7102653384208679\n",
      "epoch n°589 : train_loss = 2.0059115886688232, val_loss = 0.7084113359451294\n",
      "epoch n°590 : train_loss = 2.002675771713257, val_loss = 0.7121758460998535\n",
      "epoch n°591 : train_loss = 2.0098516941070557, val_loss = 0.7080888152122498\n",
      "epoch n°592 : train_loss = 2.003662109375, val_loss = 0.7144041061401367\n",
      "epoch n°593 : train_loss = 2.004284143447876, val_loss = 0.7063194513320923\n",
      "epoch n°594 : train_loss = 2.0009520053863525, val_loss = 0.7021299004554749\n",
      "epoch n°595 : train_loss = 2.0065500736236572, val_loss = 0.7040061950683594\n",
      "epoch n°596 : train_loss = 1.9998897314071655, val_loss = 0.7080475091934204\n",
      "epoch n°597 : train_loss = 2.007284641265869, val_loss = 0.7053295373916626\n",
      "epoch n°598 : train_loss = 2.0042061805725098, val_loss = 0.7062965035438538\n",
      "epoch n°599 : train_loss = 2.0040123462677, val_loss = 0.7098855376243591\n",
      "epoch n°600 : train_loss = 2.001316547393799, val_loss = 0.7091206312179565\n",
      "epoch n°601 : train_loss = 2.0065975189208984, val_loss = 0.7086777687072754\n",
      "epoch n°602 : train_loss = 1.9930263757705688, val_loss = 0.7028707265853882\n",
      "epoch n°603 : train_loss = 2.0104990005493164, val_loss = 0.7067298889160156\n",
      "epoch n°604 : train_loss = 2.002997398376465, val_loss = 0.7136238813400269\n",
      "epoch n°605 : train_loss = 2.006981134414673, val_loss = 0.7062371373176575\n",
      "epoch n°606 : train_loss = 1.9996837377548218, val_loss = 0.7111836075782776\n",
      "epoch n°607 : train_loss = 2.001412868499756, val_loss = 0.7053402066230774\n",
      "epoch n°608 : train_loss = 2.004775047302246, val_loss = 0.7070496678352356\n",
      "epoch n°609 : train_loss = 2.01189923286438, val_loss = 0.7110163569450378\n",
      "epoch n°610 : train_loss = 2.010845184326172, val_loss = 0.7038316130638123\n",
      "epoch n°611 : train_loss = 2.005614757537842, val_loss = 0.7091585993766785\n",
      "epoch n°612 : train_loss = 2.0097615718841553, val_loss = 0.7052229046821594\n",
      "epoch n°613 : train_loss = 1.9948081970214844, val_loss = 0.7067537307739258\n",
      "epoch n°614 : train_loss = 2.00297212600708, val_loss = 0.7084225416183472\n",
      "epoch n°615 : train_loss = 2.0048890113830566, val_loss = 0.7111537456512451\n",
      "epoch n°616 : train_loss = 1.9936269521713257, val_loss = 0.7040930986404419\n",
      "epoch n°617 : train_loss = 1.9985157251358032, val_loss = 0.7029954791069031\n",
      "epoch n°618 : train_loss = 2.005826950073242, val_loss = 0.7129817008972168\n",
      "epoch n°619 : train_loss = 2.00126576423645, val_loss = 0.7142528891563416\n",
      "epoch n°620 : train_loss = 2.0017991065979004, val_loss = 0.7091676592826843\n",
      "epoch n°621 : train_loss = 2.007089614868164, val_loss = 0.7016189694404602\n",
      "epoch n°622 : train_loss = 2.01080322265625, val_loss = 0.7037915587425232\n",
      "epoch n°623 : train_loss = 1.9951097965240479, val_loss = 0.7013004422187805\n",
      "epoch n°624 : train_loss = 1.998189926147461, val_loss = 0.7115883827209473\n",
      "epoch n°625 : train_loss = 2.004707098007202, val_loss = 0.7061338424682617\n",
      "epoch n°626 : train_loss = 1.9821010828018188, val_loss = 0.7051995396614075\n",
      "epoch n°627 : train_loss = 2.007333755493164, val_loss = 0.7071380019187927\n",
      "epoch n°628 : train_loss = 1.9949277639389038, val_loss = 0.7024377584457397\n",
      "epoch n°629 : train_loss = 1.9937663078308105, val_loss = 0.7110175490379333\n",
      "epoch n°630 : train_loss = 2.0014467239379883, val_loss = 0.7016901969909668\n",
      "epoch n°631 : train_loss = 2.000689744949341, val_loss = 0.7056196331977844\n",
      "epoch n°632 : train_loss = 1.9963994026184082, val_loss = 0.7073850035667419\n",
      "epoch n°633 : train_loss = 2.0033657550811768, val_loss = 0.7076455354690552\n",
      "epoch n°634 : train_loss = 1.9908018112182617, val_loss = 0.7073334455490112\n",
      "epoch n°635 : train_loss = 2.001044511795044, val_loss = 0.7076576352119446\n",
      "epoch n°636 : train_loss = 1.9989954233169556, val_loss = 0.7087478041648865\n",
      "epoch n°637 : train_loss = 2.0149340629577637, val_loss = 0.7057150602340698\n",
      "epoch n°638 : train_loss = 1.9948748350143433, val_loss = 0.714101254940033\n",
      "epoch n°639 : train_loss = 2.0025343894958496, val_loss = 0.7069458961486816\n",
      "epoch n°640 : train_loss = 1.9977030754089355, val_loss = 0.7118411660194397\n",
      "epoch n°641 : train_loss = 1.9965802431106567, val_loss = 0.7115091681480408\n",
      "epoch n°642 : train_loss = 1.9986823797225952, val_loss = 0.7072126865386963\n",
      "epoch n°643 : train_loss = 2.011821746826172, val_loss = 0.7016370892524719\n",
      "epoch n°644 : train_loss = 1.9943753480911255, val_loss = 0.7072815895080566\n",
      "epoch n°645 : train_loss = 1.9967377185821533, val_loss = 0.7105712890625\n",
      "epoch n°646 : train_loss = 2.003133773803711, val_loss = 0.705558180809021\n",
      "epoch n°647 : train_loss = 2.0031306743621826, val_loss = 0.71043860912323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°648 : train_loss = 1.993267297744751, val_loss = 0.7050272226333618\n",
      "epoch n°649 : train_loss = 1.996922492980957, val_loss = 0.7093138098716736\n",
      "epoch n°650 : train_loss = 1.9964793920516968, val_loss = 0.7055415511131287\n",
      "epoch n°651 : train_loss = 1.986293077468872, val_loss = 0.7096483111381531\n",
      "epoch n°652 : train_loss = 1.996929407119751, val_loss = 0.7074584364891052\n",
      "epoch n°653 : train_loss = 1.993682622909546, val_loss = 0.7138285636901855\n",
      "epoch n°654 : train_loss = 2.0050766468048096, val_loss = 0.7079769968986511\n",
      "epoch n°655 : train_loss = 1.9964240789413452, val_loss = 0.7074915170669556\n",
      "epoch n°656 : train_loss = 2.0048606395721436, val_loss = 0.7069141268730164\n",
      "epoch n°657 : train_loss = 1.9978400468826294, val_loss = 0.7013769149780273\n",
      "epoch n°658 : train_loss = 1.9924778938293457, val_loss = 0.7063499689102173\n",
      "epoch n°659 : train_loss = 1.9995496273040771, val_loss = 0.7026817202568054\n",
      "epoch n°660 : train_loss = 1.9938240051269531, val_loss = 0.705988883972168\n",
      "epoch n°661 : train_loss = 1.992788314819336, val_loss = 0.7061194181442261\n",
      "epoch n°662 : train_loss = 2.0112502574920654, val_loss = 0.706784725189209\n",
      "epoch n°663 : train_loss = 1.9990239143371582, val_loss = 0.7124337553977966\n",
      "epoch n°664 : train_loss = 2.0124783515930176, val_loss = 0.7071125507354736\n",
      "epoch n°665 : train_loss = 1.9882646799087524, val_loss = 0.7019651532173157\n",
      "epoch n°666 : train_loss = 1.9936347007751465, val_loss = 0.7030075788497925\n",
      "epoch n°667 : train_loss = 1.9870556592941284, val_loss = 0.709678590297699\n",
      "epoch n°668 : train_loss = 2.0104568004608154, val_loss = 0.7078676223754883\n",
      "epoch n°669 : train_loss = 1.9982399940490723, val_loss = 0.7040786743164062\n",
      "epoch n°670 : train_loss = 1.9960196018218994, val_loss = 0.702748715877533\n",
      "epoch n°671 : train_loss = 2.0037689208984375, val_loss = 0.7043274641036987\n",
      "epoch n°672 : train_loss = 1.9973787069320679, val_loss = 0.7042616009712219\n",
      "epoch n°673 : train_loss = 1.9969455003738403, val_loss = 0.7067447900772095\n",
      "epoch n°674 : train_loss = 1.9872170686721802, val_loss = 0.7062478065490723\n",
      "epoch n°675 : train_loss = 1.9979504346847534, val_loss = 0.7066203355789185\n",
      "epoch n°676 : train_loss = 1.9917802810668945, val_loss = 0.7054846882820129\n",
      "epoch n°677 : train_loss = 2.0015430450439453, val_loss = 0.7055544853210449\n",
      "epoch n°678 : train_loss = 1.9923017024993896, val_loss = 0.7083856463432312\n",
      "epoch n°679 : train_loss = 2.0069806575775146, val_loss = 0.7088444232940674\n",
      "epoch n°680 : train_loss = 1.9939320087432861, val_loss = 0.7053974270820618\n",
      "epoch n°681 : train_loss = 2.006303071975708, val_loss = 0.7108910083770752\n",
      "epoch n°682 : train_loss = 2.011091709136963, val_loss = 0.7072489857673645\n",
      "epoch n°683 : train_loss = 2.0049147605895996, val_loss = 0.7094363570213318\n",
      "epoch n°684 : train_loss = 2.0033938884735107, val_loss = 0.7094313502311707\n",
      "epoch n°685 : train_loss = 1.9963396787643433, val_loss = 0.7066185474395752\n",
      "epoch n°686 : train_loss = 1.9934443235397339, val_loss = 0.6996598839759827\n",
      "epoch n°687 : train_loss = 1.991020917892456, val_loss = 0.704332172870636\n",
      "epoch n°688 : train_loss = 1.990996241569519, val_loss = 0.7092256546020508\n",
      "epoch n°689 : train_loss = 1.9886136054992676, val_loss = 0.7087873220443726\n",
      "epoch n°690 : train_loss = 1.9907339811325073, val_loss = 0.7028718590736389\n",
      "epoch n°691 : train_loss = 2.0016744136810303, val_loss = 0.7075945734977722\n",
      "epoch n°692 : train_loss = 2.0086894035339355, val_loss = 0.702876627445221\n",
      "epoch n°693 : train_loss = 1.994493007659912, val_loss = 0.707870364189148\n",
      "epoch n°694 : train_loss = 2.0053327083587646, val_loss = 0.7040446996688843\n",
      "epoch n°695 : train_loss = 1.9936349391937256, val_loss = 0.7056742310523987\n",
      "epoch n°696 : train_loss = 1.989373803138733, val_loss = 0.7030616402626038\n",
      "epoch n°697 : train_loss = 1.9846855401992798, val_loss = 0.7081461548805237\n",
      "epoch n°698 : train_loss = 1.98581063747406, val_loss = 0.704978883266449\n",
      "epoch n°699 : train_loss = 1.9937019348144531, val_loss = 0.7071917653083801\n",
      "epoch n°700 : train_loss = 1.9953042268753052, val_loss = 0.705737292766571\n",
      "epoch n°701 : train_loss = 1.9944531917572021, val_loss = 0.7066960334777832\n",
      "epoch n°702 : train_loss = 2.0030012130737305, val_loss = 0.7056389451026917\n",
      "epoch n°703 : train_loss = 1.9895230531692505, val_loss = 0.7009143233299255\n",
      "epoch n°704 : train_loss = 1.9764324426651, val_loss = 0.7059714794158936\n",
      "epoch n°705 : train_loss = 1.9841914176940918, val_loss = 0.7025827765464783\n",
      "epoch n°706 : train_loss = 1.9937100410461426, val_loss = 0.7101714611053467\n",
      "epoch n°707 : train_loss = 1.9999637603759766, val_loss = 0.7023912668228149\n",
      "epoch n°708 : train_loss = 1.9891207218170166, val_loss = 0.7086582779884338\n",
      "epoch n°709 : train_loss = 2.0035266876220703, val_loss = 0.7044106721878052\n",
      "epoch n°710 : train_loss = 1.998051404953003, val_loss = 0.710660994052887\n",
      "epoch n°711 : train_loss = 1.994315505027771, val_loss = 0.7043578624725342\n",
      "epoch n°712 : train_loss = 2.0050134658813477, val_loss = 0.7090638875961304\n",
      "epoch n°713 : train_loss = 1.9984467029571533, val_loss = 0.7032462954521179\n",
      "epoch n°714 : train_loss = 1.9935581684112549, val_loss = 0.7021597623825073\n",
      "epoch n°715 : train_loss = 2.0024306774139404, val_loss = 0.7076832056045532\n",
      "epoch n°716 : train_loss = 1.9865111112594604, val_loss = 0.703838586807251\n",
      "epoch n°717 : train_loss = 1.9865399599075317, val_loss = 0.7028345465660095\n",
      "epoch n°718 : train_loss = 1.9939367771148682, val_loss = 0.7110579013824463\n",
      "epoch n°719 : train_loss = 2.0007941722869873, val_loss = 0.70121169090271\n",
      "epoch n°720 : train_loss = 1.986061692237854, val_loss = 0.7011718153953552\n",
      "epoch n°721 : train_loss = 2.0037405490875244, val_loss = 0.7061012387275696\n",
      "epoch n°722 : train_loss = 1.9854655265808105, val_loss = 0.7060117125511169\n",
      "epoch n°723 : train_loss = 1.9913089275360107, val_loss = 0.7042914628982544\n",
      "epoch n°724 : train_loss = 1.989211916923523, val_loss = 0.7057763934135437\n",
      "epoch n°725 : train_loss = 1.9974499940872192, val_loss = 0.7088245153427124\n",
      "epoch n°726 : train_loss = 1.9878566265106201, val_loss = 0.7070176601409912\n",
      "epoch n°727 : train_loss = 1.9854148626327515, val_loss = 0.7001618146896362\n",
      "epoch n°728 : train_loss = 1.99937105178833, val_loss = 0.7060746550559998\n",
      "epoch n°729 : train_loss = 1.996506690979004, val_loss = 0.7060348987579346\n",
      "epoch n°730 : train_loss = 2.003145456314087, val_loss = 0.7113986015319824\n",
      "epoch n°731 : train_loss = 1.9929285049438477, val_loss = 0.7059915065765381\n",
      "epoch n°732 : train_loss = 2.0038981437683105, val_loss = 0.7041957378387451\n",
      "epoch n°733 : train_loss = 1.986950159072876, val_loss = 0.7112041711807251\n",
      "epoch n°734 : train_loss = 2.0029587745666504, val_loss = 0.7019692659378052\n",
      "epoch n°735 : train_loss = 2.005345106124878, val_loss = 0.7063190340995789\n",
      "epoch n°736 : train_loss = 1.9934666156768799, val_loss = 0.6968499422073364\n",
      "epoch n°737 : train_loss = 1.989824652671814, val_loss = 0.7027117609977722\n",
      "epoch n°738 : train_loss = 1.9881517887115479, val_loss = 0.7091537117958069\n",
      "epoch n°739 : train_loss = 2.003715991973877, val_loss = 0.6998741626739502\n",
      "epoch n°740 : train_loss = 1.9925109148025513, val_loss = 0.7045927047729492\n",
      "epoch n°741 : train_loss = 1.9942736625671387, val_loss = 0.7064543962478638\n",
      "epoch n°742 : train_loss = 1.990157961845398, val_loss = 0.7042683362960815\n",
      "epoch n°743 : train_loss = 1.9931509494781494, val_loss = 0.7088449597358704\n",
      "epoch n°744 : train_loss = 1.9962239265441895, val_loss = 0.7104719281196594\n",
      "epoch n°745 : train_loss = 1.9880995750427246, val_loss = 0.7049049735069275\n",
      "epoch n°746 : train_loss = 1.9900611639022827, val_loss = 0.7048847675323486\n",
      "epoch n°747 : train_loss = 1.993825078010559, val_loss = 0.7036263942718506\n",
      "epoch n°748 : train_loss = 1.9836859703063965, val_loss = 0.7107787728309631\n",
      "epoch n°749 : train_loss = 1.9996193647384644, val_loss = 0.699221134185791\n",
      "epoch n°750 : train_loss = 1.9938629865646362, val_loss = 0.7045640349388123\n",
      "epoch n°751 : train_loss = 1.9974015951156616, val_loss = 0.7059097290039062\n",
      "epoch n°752 : train_loss = 1.9894009828567505, val_loss = 0.7037999629974365\n",
      "epoch n°753 : train_loss = 1.989249587059021, val_loss = 0.7058679461479187\n",
      "epoch n°754 : train_loss = 1.9972420930862427, val_loss = 0.7053713202476501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°755 : train_loss = 1.9899845123291016, val_loss = 0.6993194222450256\n",
      "epoch n°756 : train_loss = 1.9963915348052979, val_loss = 0.7071478366851807\n",
      "epoch n°757 : train_loss = 1.9911328554153442, val_loss = 0.7022175788879395\n",
      "epoch n°758 : train_loss = 1.9893194437026978, val_loss = 0.7044041752815247\n",
      "epoch n°759 : train_loss = 1.9910963773727417, val_loss = 0.6999436616897583\n",
      "epoch n°760 : train_loss = 1.9929217100143433, val_loss = 0.7007266283035278\n",
      "epoch n°761 : train_loss = 1.9989320039749146, val_loss = 0.706024706363678\n",
      "epoch n°762 : train_loss = 1.9953511953353882, val_loss = 0.7109140157699585\n",
      "epoch n°763 : train_loss = 1.9882868528366089, val_loss = 0.7048808336257935\n",
      "epoch n°764 : train_loss = 1.9923745393753052, val_loss = 0.706713855266571\n",
      "epoch n°765 : train_loss = 1.9861860275268555, val_loss = 0.7125834226608276\n",
      "epoch n°766 : train_loss = 1.9897210597991943, val_loss = 0.7086006999015808\n",
      "epoch n°767 : train_loss = 1.9899653196334839, val_loss = 0.7116923928260803\n",
      "epoch n°768 : train_loss = 1.9932504892349243, val_loss = 0.7061976790428162\n",
      "epoch n°769 : train_loss = 1.9829667806625366, val_loss = 0.7085961103439331\n",
      "epoch n°770 : train_loss = 1.9913514852523804, val_loss = 0.7042779922485352\n",
      "epoch n°771 : train_loss = 1.9831637144088745, val_loss = 0.7078493237495422\n",
      "epoch n°772 : train_loss = 1.9921592473983765, val_loss = 0.7064804434776306\n",
      "epoch n°773 : train_loss = 1.9802416563034058, val_loss = 0.7030916213989258\n",
      "epoch n°774 : train_loss = 1.9895546436309814, val_loss = 0.7076457738876343\n",
      "epoch n°775 : train_loss = 1.9930144548416138, val_loss = 0.696739912033081\n",
      "epoch n°776 : train_loss = 1.9909062385559082, val_loss = 0.7089595198631287\n",
      "epoch n°777 : train_loss = 1.9801853895187378, val_loss = 0.7019292712211609\n",
      "epoch n°778 : train_loss = 1.9859538078308105, val_loss = 0.7059731483459473\n",
      "epoch n°779 : train_loss = 1.9904062747955322, val_loss = 0.7088579535484314\n",
      "epoch n°780 : train_loss = 1.9937067031860352, val_loss = 0.7012577056884766\n",
      "epoch n°781 : train_loss = 1.9741111993789673, val_loss = 0.7049087882041931\n",
      "epoch n°782 : train_loss = 1.988534688949585, val_loss = 0.7002849578857422\n",
      "epoch n°783 : train_loss = 1.9924931526184082, val_loss = 0.7100739479064941\n",
      "epoch n°784 : train_loss = 1.9894450902938843, val_loss = 0.7058538198471069\n",
      "epoch n°785 : train_loss = 1.984006404876709, val_loss = 0.7045855522155762\n",
      "epoch n°786 : train_loss = 1.9919359683990479, val_loss = 0.7068116664886475\n",
      "epoch n°787 : train_loss = 1.9937502145767212, val_loss = 0.7057600021362305\n",
      "epoch n°788 : train_loss = 1.98811936378479, val_loss = 0.7029731869697571\n",
      "epoch n°789 : train_loss = 1.9917289018630981, val_loss = 0.6972644925117493\n",
      "epoch n°790 : train_loss = 1.9895634651184082, val_loss = 0.6999760866165161\n",
      "epoch n°791 : train_loss = 1.9744518995285034, val_loss = 0.7021133303642273\n",
      "epoch n°792 : train_loss = 1.98257315158844, val_loss = 0.7125024199485779\n",
      "epoch n°793 : train_loss = 2.000145435333252, val_loss = 0.7108340859413147\n",
      "epoch n°794 : train_loss = 1.979081630706787, val_loss = 0.7093963623046875\n",
      "epoch n°795 : train_loss = 1.9899547100067139, val_loss = 0.7056789398193359\n",
      "epoch n°796 : train_loss = 1.9939521551132202, val_loss = 0.7030655741691589\n",
      "epoch n°797 : train_loss = 1.988315463066101, val_loss = 0.7079221606254578\n",
      "epoch n°798 : train_loss = 1.9976558685302734, val_loss = 0.7045518755912781\n",
      "epoch n°799 : train_loss = 1.9918806552886963, val_loss = 0.703110933303833\n",
      "epoch n°800 : train_loss = 1.9895243644714355, val_loss = 0.705258309841156\n",
      "epoch n°801 : train_loss = 1.9771050214767456, val_loss = 0.7040619850158691\n",
      "epoch n°802 : train_loss = 1.9910681247711182, val_loss = 0.7052647471427917\n",
      "epoch n°803 : train_loss = 1.9888379573822021, val_loss = 0.7035231590270996\n",
      "epoch n°804 : train_loss = 1.9824544191360474, val_loss = 0.7013753056526184\n",
      "epoch n°805 : train_loss = 1.9867281913757324, val_loss = 0.7073938846588135\n",
      "epoch n°806 : train_loss = 1.997234582901001, val_loss = 0.7012340426445007\n",
      "epoch n°807 : train_loss = 1.9966769218444824, val_loss = 0.7088620066642761\n",
      "epoch n°808 : train_loss = 1.9831576347351074, val_loss = 0.7023757100105286\n",
      "epoch n°809 : train_loss = 1.989998459815979, val_loss = 0.7002604603767395\n",
      "epoch n°810 : train_loss = 1.9788181781768799, val_loss = 0.7069350481033325\n",
      "epoch n°811 : train_loss = 1.9867676496505737, val_loss = 0.7025012969970703\n",
      "epoch n°812 : train_loss = 1.9901047945022583, val_loss = 0.7060971856117249\n",
      "epoch n°813 : train_loss = 1.9860328435897827, val_loss = 0.7036584615707397\n",
      "epoch n°814 : train_loss = 1.9778313636779785, val_loss = 0.6987776756286621\n",
      "epoch n°815 : train_loss = 1.9845967292785645, val_loss = 0.702957034111023\n",
      "epoch n°816 : train_loss = 1.9945652484893799, val_loss = 0.7034430503845215\n",
      "epoch n°817 : train_loss = 1.9890154600143433, val_loss = 0.7070758938789368\n",
      "epoch n°818 : train_loss = 1.9851542711257935, val_loss = 0.6986166834831238\n",
      "epoch n°819 : train_loss = 1.9909580945968628, val_loss = 0.7072060704231262\n",
      "epoch n°820 : train_loss = 1.976801872253418, val_loss = 0.7146342396736145\n",
      "epoch n°821 : train_loss = 1.9894733428955078, val_loss = 0.7104425430297852\n",
      "epoch n°822 : train_loss = 1.9787178039550781, val_loss = 0.7004367709159851\n",
      "epoch n°823 : train_loss = 2.00062894821167, val_loss = 0.7036241292953491\n",
      "epoch n°824 : train_loss = 1.9777072668075562, val_loss = 0.7079941630363464\n",
      "epoch n°825 : train_loss = 1.981475591659546, val_loss = 0.7039011716842651\n",
      "epoch n°826 : train_loss = 1.9822157621383667, val_loss = 0.7040328979492188\n",
      "epoch n°827 : train_loss = 1.9720762968063354, val_loss = 0.7008362412452698\n",
      "epoch n°828 : train_loss = 1.987246036529541, val_loss = 0.7037604451179504\n",
      "epoch n°829 : train_loss = 1.9787896871566772, val_loss = 0.7031944990158081\n",
      "epoch n°830 : train_loss = 1.9903615713119507, val_loss = 0.7037869095802307\n",
      "epoch n°831 : train_loss = 1.988205909729004, val_loss = 0.7046331763267517\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,3,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[512,128,32])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/state_Q_mat.pth\" \n",
    "start = time.time()\n",
    "\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=2032,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 #moving average\n",
    "smothed_Eval = np.convolve(Eval, np.ones(N)/N, mode='valid')\n",
    "plt.plot(np.arange(len(smothed_Eval)),smothed_Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SCORES = eval_model_PID(fname,train_dataloader,test_dataloader,val_dataloader,N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SCORES = eval_model_acc_PID(fname,train_dataloader,test_dataloader,val_dataloader,N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51813229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    model = torch.load(fp).model.to('cpu').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b41e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,y,PID,lPID,pos,length in test_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4802dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = model.to_input(X,PID,pos,length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a4bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 42\n",
    "L = [X_input.detach().cpu()]\n",
    "x = X_input\n",
    "for i,f in model.feature_extractor.named_children():\n",
    "    x = f(x)\n",
    "    L.append(x.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6197950",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for X in L[:-1]:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    im = ax1.imshow(X[idx][0])\n",
    "    im = ax2.imshow(X[idx][1])\n",
    "    fig.colorbar(im)\n",
    "    plt.show()\n",
    "\n",
    "plt.imshow(L[-1][idx])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f5478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f19eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
