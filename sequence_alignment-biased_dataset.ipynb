{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f85dd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import linecache #fast access to a specific file line\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torchinfo\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9bab698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "11.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1eac7b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system') #to avoid issues in the dataloading\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1378df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONT_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "453df495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'Z': 21}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\",\n",
    "            \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "ALPHABET = {ALPHABET[i]:i for i in range(len(ALPHABET))}\n",
    "\n",
    "ALPHABET['-']= 20\n",
    "ALPHABET['Z']= 21\n",
    "\n",
    "print(ALPHABET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba2f8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max 4\n",
    "rep = torch.tensor([4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0, 0, 0, 0, 0, 0, 0,0, 0])\n",
    "#max 8 \n",
    "rep = torch.tensor([8, 8, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 6, 8, 8, 8, 8, 8, 8, 8])\n",
    "rand = torch.tensor([0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0.5, 0.7, 0.1, 0.1, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "475e513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, cont_size=6,div=1400000,verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by precomputing a bunch of data on the sequence families\n",
    "        \"\"\"\n",
    "        self.col_size = 60 #number of column per file (Fasta standard)\n",
    "        self.data_dir = data_dir #directory of the dataset\n",
    "        self.cont_size = cont_size\n",
    "        self.div = div\n",
    "        self.len = 0  #number of families of sequences (1 per file)\n",
    "        self.paths = {} #path of each families in the folder\n",
    "        self.seq_lens = {} #length of each member of the family\n",
    "        self.seq_nums = {} #number of member of the family\n",
    "        self.aa_freqs = {} #frequencies of each symbol in the sequence family\n",
    "        self.p_aa_freqs = {} #frequencies of each symbol in each sequence of a family\n",
    "        \n",
    "        \n",
    "        dir_path = data_dir\n",
    "        count = 0\n",
    "        \n",
    "        # Iterate directory\n",
    "        for path in os.listdir(dir_path):\n",
    "            # check if current path is a file\n",
    "            temp_path = os.path.join(dir_path, path)\n",
    "            if os.path.isfile(temp_path):\n",
    "                n = 0 #number of sequences\n",
    "                p = 0 # used to calculate the length of the sequences\n",
    "                r = 0 # also used this way\n",
    "\n",
    "                l = 0 # length of the seq l = p * self.col_size + r \n",
    "\n",
    "                cpt = 0 # to detect inconsistencies\n",
    "                \n",
    "                with open(temp_path, newline='') as f:\n",
    "                    first_prot = True\n",
    "                    newf = True\n",
    "                    \n",
    "                    aa_freq = torch.zeros(20)\n",
    "                    p_aa_freq = torch.zeros(0)\n",
    "                    \n",
    "                    #parsing the file\n",
    "                    line = f.readline()[:-1]\n",
    "                    while line:\n",
    "                        cpt += 1\n",
    "                        if line[0] == '>': #header line\n",
    "                            if not first_prot:\n",
    "                                p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                            prot_aa_freq = torch.zeros(1,20)\n",
    "                            n += 1\n",
    "                            if newf and not first_prot:\n",
    "                                newf = False\n",
    "                            first_prot = False\n",
    "                                \n",
    "                        else:# sequence line\n",
    "                            if newf and len(line) == self.col_size:\n",
    "                                p += 1\n",
    "\n",
    "                            if newf and len(line) != self.col_size:\n",
    "                                r = len(line)\n",
    "                            for aa in line:\n",
    "                                aa_id = ALPHABET.get(aa,21)\n",
    "                                if aa_id < 20:\n",
    "                                    aa_freq[aa_id] += 1\n",
    "                                    prot_aa_freq[0][aa_id] += 1\n",
    "\n",
    "                            assert len(line) == self.col_size or len(line) == r\n",
    "                        line = f.readline()[:-1]\n",
    "                    \n",
    "                    p_aa_freq = torch.cat([p_aa_freq,prot_aa_freq])\n",
    "                    aa_freq = F.normalize(aa_freq,dim=0,p=1)\n",
    "                    p_aa_freq = F.normalize(p_aa_freq,dim=1,p=1)\n",
    "\n",
    "                l = p*self.col_size + r\n",
    "                \n",
    "                #sanity check\n",
    "                #if the file line count is coherent with the number of sequences and their line count\n",
    "                try: #if r != 0\n",
    "                    assert (p+2) * n == cpt\n",
    "                except: #if r == 0\n",
    "                    assert (p+1) * n == cpt\n",
    "                    assert r == 0\n",
    "                    \n",
    "                \n",
    "                if n>1: #if this is false, we can't find pairs\n",
    "                    self.paths[count] = path\n",
    "                    self.seq_lens[count] = l\n",
    "                    self.seq_nums[count] = n\n",
    "                    self.aa_freqs[count] = aa_freq\n",
    "                    self.p_aa_freqs[count] = p_aa_freq\n",
    "                    count += 1\n",
    "                    \n",
    "                    if verbose and (count % 100 ==0) : print(f\"seen = {count}\")\n",
    "            \n",
    "        self.len = count\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "     \n",
    "    def sample(self, high, low=0, s=1):\n",
    "        sample = np.random.choice(high-low, s, replace=False)\n",
    "        return sample + low\n",
    "    \n",
    "    def __getitem__(self, idx, sample_size='auto',rep=rep,rand=rand): \n",
    "        \"\"\"\n",
    "        input idx of the family of the sample\n",
    "        return a Tensor containing several samples from the family corresponding to the index\n",
    "        \"\"\"\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        PIDs = []\n",
    "        local_PIDs = []\n",
    "        \n",
    "        pfreqs = []\n",
    "        local_pfreqs = []\n",
    "        \n",
    "        lengths = []\n",
    "        \n",
    "        pos = []\n",
    "        \n",
    "        bin_n = len(rep)\n",
    "        \n",
    "        precomputed_pos = []\n",
    "        for i in range(-self.cont_size,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(-self.cont_size,0):\n",
    "            precomputed_pos.append(i)\n",
    "        for i in range(1,self.cont_size+1):\n",
    "            precomputed_pos.append(i)\n",
    "        \n",
    "        precomputed_pos = torch.tensor(precomputed_pos).float()\n",
    "        \n",
    "        data_path = os.path.join(self.data_dir, self.paths[idx])\n",
    "        try:\n",
    "            n = self.seq_nums[idx]\n",
    "            l = self.seq_lens[idx]\n",
    "        except:\n",
    "            print(idx)\n",
    "            pass\n",
    "        \n",
    "        if type(sample_size) != int:\n",
    "            coef = round((n**2 * l)/self.div) \n",
    "            sample_size = max(1,coef)\n",
    "        \n",
    "        p = l // self.col_size\n",
    "        r = l % self.col_size # l = p * q + r\n",
    "        sequence_line_count = p+2 if r else p+1\n",
    "\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            i,j = self.sample(n,s=2)\n",
    "\n",
    "            start_i = 2 + (sequence_line_count)*i #start line of protein i\n",
    "            start_j = 2 + (sequence_line_count)*j #start line of protein j\n",
    "            \n",
    "            seq_i = ''\n",
    "            seq_j = ''\n",
    "            \n",
    "            PID_ij = 0\n",
    "            \n",
    "            l_ij = 0\n",
    "            for offset in range(sequence_line_count-1):\n",
    "                line_i = linecache.getline(data_path, (start_i + offset))[:-1]\n",
    "                line_j = linecache.getline(data_path, (start_j + offset))[:-1]\n",
    "                for aa_i, aa_j in zip(line_i,line_j):\n",
    "                    if aa_i == aa_j:\n",
    "                        if aa_i != '-':\n",
    "                            PID_ij += 1\n",
    "                            seq_i += aa_i\n",
    "                            seq_j += aa_j        \n",
    "                    else:\n",
    "                        seq_i += aa_i\n",
    "                        seq_j += aa_j\n",
    "                    \n",
    "                    if aa_i != '-' and aa_j != '-':\n",
    "                        l_ij += 1\n",
    "            \n",
    "            try:\n",
    "                PID_ij = PID_ij/l_ij\n",
    "            except:\n",
    "                PID_ij = 0 #case 0/0\n",
    "            \n",
    "            align_l = len(seq_i)\n",
    "            possible_k = []\n",
    "            for k,(a_i,a_j) in enumerate(zip(seq_i,seq_j)):   \n",
    "                if ALPHABET.get(a_i,21) < 20 and ALPHABET.get(a_j,21) < 20:\n",
    "                    possible_k.append(k)\n",
    "            \n",
    "            \n",
    "            bin_idx = int(PID_ij//(1/bin_n))\n",
    "            rep_number = rep[bin_idx].clone()\n",
    "            if torch.rand(1) < rand[bin_idx]:\n",
    "                rep_number+=1\n",
    "            \n",
    "            for _ in range(rep_number):\n",
    "                try:   \n",
    "                    k = np.random.choice(possible_k)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                lengths.append(align_l)\n",
    "                pos_ij = (k + precomputed_pos)\n",
    "                pos.append(pos_ij)\n",
    "\n",
    "                window_i = ''\n",
    "                window_j = ''\n",
    "\n",
    "                for w in range(k-self.cont_size,k+self.cont_size+1):\n",
    "                    if w < 0 or w >= align_l: #case of the edges\n",
    "                        window_i += 'Z'\n",
    "                        window_j += 'Z'\n",
    "                    else:\n",
    "                        window_i += seq_i[w]\n",
    "                        window_j += seq_j[w]\n",
    "\n",
    "                y_j = ALPHABET.get(window_j[self.cont_size], 21) # 'Z' is the default value for rare AA\n",
    "                X_i = [ALPHABET.get(i, 21) for i in (window_i+window_j[:self.cont_size]+window_j[self.cont_size+1:])]       \n",
    "\n",
    "                X.append(X_i)\n",
    "                y.append(y_j)\n",
    "                PIDs.append(PID_ij)\n",
    "                local_PID_ij = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if AA1 == AA2 and ALPHABET.get(AA1,21) < 20)\n",
    "\n",
    "                loc_comp = sum(1 for AA1,AA2 in zip(window_i, window_j[:self.cont_size]) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20) \\\n",
    "                             + sum(1 for AA1,AA2 in zip(reversed(window_i), reversed(window_j[self.cont_size+1:])) if ALPHABET.get(AA1,21) < 20 and ALPHABET.get(AA2,21) < 20)\n",
    "                try:\n",
    "                    tmp = local_PID_ij/loc_comp\n",
    "                except:\n",
    "                    tmp = 0\n",
    "\n",
    "                local_PIDs.append(tmp)\n",
    "                pfreqs.append(self.aa_freqs[idx])\n",
    "                p_i_freqs = self.p_aa_freqs[idx][i]\n",
    "                p_j_freqs = self.p_aa_freqs[idx][j]\n",
    "\n",
    "                local_pfreqs.append(torch.stack((p_i_freqs,p_j_freqs)))\n",
    "\n",
    "                assert y_j < 20\n",
    "                assert X_i[self.cont_size] < 20\n",
    "            \n",
    "        linecache.clearcache()   \n",
    "        X = torch.tensor(X)\n",
    "        try:\n",
    "            X = F.one_hot(X,22)[:,:,0:-1]\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        if len(pos) == 0:\n",
    "            pos = torch.tensor(pos)\n",
    "        else:\n",
    "            pos = torch.stack(pos)\n",
    "        pfreqs = torch.stack(pfreqs)\n",
    "        local_pfreqs = torch.stack(local_pfreqs)\n",
    "        X = X.float()\n",
    "        y = torch.tensor(y)\n",
    "        PIDs = torch.tensor(PIDs)\n",
    "        local_PIDs = torch.tensor(local_PIDs)\n",
    "        lengths = torch.tensor(lengths)\n",
    "        out = X,y.long(),PIDs,local_PIDs,pfreqs,local_pfreqs,pos,lengths\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d9b12fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\\ntest_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\\nval_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\\n\\nfname = \\'data/train_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(train_dataset,fp)\\n    \\nfname = \\'data/test_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(test_dataset,fp)\\n    \\nfname = \\'data/val_dataset1.pth\\'\\nsavepath = Path(fname)\\nwith savepath.open(\"wb\") as fp:\\n    torch.save(val_dataset,fp)\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run This for new data\n",
    "\"\"\"\n",
    "train_dataset = MyDataset(r\"data/train_data\",cont_size = 6)\n",
    "test_dataset = MyDataset(r\"data/test_data\",cont_size = 6,div=700000)\n",
    "val_dataset = MyDataset(r\"data/val_data\",cont_size = 6,div=700000)\n",
    "\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(train_dataset,fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(test_dataset,fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"wb\") as fp:\n",
    "    torch.save(val_dataset,fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b79b8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13219\n",
      "2838\n",
      "2826\n"
     ]
    }
   ],
   "source": [
    "#To load datasets with all features computed\n",
    "fname = 'data/train_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    train_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/test_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    test_dataset = torch.load(fp)\n",
    "    \n",
    "fname = 'data/val_dataset1.pth'\n",
    "savepath = Path(fname)\n",
    "with savepath.open(\"rb\") as fp:\n",
    "    val_dataset = torch.load(fp)\n",
    "    \n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))\n",
    "\n",
    "train_dataset.cont_size = CONT_SIZE\n",
    "test_dataset.cont_size = CONT_SIZE\n",
    "val_dataset.cont_size = CONT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "378c93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\"\n",
    "    Transforms a list of tensors to a batch tensor\n",
    "    \"\"\"\n",
    "    data = torch.cat([item[0] for item in batch],dim=0)\n",
    "    target = torch.cat([item[1] for item in batch],dim=0)\n",
    "    PID = torch.cat([item[2] for item in batch],dim=0)\n",
    "    lPID = torch.cat([item[3] for item in batch],dim=0)\n",
    "    pfreqs = torch.cat([item[4] for item in batch],dim=0)\n",
    "    lpfreqs = torch.cat([item[5] for item in batch],dim=0)\n",
    "    pos = torch.cat([item[6] for item in batch],dim=0)\n",
    "    length = torch.cat([item[7] for item in batch],dim=0)\n",
    "    return data, target, PID, lPID,pfreqs,lpfreqs, pos, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66f5e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True,collate_fn=my_collate,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c0f4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,out_features=None,num_heads=8,head_dims=24):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.Q_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.K_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        self.V_w = nn.Linear(in_features,num_heads*head_dims,bias=False)\n",
    "        \n",
    "        self.att = nn.MultiheadAttention(num_heads*head_dims,num_heads=num_heads,batch_first=True)\n",
    "        self.lin = nn.Linear(num_heads*head_dims,out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Q = self.Q_w(x)\n",
    "        K = self.K_w(x)\n",
    "        V = self.V_w(x)\n",
    "        out,_ = self.att(Q,K,V,need_weights=False)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d97a7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP Block of the Transformer\n",
    "    \"\"\"    \n",
    "    def __init__(self,in_features,out_features=None,wide_factor=4):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_dim = wide_factor * in_features\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features,hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        self.lin2 = nn.Linear(hidden_dim,out_features)\n",
    "        self.act2 = nn.GELU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.act1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.act2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e2f0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(from the timm library)\n",
    "def drop_path(x, drop_prob: float = 0.1, training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None, scale_by_keep=True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb41b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Block of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads=8,head_dims=24,wide_factor=4,drop=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.att_block = AttBlock(in_features,num_heads=num_heads,head_dims=head_dims)\n",
    "        self.ff = FeedFoward(in_features,wide_factor=wide_factor)\n",
    "        self.drop_path = DropPath(drop)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_features)\n",
    "        self.norm2 = nn.LayerNorm(in_features)\n",
    "    def forward(self,x):\n",
    "        out = x + self.drop_path(self.att_block(x))\n",
    "        out = self.norm1(out)\n",
    "        out = out + self.drop_path(self.ff(out))\n",
    "        out = self.norm2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c55b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier Head of the Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,clf_dims,out_size,seq_len):\n",
    "        super().__init__()\n",
    "        in_dim = seq_len*in_features\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        layers = []\n",
    "        for out_dim in clf_dims:\n",
    "            layers.append(nn.Linear(in_dim,out_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "            in_dim = out_dim\n",
    "\n",
    "        layers.append(nn.Linear(in_dim,out_size))\n",
    "        \n",
    "        self.clf = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.reshape((-1,self.in_dim))\n",
    "        \n",
    "        out = self.clf(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a397a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-like neural net\n",
    "    \"\"\"\n",
    "    def __init__(self,in_features,num_heads,head_dims,wide_factors,drops,input_dim=21,out_size=20,seq_len=4*CONT_SIZE+6,clf_dims=[256,64],cont_size=CONT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        blocks = []\n",
    "        for n_h, h_d,w,d in zip(num_heads,head_dims,wide_factors,drops):\n",
    "            blocks.append(Block(in_features,num_heads=n_h,head_dims=h_d,wide_factor=w,drop=d))\n",
    "        self.feature_extractor = nn.Sequential(*blocks)\n",
    "        self.in_features = in_features\n",
    "        self.input_dim = input_dim\n",
    "        self.clf = Classifier_Head(in_features,clf_dims,out_size=out_size,seq_len=seq_len)\n",
    "        \n",
    "        self.cont_size=cont_size\n",
    "        \n",
    "        sp = Path(\"data/freq.pth\")\n",
    "        with sp.open(\"rb\") as fp:\n",
    "            self.F = nn.Parameter(torch.log(torch.load(fp)))\n",
    "            \n",
    "        pid_layers = [nn.Linear(1,in_features),nn.Sigmoid()]\n",
    "        self.pid_l = nn.Sequential(*pid_layers)\n",
    "    \n",
    "    def to_input(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_idx = torch.argmax(x[:,self.cont_size],dim=1)\n",
    "        seq1 = x[:,:2*self.cont_size+1]\n",
    "        y_freq = F.pad(F.softmax(self.F[X_idx],dim=1).unsqueeze(1), pad=(0, 1), mode='constant', value=0) \n",
    "        seq2 = torch.cat((x[:,2*self.cont_size+1:3*self.cont_size+1],y_freq,x[:,3*self.cont_size+1:]),dim=1)\n",
    "        aa_pos = pos[:,:2*self.cont_size+1]/length.unsqueeze(1)\n",
    "        aa_pos = aa_pos.unsqueeze(2)\n",
    "        pos_dim = (self.in_features-self.input_dim-1)//2\n",
    "        \n",
    "        for i in range(pos_dim): #positionnal_encoding\n",
    "            p = torch.cos(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            ip = torch.sin(pos[:,:2*self.cont_size+1]/(4**(2*i/pos_dim))).unsqueeze(2)\n",
    "            aa_pos = torch.cat([aa_pos,p,ip],dim=2)\n",
    "        \n",
    "        seq1 = torch.cat([seq1,aa_pos],dim=2)\n",
    "        seq2 = torch.cat([seq2,aa_pos],dim=2)\n",
    "        X = torch.cat([seq1,seq2],dim=1)\n",
    "        \n",
    "        pad = (0,self.in_features-self.input_dim+1)\n",
    "        pf = F.pad(pfreqs.unsqueeze(1),pad =pad, mode='constant', value=0)\n",
    "        pid = self.pid_l(PID.unsqueeze(1)).unsqueeze(1)\n",
    "        lpf = F.pad(lpfreqs,pad=pad, mode='constant', value=0)\n",
    "\n",
    "        X_input = torch.cat([X,pf,lpf,pid],dim=1)\n",
    "        \n",
    "        return X_input\n",
    "    \n",
    "    def forward(self,x,PID,pfreqs,lpfreqs,pos,length):\n",
    "        X_input = self.to_input(x,PID,pfreqs,lpfreqs,pos,length)\n",
    "        features = self.feature_extractor(X_input)\n",
    "        out = self.clf(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a48c6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(input_size,N,head,head_dim,wide_factor,drop_prob):\n",
    "    \"\"\"\n",
    "    Returns the initialization parameters of the Transformer\n",
    "    \"\"\"\n",
    "    return input_size, [head for _ in range(N)], [head_dim for _ in range(N)], [wide_factor for _ in range(N)], [drop_prob for _ in range(N)], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b97a85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "AttNet                                        [1, 20]                   400\n",
       "├─Sequential: 1-1                             [1, 32]                   --\n",
       "│    └─Linear: 2-1                            [1, 32]                   64\n",
       "│    └─Sigmoid: 2-2                           [1, 32]                   --\n",
       "├─Sequential: 1-2                             [1, 54, 32]               --\n",
       "│    └─Block: 2-3                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-1                     [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-2                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-3                    [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-4                   [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-5                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-6                    [1, 54, 32]               64\n",
       "│    └─Block: 2-4                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-7                     [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-8                     [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-9                    [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-10                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-11                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-12                   [1, 54, 32]               64\n",
       "│    └─Block: 2-5                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-13                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-14                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-15                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-16                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-17                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-18                   [1, 54, 32]               64\n",
       "│    └─Block: 2-6                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-19                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-20                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-21                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-22                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-23                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-24                   [1, 54, 32]               64\n",
       "│    └─Block: 2-7                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-25                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-26                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-27                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-28                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-29                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-30                   [1, 54, 32]               64\n",
       "│    └─Block: 2-8                             [1, 54, 32]               --\n",
       "│    │    └─AttBlock: 3-31                    [1, 54, 32]               295,968\n",
       "│    │    └─DropPath: 3-32                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-33                   [1, 54, 32]               64\n",
       "│    │    └─FeedFoward: 3-34                  [1, 54, 32]               8,352\n",
       "│    │    └─DropPath: 3-35                    [1, 54, 32]               --\n",
       "│    │    └─LayerNorm: 3-36                   [1, 54, 32]               64\n",
       "├─Classifier_Head: 1-3                        [1, 20]                   --\n",
       "│    └─Sequential: 2-9                        [1, 20]                   --\n",
       "│    │    └─Linear: 3-37                      [1, 1024]                 1,770,496\n",
       "│    │    └─GELU: 3-38                        [1, 1024]                 --\n",
       "│    │    └─Dropout: 3-39                     [1, 1024]                 --\n",
       "│    │    └─Linear: 3-40                      [1, 256]                  262,400\n",
       "│    │    └─GELU: 3-41                        [1, 256]                  --\n",
       "│    │    └─Dropout: 3-42                     [1, 256]                  --\n",
       "│    │    └─Linear: 3-43                      [1, 64]                   16,448\n",
       "│    │    └─GELU: 3-44                        [1, 64]                   --\n",
       "│    │    └─Dropout: 3-45                     [1, 64]                   --\n",
       "│    │    └─Linear: 3-46                      [1, 20]                   1,300\n",
       "===============================================================================================\n",
       "Total params: 3,877,796\n",
       "Trainable params: 3,877,796\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.30\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 2.67\n",
       "Params size (MB): 9.19\n",
       "Estimated Total Size (MB): 11.86\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model,[(1,49,21),(1,),(1,20),(1,2,20),(1,49),(1,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "313c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \"\"\"\n",
    "    Used for checkpointing\n",
    "    \"\"\"\n",
    "    def __init__(self,model,optim,scheduler):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.scheduler = scheduler\n",
    "        self.epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8dab16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        \n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "\n",
    "        score_train = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_train = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in test:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_test = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_test = }\\n\")\n",
    "\n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos, length in val:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                lPID = lPID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "        score_val = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "        print(f\"{score_val = }\\n\")\n",
    "    \n",
    "    return score_train, score_test, score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0982994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the MSE (Brier Score) of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_acc_PID(fname,train,test,val,N=10):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of the model for different slices of PID in the data\n",
    "    \"\"\"\n",
    "    savepath = Path(fname)\n",
    "    with savepath.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "    bin_n = 20\n",
    "    state.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print('EVALUATING ON TRAIN DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "                \n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_train = torch.zeros(bin_n)\n",
    "        bar_train = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_train[i] += 1\n",
    "            Losses_train[i] += n\n",
    "        Losses_train = Losses_train/bar_train\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_train,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Train dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON TEST DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in test:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_test = torch.zeros(bin_n)\n",
    "        bar_test = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_test[i] += 1\n",
    "            Losses_test[i] += n\n",
    "        Losses_test = Losses_test/bar_test\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_test,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Test dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "        print('EVALUATING ON VAL DATA : ')\n",
    "        eval_losses = []\n",
    "        PIDS = []\n",
    "        for _ in range(N):\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in val:\n",
    "                PIDS.append(PID)\n",
    "  \n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = torch.argmax(y_hat,dim=1)\n",
    "\n",
    "                eval_l = (y == y_hat).long()\n",
    "                eval_losses.append(eval_l.detach().cpu())\n",
    "        PIDS = torch.cat(PIDS)\n",
    "        BINS = (PIDS/(1/bin_n)).long()\n",
    "        scores = torch.cat(eval_losses).detach().cpu()\n",
    "        \n",
    "        Losses_val = torch.zeros(bin_n)\n",
    "        bar_val = torch.zeros(bin_n)\n",
    "        for i,n in zip(BINS,scores):\n",
    "            bar_val[i] += 1\n",
    "            Losses_val[i] += n\n",
    "        Losses_val = Losses_val/bar_val\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),Losses_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Brier Score wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        plt.bar(np.arange(bin_n)/bin_n+(1/(2*bin_n)),bar_val,width=1/(bin_n+1))\n",
    "        plt.title(\"Example count wrt PID on Val dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return Losses_train, bar_train, Losses_test, bar_test, Losses_val,bar_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00349869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_loader,val_loader,epochs=101,fname=\"models/state.pth\",fnameb=None,state=None,last_epoch_sched=float('inf'),use_mut=True,cont_size=CONT_SIZE):\n",
    "    \"\"\"\n",
    "    Trains a model\n",
    "    \"\"\"\n",
    "    #to get the best model\n",
    "    best = float('inf')\n",
    "    \n",
    "    #getting the acceleration device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    #loading from previous checkpoint\n",
    "    if fnameb is None:\n",
    "        fnameb = fname[:-4] + '_best' +fname[-4:]\n",
    "        \n",
    "    savepath = Path(fname)\n",
    "    if savepath.is_file():\n",
    "        with savepath.open(\"rb\") as fp:\n",
    "            state = torch.load(fp)\n",
    "    else:\n",
    "        if state is None:\n",
    "            model = AttNet(22,[8,8,8],[24,24,24],[4,4,4],[0.1,0.1,0.1])\n",
    "            model = model.to(device)\n",
    "            optim = torch.optim.AdamW(model.parameters(),lr = 0.0001)\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "            state = State(model,optim,scheduler)\n",
    "    \n",
    "    \n",
    "    Loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "    LossMut = nn.BCELoss(reduction='sum')\n",
    "    EvalLoss = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    #for logs\n",
    "    List_Loss = []\n",
    "    Eval_Loss = []\n",
    "    for epoch in range(state.epoch, epochs):\n",
    "        batch_losses = []\n",
    "        state.model.train()\n",
    "        for X,y, PID, lPID,pfreqs,lpfreqs,pos,length in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            PID = PID.to(device)\n",
    "            pos = pos.to(device)\n",
    "            pfreqs = pfreqs.to(device)\n",
    "            lpfreqs = lpfreqs.to(device)\n",
    "            length = length.to(device)\n",
    "            \n",
    "            state.optim.zero_grad()\n",
    "            y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "            \n",
    "\n",
    "            if use_mut:\n",
    "                X_idx = torch.argmax(X[:,cont_size],dim=1)\n",
    "                y_true = (X_idx == y).float().unsqueeze(1)  #0 if a mutation happens else 1 \n",
    "                y_pred = F.softmax(y_hat,dim=1)\n",
    "                y_pred = y_pred.gather(1,X_idx.view(-1,1)) #the Xth component of y_hat should be predicting ^\n",
    "                l = (Loss(y_hat,y) + LossMut(y_pred,y_true))/475 #for max rep 1 4 8 : use 311 425 475\n",
    "            else:\n",
    "                l = Loss(y_hat,y)/475 #for max rep 1 4 8 : use 311 425 475\n",
    "            l.backward()\n",
    "            state.optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_losses.append(l.detach().cpu())\n",
    "        List_Loss.append(torch.mean(torch.stack(batch_losses)).detach().cpu())\n",
    "        state.epoch = epoch + 1\n",
    "        if epoch < last_epoch_sched:\n",
    "            state.scheduler.step()\n",
    "        \n",
    "        savepath = Path(fname)\n",
    "        with savepath.open(\"wb\") as fp:\n",
    "            torch.save(state,fp)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            eval_losses = [] \n",
    "            state.model.eval()\n",
    "            for X,y, PID, lPID,pfreqs,lpfreqs, pos,length in val_loader:\n",
    "\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                PID = PID.to(device)\n",
    "                pos = pos.to(device)\n",
    "                pfreqs = pfreqs.to(device)\n",
    "                lpfreqs = lpfreqs.to(device)\n",
    "                length = length.to(device)\n",
    "\n",
    "                y_hat = state.model(X,PID,pfreqs,lpfreqs,pos,length)\n",
    "                y_hat = F.softmax(y_hat,dim=1)\n",
    "\n",
    "                y = F.one_hot(y, 20)\n",
    "                eval_l = (y_hat-y)**2\n",
    "                eval_losses.append(torch.sum(eval_l,dim=1).detach().cpu())\n",
    "    \n",
    "            score = torch.mean(torch.cat(eval_losses)).detach().cpu().item()\n",
    "            Eval_Loss.append(score)\n",
    "        \n",
    "        if score < best :\n",
    "            best = score\n",
    "            savepath = Path(fnameb)\n",
    "            with savepath.open(\"wb\") as fp:\n",
    "                torch.save(state,fp)\n",
    "        \n",
    "        print(f\"epoch n°{epoch} : train_loss = {List_Loss[-1]}, val_loss = {Eval_Loss[-1]}\") \n",
    "\n",
    "\n",
    "        \n",
    "    return List_Loss,Eval_Loss,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°0 : train_loss = 2.7078588008880615, val_loss = 0.8323175311088562\n",
      "epoch n°1 : train_loss = 2.255880117416382, val_loss = 0.7608538866043091\n",
      "epoch n°2 : train_loss = 2.1833884716033936, val_loss = 0.7476219534873962\n",
      "epoch n°3 : train_loss = 2.145447254180908, val_loss = 0.7499938011169434\n",
      "epoch n°4 : train_loss = 2.1305720806121826, val_loss = 0.7456762194633484\n",
      "epoch n°5 : train_loss = 2.1098127365112305, val_loss = 0.7507891654968262\n",
      "epoch n°6 : train_loss = 2.118419885635376, val_loss = 0.7451786398887634\n",
      "epoch n°7 : train_loss = 2.0947751998901367, val_loss = 0.7464073300361633\n",
      "epoch n°8 : train_loss = 2.087336540222168, val_loss = 0.7500407099723816\n",
      "epoch n°9 : train_loss = 2.090449094772339, val_loss = 0.7475588917732239\n",
      "epoch n°10 : train_loss = 2.0792298316955566, val_loss = 0.7383466958999634\n",
      "epoch n°11 : train_loss = 2.0862302780151367, val_loss = 0.740938663482666\n",
      "epoch n°12 : train_loss = 2.084897994995117, val_loss = 0.7436497807502747\n",
      "epoch n°13 : train_loss = 2.0796310901641846, val_loss = 0.739556610584259\n",
      "epoch n°14 : train_loss = 2.062603712081909, val_loss = 0.7466696500778198\n",
      "epoch n°15 : train_loss = 2.0741374492645264, val_loss = 0.7383598685264587\n",
      "epoch n°16 : train_loss = 2.1010191440582275, val_loss = 0.7431247234344482\n",
      "epoch n°17 : train_loss = 2.0910098552703857, val_loss = 0.7423720955848694\n",
      "epoch n°18 : train_loss = 2.1060121059417725, val_loss = 0.7423264384269714\n",
      "epoch n°19 : train_loss = 2.095402717590332, val_loss = 0.7440365552902222\n",
      "epoch n°20 : train_loss = 2.0784716606140137, val_loss = 0.7452077865600586\n",
      "epoch n°21 : train_loss = 2.081087112426758, val_loss = 0.739569365978241\n",
      "epoch n°22 : train_loss = 2.0755629539489746, val_loss = 0.7424851059913635\n",
      "epoch n°23 : train_loss = 2.058950662612915, val_loss = 0.7361321449279785\n",
      "epoch n°24 : train_loss = 2.0629899501800537, val_loss = 0.7398276925086975\n",
      "epoch n°25 : train_loss = 2.0682120323181152, val_loss = 0.7480189204216003\n",
      "epoch n°26 : train_loss = 2.061415910720825, val_loss = 0.740049421787262\n",
      "epoch n°27 : train_loss = 2.0520074367523193, val_loss = 0.7407849431037903\n",
      "epoch n°28 : train_loss = 2.047792673110962, val_loss = 0.737342119216919\n",
      "epoch n°29 : train_loss = 2.0486502647399902, val_loss = 0.7389453649520874\n",
      "epoch n°30 : train_loss = 2.0428574085235596, val_loss = 0.743157148361206\n",
      "epoch n°31 : train_loss = 2.0444605350494385, val_loss = 0.7406463027000427\n",
      "epoch n°32 : train_loss = 2.058072090148926, val_loss = 0.7368501424789429\n",
      "epoch n°33 : train_loss = 2.038247585296631, val_loss = 0.7392164468765259\n",
      "epoch n°34 : train_loss = 2.039315700531006, val_loss = 0.7342831492424011\n",
      "epoch n°35 : train_loss = 2.032702684402466, val_loss = 0.7364310622215271\n",
      "epoch n°36 : train_loss = 2.032623291015625, val_loss = 0.7361570596694946\n",
      "epoch n°37 : train_loss = 2.0288586616516113, val_loss = 0.7415787577629089\n",
      "epoch n°38 : train_loss = 2.037546396255493, val_loss = 0.7346189022064209\n",
      "epoch n°39 : train_loss = 2.0177392959594727, val_loss = 0.7370137572288513\n",
      "epoch n°40 : train_loss = 2.0204906463623047, val_loss = 0.7346979975700378\n",
      "epoch n°41 : train_loss = 2.016232490539551, val_loss = 0.7381957769393921\n",
      "epoch n°42 : train_loss = 2.0215818881988525, val_loss = 0.734325647354126\n",
      "epoch n°43 : train_loss = 2.0236756801605225, val_loss = 0.730863630771637\n",
      "epoch n°44 : train_loss = 2.015105962753296, val_loss = 0.731451153755188\n",
      "epoch n°45 : train_loss = 2.0199737548828125, val_loss = 0.73201584815979\n",
      "epoch n°46 : train_loss = 2.027567148208618, val_loss = 0.7340981364250183\n",
      "epoch n°47 : train_loss = 2.0116586685180664, val_loss = 0.7315365076065063\n",
      "epoch n°48 : train_loss = 2.04533314704895, val_loss = 0.7351788282394409\n",
      "epoch n°49 : train_loss = 2.034226179122925, val_loss = 0.7346731424331665\n",
      "epoch n°50 : train_loss = 2.051867723464966, val_loss = 0.740657389163971\n",
      "epoch n°51 : train_loss = 2.0233497619628906, val_loss = 0.7379770278930664\n",
      "epoch n°52 : train_loss = 2.0348188877105713, val_loss = 0.7348526120185852\n",
      "epoch n°53 : train_loss = 2.0397698879241943, val_loss = 0.7382081151008606\n",
      "epoch n°54 : train_loss = 2.032442569732666, val_loss = 0.7427071332931519\n",
      "epoch n°55 : train_loss = 2.0329666137695312, val_loss = 0.7392077445983887\n",
      "epoch n°56 : train_loss = 2.0356616973876953, val_loss = 0.7338465452194214\n",
      "epoch n°57 : train_loss = 2.033273696899414, val_loss = 0.7345101237297058\n",
      "epoch n°58 : train_loss = 2.0311648845672607, val_loss = 0.7368559837341309\n",
      "epoch n°59 : train_loss = 2.031989097595215, val_loss = 0.7305474281311035\n",
      "epoch n°60 : train_loss = 2.0159218311309814, val_loss = 0.7331220507621765\n",
      "epoch n°61 : train_loss = 2.0164685249328613, val_loss = 0.7318980097770691\n",
      "epoch n°62 : train_loss = 2.041027307510376, val_loss = 0.7387858629226685\n",
      "epoch n°63 : train_loss = 2.015683650970459, val_loss = 0.7394298911094666\n",
      "epoch n°64 : train_loss = 2.0288314819335938, val_loss = 0.7297936677932739\n",
      "epoch n°65 : train_loss = 2.0357730388641357, val_loss = 0.7332558631896973\n",
      "epoch n°66 : train_loss = 2.024219274520874, val_loss = 0.7384133338928223\n",
      "epoch n°67 : train_loss = 2.026099681854248, val_loss = 0.7338805794715881\n",
      "epoch n°68 : train_loss = 2.0366928577423096, val_loss = 0.7348796129226685\n",
      "epoch n°69 : train_loss = 2.0099642276763916, val_loss = 0.7364528179168701\n",
      "epoch n°70 : train_loss = 2.0135016441345215, val_loss = 0.7328992486000061\n",
      "epoch n°71 : train_loss = 2.006889820098877, val_loss = 0.7341403961181641\n",
      "epoch n°72 : train_loss = 2.0201194286346436, val_loss = 0.7380486726760864\n",
      "epoch n°73 : train_loss = 2.019965648651123, val_loss = 0.7323510646820068\n",
      "epoch n°74 : train_loss = 2.01312518119812, val_loss = 0.7332122921943665\n",
      "epoch n°75 : train_loss = 2.018911123275757, val_loss = 0.7351807951927185\n",
      "epoch n°76 : train_loss = 2.006075620651245, val_loss = 0.731311559677124\n",
      "epoch n°77 : train_loss = 2.0104970932006836, val_loss = 0.7300093173980713\n",
      "epoch n°78 : train_loss = 2.0127153396606445, val_loss = 0.7351434230804443\n",
      "epoch n°79 : train_loss = 1.9933654069900513, val_loss = 0.7340551614761353\n",
      "epoch n°80 : train_loss = 2.0011661052703857, val_loss = 0.7323158979415894\n",
      "epoch n°81 : train_loss = 1.9977660179138184, val_loss = 0.7356647849082947\n",
      "epoch n°82 : train_loss = 1.9956623315811157, val_loss = 0.7318819761276245\n",
      "epoch n°83 : train_loss = 1.9957942962646484, val_loss = 0.7311955094337463\n",
      "epoch n°84 : train_loss = 2.002824306488037, val_loss = 0.7322789430618286\n",
      "epoch n°85 : train_loss = 1.9965367317199707, val_loss = 0.7339008450508118\n",
      "epoch n°86 : train_loss = 1.9904593229293823, val_loss = 0.7259196043014526\n",
      "epoch n°87 : train_loss = 1.9870202541351318, val_loss = 0.7310476303100586\n",
      "epoch n°88 : train_loss = 1.9962396621704102, val_loss = 0.7259839177131653\n",
      "epoch n°89 : train_loss = 1.9920045137405396, val_loss = 0.7298808097839355\n",
      "epoch n°90 : train_loss = 1.9889017343521118, val_loss = 0.7295523285865784\n",
      "epoch n°91 : train_loss = 1.99339759349823, val_loss = 0.7307631373405457\n",
      "epoch n°92 : train_loss = 1.98912513256073, val_loss = 0.7275831699371338\n",
      "epoch n°93 : train_loss = 1.981501579284668, val_loss = 0.734948456287384\n",
      "epoch n°94 : train_loss = 1.9931416511535645, val_loss = 0.7322996258735657\n",
      "epoch n°95 : train_loss = 1.9841777086257935, val_loss = 0.7286194562911987\n",
      "epoch n°96 : train_loss = 1.987252950668335, val_loss = 0.7302200198173523\n",
      "epoch n°97 : train_loss = 1.9957737922668457, val_loss = 0.7272306680679321\n",
      "epoch n°98 : train_loss = 1.978444218635559, val_loss = 0.730586588382721\n",
      "epoch n°99 : train_loss = 1.986222743988037, val_loss = 0.725022554397583\n",
      "epoch n°100 : train_loss = 1.9910390377044678, val_loss = 0.7282423377037048\n",
      "epoch n°101 : train_loss = 1.9839603900909424, val_loss = 0.7248582243919373\n",
      "epoch n°102 : train_loss = 1.9721449613571167, val_loss = 0.7316873669624329\n",
      "epoch n°103 : train_loss = 1.9841371774673462, val_loss = 0.733915388584137\n",
      "epoch n°104 : train_loss = 1.9810811281204224, val_loss = 0.7320826053619385\n",
      "epoch n°105 : train_loss = 1.9780648946762085, val_loss = 0.7287593483924866\n",
      "epoch n°106 : train_loss = 1.9880425930023193, val_loss = 0.7298141121864319\n",
      "epoch n°107 : train_loss = 1.9851042032241821, val_loss = 0.724096417427063\n",
      "epoch n°108 : train_loss = 1.9931635856628418, val_loss = 0.7286596298217773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°109 : train_loss = 1.9840521812438965, val_loss = 0.7279499769210815\n",
      "epoch n°110 : train_loss = 1.9725931882858276, val_loss = 0.7236574292182922\n",
      "epoch n°111 : train_loss = 1.9734901189804077, val_loss = 0.726706326007843\n",
      "epoch n°112 : train_loss = 2.007477045059204, val_loss = 0.7333267331123352\n",
      "epoch n°113 : train_loss = 2.001431465148926, val_loss = 0.7292020320892334\n",
      "epoch n°114 : train_loss = 2.001767158508301, val_loss = 0.729005753993988\n",
      "epoch n°115 : train_loss = 1.9929035902023315, val_loss = 0.7324731349945068\n",
      "epoch n°116 : train_loss = 2.0025761127471924, val_loss = 0.7357401847839355\n",
      "epoch n°117 : train_loss = 2.000272035598755, val_loss = 0.7363021969795227\n",
      "epoch n°118 : train_loss = 2.0034749507904053, val_loss = 0.7313282489776611\n",
      "epoch n°119 : train_loss = 2.003446102142334, val_loss = 0.7346231937408447\n",
      "epoch n°120 : train_loss = 1.9984562397003174, val_loss = 0.7332544922828674\n",
      "epoch n°121 : train_loss = 1.9982668161392212, val_loss = 0.7358217835426331\n",
      "epoch n°122 : train_loss = 1.99986732006073, val_loss = 0.7310740947723389\n",
      "epoch n°123 : train_loss = 1.9851115942001343, val_loss = 0.7323151230812073\n",
      "epoch n°124 : train_loss = 2.001115083694458, val_loss = 0.7303400635719299\n",
      "epoch n°125 : train_loss = 1.990473985671997, val_loss = 0.7333955764770508\n",
      "epoch n°126 : train_loss = 1.9994301795959473, val_loss = 0.7350506782531738\n",
      "epoch n°127 : train_loss = 1.9920718669891357, val_loss = 0.7316909432411194\n",
      "epoch n°128 : train_loss = 2.003751039505005, val_loss = 0.730305552482605\n",
      "epoch n°129 : train_loss = 1.9931564331054688, val_loss = 0.7370175123214722\n",
      "epoch n°130 : train_loss = 1.9962053298950195, val_loss = 0.7303014397621155\n",
      "epoch n°131 : train_loss = 2.007025718688965, val_loss = 0.7350597977638245\n",
      "epoch n°132 : train_loss = 1.9959087371826172, val_loss = 0.7314902544021606\n",
      "epoch n°133 : train_loss = 2.0006308555603027, val_loss = 0.7320907711982727\n",
      "epoch n°134 : train_loss = 1.9838734865188599, val_loss = 0.731018602848053\n",
      "epoch n°135 : train_loss = 1.9868817329406738, val_loss = 0.7350133657455444\n",
      "epoch n°136 : train_loss = 1.9889225959777832, val_loss = 0.73170405626297\n",
      "epoch n°137 : train_loss = 1.999089002609253, val_loss = 0.7297422289848328\n",
      "epoch n°138 : train_loss = 1.99350106716156, val_loss = 0.735200047492981\n",
      "epoch n°139 : train_loss = 1.9896425008773804, val_loss = 0.7345513105392456\n",
      "epoch n°140 : train_loss = 1.9931902885437012, val_loss = 0.7347815036773682\n",
      "epoch n°141 : train_loss = 2.0036087036132812, val_loss = 0.7331980466842651\n",
      "epoch n°142 : train_loss = 1.997225284576416, val_loss = 0.7305378913879395\n",
      "epoch n°143 : train_loss = 1.9922945499420166, val_loss = 0.7305656671524048\n",
      "epoch n°144 : train_loss = 1.97408127784729, val_loss = 0.7356153726577759\n",
      "epoch n°145 : train_loss = 1.9895310401916504, val_loss = 0.728705883026123\n",
      "epoch n°146 : train_loss = 1.9769785404205322, val_loss = 0.7259891033172607\n",
      "epoch n°147 : train_loss = 1.9972361326217651, val_loss = 0.7295514345169067\n",
      "epoch n°148 : train_loss = 1.9781502485275269, val_loss = 0.7340786457061768\n",
      "epoch n°149 : train_loss = 1.9857410192489624, val_loss = 0.7297239303588867\n",
      "epoch n°150 : train_loss = 1.989546298980713, val_loss = 0.7344815731048584\n",
      "epoch n°151 : train_loss = 1.965880274772644, val_loss = 0.7311257719993591\n",
      "epoch n°152 : train_loss = 1.9734430313110352, val_loss = 0.7299772500991821\n",
      "epoch n°153 : train_loss = 1.977689266204834, val_loss = 0.7305024266242981\n",
      "epoch n°154 : train_loss = 1.9878566265106201, val_loss = 0.7314921617507935\n",
      "epoch n°155 : train_loss = 1.9771722555160522, val_loss = 0.7269608378410339\n",
      "epoch n°156 : train_loss = 1.9786475896835327, val_loss = 0.7266343832015991\n",
      "epoch n°157 : train_loss = 1.9719599485397339, val_loss = 0.7284714579582214\n",
      "epoch n°158 : train_loss = 1.9771672487258911, val_loss = 0.7264928221702576\n",
      "epoch n°159 : train_loss = 1.9757790565490723, val_loss = 0.7315393686294556\n",
      "epoch n°160 : train_loss = 1.9697283506393433, val_loss = 0.7317769527435303\n",
      "epoch n°161 : train_loss = 1.98162043094635, val_loss = 0.7331709265708923\n",
      "epoch n°162 : train_loss = 1.970658302307129, val_loss = 0.7269775867462158\n",
      "epoch n°163 : train_loss = 1.974172592163086, val_loss = 0.7343509793281555\n",
      "epoch n°164 : train_loss = 1.9803829193115234, val_loss = 0.7277171611785889\n",
      "epoch n°165 : train_loss = 1.966618299484253, val_loss = 0.7264624834060669\n",
      "epoch n°166 : train_loss = 1.9608560800552368, val_loss = 0.7263049483299255\n",
      "epoch n°167 : train_loss = 1.973875880241394, val_loss = 0.730774462223053\n",
      "epoch n°168 : train_loss = 1.962303876876831, val_loss = 0.7312871217727661\n",
      "epoch n°169 : train_loss = 1.9799928665161133, val_loss = 0.7292040586471558\n",
      "epoch n°170 : train_loss = 1.9749220609664917, val_loss = 0.73049396276474\n",
      "epoch n°171 : train_loss = 1.959778904914856, val_loss = 0.7341294884681702\n",
      "epoch n°172 : train_loss = 1.9641083478927612, val_loss = 0.729789674282074\n",
      "epoch n°173 : train_loss = 1.968687653541565, val_loss = 0.7326778173446655\n",
      "epoch n°174 : train_loss = 1.957500696182251, val_loss = 0.7294600605964661\n",
      "epoch n°175 : train_loss = 1.9601414203643799, val_loss = 0.7343773245811462\n",
      "epoch n°176 : train_loss = 1.9605751037597656, val_loss = 0.7319836020469666\n",
      "epoch n°177 : train_loss = 1.9594718217849731, val_loss = 0.7280710339546204\n",
      "epoch n°178 : train_loss = 1.9605027437210083, val_loss = 0.7303889989852905\n",
      "epoch n°179 : train_loss = 1.9504038095474243, val_loss = 0.7227058410644531\n",
      "epoch n°180 : train_loss = 1.9641543626785278, val_loss = 0.7263402938842773\n",
      "epoch n°181 : train_loss = 1.9520514011383057, val_loss = 0.7341635823249817\n",
      "epoch n°182 : train_loss = 1.962138056755066, val_loss = 0.7243383526802063\n",
      "epoch n°183 : train_loss = 1.9653880596160889, val_loss = 0.7269458770751953\n",
      "epoch n°184 : train_loss = 1.9584711790084839, val_loss = 0.7311854958534241\n",
      "epoch n°185 : train_loss = 1.9398118257522583, val_loss = 0.7294513583183289\n",
      "epoch n°186 : train_loss = 1.9555963277816772, val_loss = 0.7260401248931885\n",
      "epoch n°187 : train_loss = 1.962534785270691, val_loss = 0.7271503210067749\n",
      "epoch n°188 : train_loss = 1.9510011672973633, val_loss = 0.726597011089325\n",
      "epoch n°189 : train_loss = 1.9479351043701172, val_loss = 0.7258272767066956\n",
      "epoch n°190 : train_loss = 1.9529463052749634, val_loss = 0.7228536009788513\n",
      "epoch n°191 : train_loss = 1.9485909938812256, val_loss = 0.7263790965080261\n",
      "epoch n°192 : train_loss = 1.9471558332443237, val_loss = 0.7257568836212158\n",
      "epoch n°193 : train_loss = 1.94636070728302, val_loss = 0.7236882448196411\n",
      "epoch n°194 : train_loss = 1.947174310684204, val_loss = 0.7249938249588013\n",
      "epoch n°195 : train_loss = 1.9444886445999146, val_loss = 0.7269560098648071\n",
      "epoch n°196 : train_loss = 1.9541761875152588, val_loss = 0.732517421245575\n",
      "epoch n°197 : train_loss = 1.9505372047424316, val_loss = 0.7265889644622803\n",
      "epoch n°198 : train_loss = 1.943412184715271, val_loss = 0.7271294593811035\n",
      "epoch n°199 : train_loss = 1.950134515762329, val_loss = 0.7276554107666016\n",
      "epoch n°200 : train_loss = 1.9550421237945557, val_loss = 0.7268325090408325\n",
      "epoch n°201 : train_loss = 1.937307596206665, val_loss = 0.7286218404769897\n",
      "epoch n°202 : train_loss = 1.9400802850723267, val_loss = 0.7260599136352539\n",
      "epoch n°203 : train_loss = 1.9463971853256226, val_loss = 0.7322566509246826\n",
      "epoch n°204 : train_loss = 1.9582440853118896, val_loss = 0.7281339168548584\n",
      "epoch n°205 : train_loss = 1.946946620941162, val_loss = 0.7294893860816956\n",
      "epoch n°206 : train_loss = 1.9378905296325684, val_loss = 0.727931797504425\n",
      "epoch n°207 : train_loss = 1.939457893371582, val_loss = 0.720118522644043\n",
      "epoch n°208 : train_loss = 1.9324917793273926, val_loss = 0.7245709896087646\n",
      "epoch n°209 : train_loss = 1.9390236139297485, val_loss = 0.7260686755180359\n",
      "epoch n°210 : train_loss = 1.9362022876739502, val_loss = 0.7231301069259644\n",
      "epoch n°211 : train_loss = 1.9295964241027832, val_loss = 0.7293135523796082\n",
      "epoch n°212 : train_loss = 1.9395676851272583, val_loss = 0.7234331369400024\n",
      "epoch n°213 : train_loss = 1.9446018934249878, val_loss = 0.7237773537635803\n",
      "epoch n°214 : train_loss = 1.945451259613037, val_loss = 0.7278886437416077\n",
      "epoch n°215 : train_loss = 1.9509308338165283, val_loss = 0.723702609539032\n",
      "epoch n°216 : train_loss = 1.9444024562835693, val_loss = 0.7275902032852173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°217 : train_loss = 1.9441889524459839, val_loss = 0.7248989343643188\n",
      "epoch n°218 : train_loss = 1.95054292678833, val_loss = 0.7246847152709961\n",
      "epoch n°219 : train_loss = 1.9434691667556763, val_loss = 0.7268033027648926\n",
      "epoch n°220 : train_loss = 1.9372066259384155, val_loss = 0.729185163974762\n",
      "epoch n°221 : train_loss = 1.938293218612671, val_loss = 0.7301806807518005\n",
      "epoch n°222 : train_loss = 1.9396922588348389, val_loss = 0.7287731766700745\n",
      "epoch n°223 : train_loss = 1.9374659061431885, val_loss = 0.7261753678321838\n",
      "epoch n°224 : train_loss = 1.9425712823867798, val_loss = 0.7231400012969971\n",
      "epoch n°225 : train_loss = 1.9292234182357788, val_loss = 0.7240047454833984\n",
      "epoch n°226 : train_loss = 1.9448230266571045, val_loss = 0.7239157557487488\n",
      "epoch n°227 : train_loss = 1.931715488433838, val_loss = 0.7260242104530334\n",
      "epoch n°228 : train_loss = 1.939355731010437, val_loss = 0.7244805693626404\n",
      "epoch n°229 : train_loss = 1.9430782794952393, val_loss = 0.7276484966278076\n",
      "epoch n°230 : train_loss = 1.9517017602920532, val_loss = 0.7267925143241882\n",
      "epoch n°231 : train_loss = 1.9336148500442505, val_loss = 0.7232842445373535\n",
      "epoch n°232 : train_loss = 1.9302904605865479, val_loss = 0.7205081582069397\n",
      "epoch n°233 : train_loss = 1.9354888200759888, val_loss = 0.727234959602356\n",
      "epoch n°234 : train_loss = 1.9377028942108154, val_loss = 0.7243183255195618\n",
      "epoch n°235 : train_loss = 1.9419081211090088, val_loss = 0.7247084975242615\n",
      "epoch n°236 : train_loss = 1.94504714012146, val_loss = 0.7243036031723022\n",
      "epoch n°237 : train_loss = 1.9324678182601929, val_loss = 0.726606547832489\n",
      "epoch n°238 : train_loss = 1.9381574392318726, val_loss = 0.7260587811470032\n",
      "epoch n°239 : train_loss = 1.939283013343811, val_loss = 0.7296543717384338\n",
      "epoch n°240 : train_loss = 1.941845417022705, val_loss = 0.7318497896194458\n",
      "epoch n°241 : train_loss = 1.9665417671203613, val_loss = 0.7317185401916504\n",
      "epoch n°242 : train_loss = 1.9480621814727783, val_loss = 0.7293215990066528\n",
      "epoch n°243 : train_loss = 1.9653432369232178, val_loss = 0.7290397882461548\n",
      "epoch n°244 : train_loss = 1.9612668752670288, val_loss = 0.7322230935096741\n",
      "epoch n°245 : train_loss = 1.9600000381469727, val_loss = 0.7316649556159973\n",
      "epoch n°246 : train_loss = 1.9726617336273193, val_loss = 0.7293094396591187\n",
      "epoch n°247 : train_loss = 1.9644391536712646, val_loss = 0.7382767200469971\n",
      "epoch n°248 : train_loss = 1.9731780290603638, val_loss = 0.7316350936889648\n",
      "epoch n°249 : train_loss = 1.9582629203796387, val_loss = 0.7285096645355225\n",
      "epoch n°250 : train_loss = 1.9561198949813843, val_loss = 0.7299529314041138\n",
      "epoch n°251 : train_loss = 1.9725909233093262, val_loss = 0.7286822199821472\n",
      "epoch n°252 : train_loss = 1.9663046598434448, val_loss = 0.7311269640922546\n",
      "epoch n°253 : train_loss = 1.964318037033081, val_loss = 0.7252461314201355\n",
      "epoch n°254 : train_loss = 1.9673137664794922, val_loss = 0.732230544090271\n",
      "epoch n°255 : train_loss = 1.9605976343154907, val_loss = 0.727193295955658\n",
      "epoch n°256 : train_loss = 1.966686487197876, val_loss = 0.7300940752029419\n",
      "epoch n°257 : train_loss = 1.9553800821304321, val_loss = 0.7361946105957031\n",
      "epoch n°258 : train_loss = 1.9796792268753052, val_loss = 0.7346791625022888\n",
      "epoch n°259 : train_loss = 1.9534612894058228, val_loss = 0.730344831943512\n",
      "epoch n°260 : train_loss = 1.946513056755066, val_loss = 0.7328235507011414\n",
      "epoch n°261 : train_loss = 1.9571479558944702, val_loss = 0.7305107116699219\n",
      "epoch n°262 : train_loss = 1.9675648212432861, val_loss = 0.7262647747993469\n",
      "epoch n°263 : train_loss = 1.9549814462661743, val_loss = 0.7297496795654297\n",
      "epoch n°264 : train_loss = 1.9727861881256104, val_loss = 0.7314860820770264\n",
      "epoch n°265 : train_loss = 1.9665637016296387, val_loss = 0.729151725769043\n",
      "epoch n°266 : train_loss = 1.9419573545455933, val_loss = 0.7316136956214905\n",
      "epoch n°267 : train_loss = 1.9509069919586182, val_loss = 0.7277002930641174\n",
      "epoch n°268 : train_loss = 1.9560943841934204, val_loss = 0.7277534604072571\n",
      "epoch n°269 : train_loss = 1.9499143362045288, val_loss = 0.7325217127799988\n",
      "epoch n°270 : train_loss = 1.956899642944336, val_loss = 0.7258710861206055\n",
      "epoch n°271 : train_loss = 1.9472582340240479, val_loss = 0.7305850386619568\n",
      "epoch n°272 : train_loss = 1.9471181631088257, val_loss = 0.7322814464569092\n",
      "epoch n°273 : train_loss = 1.9600837230682373, val_loss = 0.7325146198272705\n",
      "epoch n°274 : train_loss = 1.9478381872177124, val_loss = 0.7344933748245239\n",
      "epoch n°275 : train_loss = 1.9524726867675781, val_loss = 0.7256802320480347\n",
      "epoch n°276 : train_loss = 1.9610289335250854, val_loss = 0.7315846681594849\n",
      "epoch n°277 : train_loss = 1.948569655418396, val_loss = 0.7325862646102905\n",
      "epoch n°278 : train_loss = 1.9596740007400513, val_loss = 0.7315418124198914\n",
      "epoch n°279 : train_loss = 1.9544415473937988, val_loss = 0.7243768572807312\n",
      "epoch n°280 : train_loss = 1.9587657451629639, val_loss = 0.7331138849258423\n",
      "epoch n°281 : train_loss = 1.9601634740829468, val_loss = 0.7290093302726746\n",
      "epoch n°282 : train_loss = 1.9542871713638306, val_loss = 0.7272498607635498\n",
      "epoch n°283 : train_loss = 1.963468074798584, val_loss = 0.7302106022834778\n",
      "epoch n°284 : train_loss = 1.9425750970840454, val_loss = 0.7305665016174316\n",
      "epoch n°285 : train_loss = 1.948706865310669, val_loss = 0.7269644737243652\n",
      "epoch n°286 : train_loss = 1.9519541263580322, val_loss = 0.7310737371444702\n",
      "epoch n°287 : train_loss = 1.9539395570755005, val_loss = 0.7339076995849609\n",
      "epoch n°288 : train_loss = 1.9499343633651733, val_loss = 0.7265311479568481\n",
      "epoch n°289 : train_loss = 1.9600430727005005, val_loss = 0.732049822807312\n",
      "epoch n°290 : train_loss = 1.9496088027954102, val_loss = 0.7275800108909607\n",
      "epoch n°291 : train_loss = 1.946025013923645, val_loss = 0.7295148968696594\n",
      "epoch n°292 : train_loss = 1.948984980583191, val_loss = 0.7288655042648315\n",
      "epoch n°293 : train_loss = 1.945417881011963, val_loss = 0.7257041931152344\n",
      "epoch n°294 : train_loss = 1.9534800052642822, val_loss = 0.7288985848426819\n",
      "epoch n°295 : train_loss = 1.9469159841537476, val_loss = 0.7304449081420898\n",
      "epoch n°296 : train_loss = 1.9415253400802612, val_loss = 0.729606032371521\n",
      "epoch n°297 : train_loss = 1.9359931945800781, val_loss = 0.7314769625663757\n",
      "epoch n°298 : train_loss = 1.9468445777893066, val_loss = 0.7270894050598145\n",
      "epoch n°299 : train_loss = 1.933582067489624, val_loss = 0.730776846408844\n",
      "epoch n°300 : train_loss = 1.934748888015747, val_loss = 0.7273528575897217\n",
      "epoch n°301 : train_loss = 1.9467484951019287, val_loss = 0.730283260345459\n",
      "epoch n°302 : train_loss = 1.9447433948516846, val_loss = 0.724851131439209\n",
      "epoch n°303 : train_loss = 1.9393728971481323, val_loss = 0.7286028861999512\n",
      "epoch n°304 : train_loss = 1.9488259553909302, val_loss = 0.72538822889328\n",
      "epoch n°305 : train_loss = 1.9504413604736328, val_loss = 0.7321034073829651\n",
      "epoch n°306 : train_loss = 1.9408127069473267, val_loss = 0.7285349369049072\n",
      "epoch n°307 : train_loss = 1.938317060470581, val_loss = 0.7253667116165161\n",
      "epoch n°308 : train_loss = 1.9365519285202026, val_loss = 0.7290861010551453\n",
      "epoch n°309 : train_loss = 1.9419238567352295, val_loss = 0.7316994071006775\n",
      "epoch n°310 : train_loss = 1.9306011199951172, val_loss = 0.7330124974250793\n",
      "epoch n°311 : train_loss = 1.9476393461227417, val_loss = 0.7245675921440125\n",
      "epoch n°312 : train_loss = 1.9537657499313354, val_loss = 0.7296363115310669\n",
      "epoch n°313 : train_loss = 1.9295531511306763, val_loss = 0.7307754158973694\n",
      "epoch n°314 : train_loss = 1.9476722478866577, val_loss = 0.727838397026062\n",
      "epoch n°315 : train_loss = 1.9501336812973022, val_loss = 0.7291256785392761\n",
      "epoch n°316 : train_loss = 1.9327484369277954, val_loss = 0.7310925126075745\n",
      "epoch n°317 : train_loss = 1.9405382871627808, val_loss = 0.7285155057907104\n",
      "epoch n°318 : train_loss = 1.9286537170410156, val_loss = 0.7255687713623047\n",
      "epoch n°319 : train_loss = 1.9324352741241455, val_loss = 0.7276653051376343\n",
      "epoch n°320 : train_loss = 1.9354504346847534, val_loss = 0.7306310534477234\n",
      "epoch n°321 : train_loss = 1.9285696744918823, val_loss = 0.7234432697296143\n",
      "epoch n°322 : train_loss = 1.9330382347106934, val_loss = 0.7272803783416748\n",
      "epoch n°323 : train_loss = 1.9351881742477417, val_loss = 0.7294692993164062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°324 : train_loss = 1.9371881484985352, val_loss = 0.7279180288314819\n",
      "epoch n°325 : train_loss = 1.9293138980865479, val_loss = 0.7245843410491943\n",
      "epoch n°326 : train_loss = 1.9295066595077515, val_loss = 0.7291541695594788\n",
      "epoch n°327 : train_loss = 1.9269031286239624, val_loss = 0.7290233969688416\n",
      "epoch n°328 : train_loss = 1.9316941499710083, val_loss = 0.7250645160675049\n",
      "epoch n°329 : train_loss = 1.9333678483963013, val_loss = 0.7332742810249329\n",
      "epoch n°330 : train_loss = 1.9271864891052246, val_loss = 0.7303193211555481\n",
      "epoch n°331 : train_loss = 1.9298357963562012, val_loss = 0.7306008338928223\n",
      "epoch n°332 : train_loss = 1.9413584470748901, val_loss = 0.7327774167060852\n",
      "epoch n°333 : train_loss = 1.9311381578445435, val_loss = 0.7293131351470947\n",
      "epoch n°334 : train_loss = 1.9267569780349731, val_loss = 0.7280483841896057\n",
      "epoch n°335 : train_loss = 1.9254437685012817, val_loss = 0.7278575301170349\n",
      "epoch n°336 : train_loss = 1.9158735275268555, val_loss = 0.7254029512405396\n",
      "epoch n°337 : train_loss = 1.92946195602417, val_loss = 0.7282419204711914\n",
      "epoch n°338 : train_loss = 1.9203492403030396, val_loss = 0.7268552184104919\n",
      "epoch n°339 : train_loss = 1.9250259399414062, val_loss = 0.7276274561882019\n",
      "epoch n°340 : train_loss = 1.9153159856796265, val_loss = 0.730812668800354\n",
      "epoch n°341 : train_loss = 1.9240810871124268, val_loss = 0.7329012155532837\n",
      "epoch n°342 : train_loss = 1.9241868257522583, val_loss = 0.7218886613845825\n",
      "epoch n°343 : train_loss = 1.931560754776001, val_loss = 0.7284068465232849\n",
      "epoch n°344 : train_loss = 1.9319788217544556, val_loss = 0.7252042293548584\n",
      "epoch n°345 : train_loss = 1.92823326587677, val_loss = 0.7225866913795471\n",
      "epoch n°346 : train_loss = 1.9230914115905762, val_loss = 0.7279930114746094\n",
      "epoch n°347 : train_loss = 1.9300774335861206, val_loss = 0.7285640835762024\n",
      "epoch n°348 : train_loss = 1.9354885816574097, val_loss = 0.7242445349693298\n",
      "epoch n°349 : train_loss = 1.9223600625991821, val_loss = 0.7244387865066528\n",
      "epoch n°350 : train_loss = 1.9230539798736572, val_loss = 0.7292546629905701\n",
      "epoch n°351 : train_loss = 1.9235726594924927, val_loss = 0.7327016592025757\n",
      "epoch n°352 : train_loss = 1.9308900833129883, val_loss = 0.7288282513618469\n",
      "epoch n°353 : train_loss = 1.906076192855835, val_loss = 0.7313061952590942\n",
      "epoch n°354 : train_loss = 1.91450035572052, val_loss = 0.7233704328536987\n",
      "epoch n°355 : train_loss = 1.924189567565918, val_loss = 0.7260530591011047\n",
      "epoch n°356 : train_loss = 1.921828269958496, val_loss = 0.7302823066711426\n",
      "epoch n°357 : train_loss = 1.9172395467758179, val_loss = 0.7301714420318604\n",
      "epoch n°358 : train_loss = 1.9184595346450806, val_loss = 0.7301740050315857\n",
      "epoch n°359 : train_loss = 1.9120874404907227, val_loss = 0.7230525016784668\n",
      "epoch n°360 : train_loss = 1.9224498271942139, val_loss = 0.7266143560409546\n",
      "epoch n°361 : train_loss = 1.914793610572815, val_loss = 0.7239416837692261\n",
      "epoch n°362 : train_loss = 1.9228583574295044, val_loss = 0.7253984212875366\n",
      "epoch n°363 : train_loss = 1.919411063194275, val_loss = 0.7243171334266663\n",
      "epoch n°364 : train_loss = 1.9179093837738037, val_loss = 0.7269884347915649\n",
      "epoch n°365 : train_loss = 1.9188698530197144, val_loss = 0.7229008078575134\n",
      "epoch n°366 : train_loss = 1.9155179262161255, val_loss = 0.7239319682121277\n",
      "epoch n°367 : train_loss = 1.9184588193893433, val_loss = 0.7265047430992126\n",
      "epoch n°368 : train_loss = 1.9094123840332031, val_loss = 0.7250782251358032\n",
      "epoch n°369 : train_loss = 1.9127304553985596, val_loss = 0.7266971468925476\n",
      "epoch n°370 : train_loss = 1.9162230491638184, val_loss = 0.7264124155044556\n",
      "epoch n°371 : train_loss = 1.8928771018981934, val_loss = 0.7236635088920593\n",
      "epoch n°372 : train_loss = 1.8958834409713745, val_loss = 0.7292735576629639\n",
      "epoch n°373 : train_loss = 1.9106084108352661, val_loss = 0.7278360724449158\n",
      "epoch n°374 : train_loss = 1.9072304964065552, val_loss = 0.7270247340202332\n",
      "epoch n°375 : train_loss = 1.9185044765472412, val_loss = 0.7270190119743347\n",
      "epoch n°376 : train_loss = 1.9130960702896118, val_loss = 0.728277325630188\n",
      "epoch n°377 : train_loss = 1.9188083410263062, val_loss = 0.727910041809082\n",
      "epoch n°378 : train_loss = 1.9082565307617188, val_loss = 0.7255646586418152\n",
      "epoch n°379 : train_loss = 1.9139728546142578, val_loss = 0.721798837184906\n",
      "epoch n°380 : train_loss = 1.912829875946045, val_loss = 0.7268731594085693\n",
      "epoch n°381 : train_loss = 1.9232189655303955, val_loss = 0.7238603234291077\n",
      "epoch n°382 : train_loss = 1.9150322675704956, val_loss = 0.7283161282539368\n",
      "epoch n°383 : train_loss = 1.9118033647537231, val_loss = 0.7287020683288574\n",
      "epoch n°384 : train_loss = 1.9082558155059814, val_loss = 0.7276514768600464\n",
      "epoch n°385 : train_loss = 1.901484727859497, val_loss = 0.7268890738487244\n",
      "epoch n°386 : train_loss = 1.907999873161316, val_loss = 0.7256439924240112\n",
      "epoch n°387 : train_loss = 1.8915996551513672, val_loss = 0.7299643158912659\n",
      "epoch n°388 : train_loss = 1.8986858129501343, val_loss = 0.7264021635055542\n",
      "epoch n°389 : train_loss = 1.9032870531082153, val_loss = 0.7266850471496582\n",
      "epoch n°390 : train_loss = 1.9055126905441284, val_loss = 0.7275077104568481\n",
      "epoch n°391 : train_loss = 1.9078749418258667, val_loss = 0.7256998419761658\n",
      "epoch n°392 : train_loss = 1.8986653089523315, val_loss = 0.7257227897644043\n",
      "epoch n°393 : train_loss = 1.9172849655151367, val_loss = 0.7278292775154114\n",
      "epoch n°394 : train_loss = 1.9055653810501099, val_loss = 0.72632896900177\n",
      "epoch n°395 : train_loss = 1.9010441303253174, val_loss = 0.7185544967651367\n",
      "epoch n°396 : train_loss = 1.9026761054992676, val_loss = 0.7274481654167175\n",
      "epoch n°397 : train_loss = 1.9099204540252686, val_loss = 0.7274879217147827\n",
      "epoch n°398 : train_loss = 1.91585373878479, val_loss = 0.7285239100456238\n",
      "epoch n°399 : train_loss = 1.9067119359970093, val_loss = 0.7240889072418213\n",
      "epoch n°400 : train_loss = 1.9006530046463013, val_loss = 0.7277576923370361\n",
      "epoch n°401 : train_loss = 1.8916696310043335, val_loss = 0.7243505120277405\n",
      "epoch n°402 : train_loss = 1.8972821235656738, val_loss = 0.7249568700790405\n",
      "epoch n°403 : train_loss = 1.8990179300308228, val_loss = 0.7261390089988708\n",
      "epoch n°404 : train_loss = 1.8991432189941406, val_loss = 0.7259793877601624\n",
      "epoch n°405 : train_loss = 1.8843340873718262, val_loss = 0.7216736078262329\n",
      "epoch n°406 : train_loss = 1.8971887826919556, val_loss = 0.7272120118141174\n",
      "epoch n°407 : train_loss = 1.9137979745864868, val_loss = 0.7252945303916931\n",
      "epoch n°408 : train_loss = 1.908774733543396, val_loss = 0.7252107858657837\n",
      "epoch n°409 : train_loss = 1.906976580619812, val_loss = 0.7293948531150818\n",
      "epoch n°410 : train_loss = 1.9016201496124268, val_loss = 0.7257798910140991\n",
      "epoch n°411 : train_loss = 1.904608964920044, val_loss = 0.7191947102546692\n",
      "epoch n°412 : train_loss = 1.889978051185608, val_loss = 0.7290765047073364\n",
      "epoch n°413 : train_loss = 1.9054332971572876, val_loss = 0.731198251247406\n",
      "epoch n°414 : train_loss = 1.9020402431488037, val_loss = 0.7255937457084656\n",
      "epoch n°415 : train_loss = 1.9084515571594238, val_loss = 0.7256483435630798\n",
      "epoch n°416 : train_loss = 1.8999117612838745, val_loss = 0.7229805588722229\n",
      "epoch n°417 : train_loss = 1.8975818157196045, val_loss = 0.7261108160018921\n",
      "epoch n°418 : train_loss = 1.8996691703796387, val_loss = 0.7257384061813354\n",
      "epoch n°419 : train_loss = 1.9031137228012085, val_loss = 0.7253038883209229\n",
      "epoch n°420 : train_loss = 1.8954006433486938, val_loss = 0.7288447618484497\n",
      "epoch n°421 : train_loss = 1.8989489078521729, val_loss = 0.722356379032135\n",
      "epoch n°422 : train_loss = 1.8983324766159058, val_loss = 0.7225607633590698\n",
      "epoch n°423 : train_loss = 1.887338399887085, val_loss = 0.7301679253578186\n",
      "epoch n°424 : train_loss = 1.8883529901504517, val_loss = 0.7215975522994995\n",
      "epoch n°425 : train_loss = 1.8972411155700684, val_loss = 0.7210658192634583\n",
      "epoch n°426 : train_loss = 1.8898746967315674, val_loss = 0.7261953353881836\n",
      "epoch n°427 : train_loss = 1.903451919555664, val_loss = 0.7292273044586182\n",
      "epoch n°428 : train_loss = 1.8960168361663818, val_loss = 0.7299997806549072\n",
      "epoch n°429 : train_loss = 1.9005882740020752, val_loss = 0.7262105941772461\n",
      "epoch n°430 : train_loss = 1.8922033309936523, val_loss = 0.7216980457305908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°431 : train_loss = 1.8900516033172607, val_loss = 0.7274057865142822\n",
      "epoch n°432 : train_loss = 1.8967887163162231, val_loss = 0.7206237316131592\n",
      "epoch n°433 : train_loss = 1.8943637609481812, val_loss = 0.7211494445800781\n",
      "epoch n°434 : train_loss = 1.882729172706604, val_loss = 0.7221794128417969\n",
      "epoch n°435 : train_loss = 1.8988847732543945, val_loss = 0.7269898056983948\n",
      "epoch n°436 : train_loss = 1.8785099983215332, val_loss = 0.7241218090057373\n",
      "epoch n°437 : train_loss = 1.8921942710876465, val_loss = 0.7239080667495728\n",
      "epoch n°438 : train_loss = 1.9000368118286133, val_loss = 0.7227378487586975\n",
      "epoch n°439 : train_loss = 1.8972253799438477, val_loss = 0.725124716758728\n",
      "epoch n°440 : train_loss = 1.8989442586898804, val_loss = 0.7260505557060242\n",
      "epoch n°441 : train_loss = 1.885783076286316, val_loss = 0.7254689931869507\n",
      "epoch n°442 : train_loss = 1.9018280506134033, val_loss = 0.7283207774162292\n",
      "epoch n°443 : train_loss = 1.9041774272918701, val_loss = 0.7253906726837158\n",
      "epoch n°444 : train_loss = 1.8871004581451416, val_loss = 0.7268569469451904\n",
      "epoch n°445 : train_loss = 1.8811194896697998, val_loss = 0.7226126194000244\n",
      "epoch n°446 : train_loss = 1.9065065383911133, val_loss = 0.7253912091255188\n",
      "epoch n°447 : train_loss = 1.8875725269317627, val_loss = 0.7217013835906982\n",
      "epoch n°448 : train_loss = 1.8935891389846802, val_loss = 0.7191485166549683\n",
      "epoch n°449 : train_loss = 1.8874573707580566, val_loss = 0.7262285351753235\n",
      "epoch n°450 : train_loss = 1.89248788356781, val_loss = 0.7277748584747314\n",
      "epoch n°451 : train_loss = 1.9000356197357178, val_loss = 0.72512286901474\n",
      "epoch n°452 : train_loss = 1.9003888368606567, val_loss = 0.7286947965621948\n",
      "epoch n°453 : train_loss = 1.8859962224960327, val_loss = 0.7283486723899841\n",
      "epoch n°454 : train_loss = 1.9050813913345337, val_loss = 0.7258548140525818\n",
      "epoch n°455 : train_loss = 1.8944159746170044, val_loss = 0.7271251678466797\n",
      "epoch n°456 : train_loss = 1.8877732753753662, val_loss = 0.7289427518844604\n",
      "epoch n°457 : train_loss = 1.890651822090149, val_loss = 0.7250202298164368\n",
      "epoch n°458 : train_loss = 1.8868283033370972, val_loss = 0.7173980474472046\n",
      "epoch n°459 : train_loss = 1.8790730237960815, val_loss = 0.7236304879188538\n",
      "epoch n°460 : train_loss = 1.884574294090271, val_loss = 0.7269417643547058\n",
      "epoch n°461 : train_loss = 1.8839738368988037, val_loss = 0.7238503098487854\n",
      "epoch n°462 : train_loss = 1.8882986307144165, val_loss = 0.7233901023864746\n",
      "epoch n°463 : train_loss = 1.8815630674362183, val_loss = 0.7251845002174377\n",
      "epoch n°464 : train_loss = 1.9021097421646118, val_loss = 0.7223469614982605\n",
      "epoch n°465 : train_loss = 1.8897966146469116, val_loss = 0.7225636839866638\n",
      "epoch n°466 : train_loss = 1.8842469453811646, val_loss = 0.7242969870567322\n",
      "epoch n°467 : train_loss = 1.8934892416000366, val_loss = 0.7261561155319214\n",
      "epoch n°468 : train_loss = 1.8878577947616577, val_loss = 0.7263855934143066\n",
      "epoch n°469 : train_loss = 1.8873227834701538, val_loss = 0.7287541031837463\n",
      "epoch n°470 : train_loss = 1.8834787607192993, val_loss = 0.7263758778572083\n",
      "epoch n°471 : train_loss = 1.8800116777420044, val_loss = 0.7241125106811523\n",
      "epoch n°472 : train_loss = 1.8886070251464844, val_loss = 0.7274080514907837\n",
      "epoch n°473 : train_loss = 1.8845263719558716, val_loss = 0.7267118096351624\n",
      "epoch n°474 : train_loss = 1.892756462097168, val_loss = 0.7239977717399597\n",
      "epoch n°475 : train_loss = 1.8963254690170288, val_loss = 0.7200340628623962\n",
      "epoch n°476 : train_loss = 1.8879077434539795, val_loss = 0.7306819558143616\n",
      "epoch n°477 : train_loss = 1.8944523334503174, val_loss = 0.7243633270263672\n",
      "epoch n°478 : train_loss = 1.8805276155471802, val_loss = 0.7249945402145386\n",
      "epoch n°479 : train_loss = 1.8901207447052002, val_loss = 0.7194546461105347\n",
      "epoch n°480 : train_loss = 1.9002610445022583, val_loss = 0.7280263304710388\n",
      "epoch n°481 : train_loss = 1.8892194032669067, val_loss = 0.7258905172348022\n",
      "epoch n°482 : train_loss = 1.8931525945663452, val_loss = 0.724050760269165\n",
      "epoch n°483 : train_loss = 1.887215256690979, val_loss = 0.7274594306945801\n",
      "epoch n°484 : train_loss = 1.8824158906936646, val_loss = 0.7282657027244568\n",
      "epoch n°485 : train_loss = 1.8814971446990967, val_loss = 0.725441038608551\n",
      "epoch n°486 : train_loss = 1.890487790107727, val_loss = 0.7246880531311035\n",
      "epoch n°487 : train_loss = 1.8785539865493774, val_loss = 0.7246736884117126\n",
      "epoch n°488 : train_loss = 1.8905452489852905, val_loss = 0.725440502166748\n",
      "epoch n°489 : train_loss = 1.8931164741516113, val_loss = 0.7218877673149109\n",
      "epoch n°490 : train_loss = 1.8918108940124512, val_loss = 0.7268995642662048\n",
      "epoch n°491 : train_loss = 1.8851505517959595, val_loss = 0.7244288325309753\n",
      "epoch n°492 : train_loss = 1.8863085508346558, val_loss = 0.7236686944961548\n",
      "epoch n°493 : train_loss = 1.894769549369812, val_loss = 0.7245078682899475\n",
      "epoch n°494 : train_loss = 1.8896257877349854, val_loss = 0.7230635285377502\n",
      "epoch n°495 : train_loss = 1.8964381217956543, val_loss = 0.7197867631912231\n",
      "epoch n°496 : train_loss = 1.9092686176300049, val_loss = 0.7316033840179443\n",
      "epoch n°497 : train_loss = 1.92328679561615, val_loss = 0.7282371520996094\n",
      "epoch n°498 : train_loss = 1.9263149499893188, val_loss = 0.7284709811210632\n",
      "epoch n°499 : train_loss = 1.9128942489624023, val_loss = 0.7269481420516968\n",
      "epoch n°500 : train_loss = 1.9040145874023438, val_loss = 0.7294820547103882\n",
      "epoch n°501 : train_loss = 1.9178030490875244, val_loss = 0.7264434695243835\n",
      "epoch n°502 : train_loss = 1.917770504951477, val_loss = 0.7316040992736816\n",
      "epoch n°503 : train_loss = 1.9242383241653442, val_loss = 0.7253100275993347\n",
      "epoch n°504 : train_loss = 1.9117794036865234, val_loss = 0.7318525314331055\n",
      "epoch n°505 : train_loss = 1.910475254058838, val_loss = 0.7267295718193054\n",
      "epoch n°506 : train_loss = 1.9197992086410522, val_loss = 0.7273702621459961\n",
      "epoch n°507 : train_loss = 1.9190362691879272, val_loss = 0.7301269173622131\n",
      "epoch n°508 : train_loss = 1.918089747428894, val_loss = 0.7280896306037903\n",
      "epoch n°509 : train_loss = 1.9112430810928345, val_loss = 0.7312736511230469\n",
      "epoch n°510 : train_loss = 1.9125806093215942, val_loss = 0.7329912185668945\n",
      "epoch n°511 : train_loss = 1.920669436454773, val_loss = 0.7302936911582947\n",
      "epoch n°512 : train_loss = 1.9147239923477173, val_loss = 0.7339248061180115\n",
      "epoch n°513 : train_loss = 1.9231117963790894, val_loss = 0.733634889125824\n",
      "epoch n°514 : train_loss = 1.934123158454895, val_loss = 0.7285917401313782\n",
      "epoch n°515 : train_loss = 1.9181832075119019, val_loss = 0.7266419529914856\n",
      "epoch n°516 : train_loss = 1.9221652746200562, val_loss = 0.7280222177505493\n",
      "epoch n°517 : train_loss = 1.9098267555236816, val_loss = 0.7240201234817505\n",
      "epoch n°518 : train_loss = 1.9203484058380127, val_loss = 0.7361857295036316\n",
      "epoch n°519 : train_loss = 1.9186713695526123, val_loss = 0.725050151348114\n",
      "epoch n°520 : train_loss = 1.9208897352218628, val_loss = 0.7304882407188416\n",
      "epoch n°521 : train_loss = 1.9227339029312134, val_loss = 0.7325349450111389\n",
      "epoch n°522 : train_loss = 1.9193177223205566, val_loss = 0.7277012467384338\n",
      "epoch n°523 : train_loss = 1.9184588193893433, val_loss = 0.7276033759117126\n",
      "epoch n°524 : train_loss = 1.9232823848724365, val_loss = 0.7244306802749634\n",
      "epoch n°525 : train_loss = 1.9181369543075562, val_loss = 0.726698100566864\n",
      "epoch n°526 : train_loss = 1.9085475206375122, val_loss = 0.7322048544883728\n",
      "epoch n°527 : train_loss = 1.9180264472961426, val_loss = 0.7291905283927917\n",
      "epoch n°528 : train_loss = 1.917942762374878, val_loss = 0.7279389500617981\n",
      "epoch n°529 : train_loss = 1.9147238731384277, val_loss = 0.7261911034584045\n",
      "epoch n°530 : train_loss = 1.9204256534576416, val_loss = 0.7291548848152161\n",
      "epoch n°531 : train_loss = 1.9282159805297852, val_loss = 0.7304852604866028\n",
      "epoch n°532 : train_loss = 1.9134607315063477, val_loss = 0.7273213267326355\n",
      "epoch n°533 : train_loss = 1.906198501586914, val_loss = 0.7311909794807434\n",
      "epoch n°534 : train_loss = 1.9218353033065796, val_loss = 0.7325830459594727\n",
      "epoch n°535 : train_loss = 1.9244753122329712, val_loss = 0.7232261896133423\n",
      "epoch n°536 : train_loss = 1.9084222316741943, val_loss = 0.7314463257789612\n",
      "epoch n°537 : train_loss = 1.9086096286773682, val_loss = 0.723944902420044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°538 : train_loss = 1.916497826576233, val_loss = 0.7283567786216736\n",
      "epoch n°539 : train_loss = 1.9253374338150024, val_loss = 0.7257511615753174\n",
      "epoch n°540 : train_loss = 1.92128586769104, val_loss = 0.7260490655899048\n",
      "epoch n°541 : train_loss = 1.9111379384994507, val_loss = 0.7288305759429932\n",
      "epoch n°542 : train_loss = 1.914551854133606, val_loss = 0.7308788895606995\n",
      "epoch n°543 : train_loss = 1.9099669456481934, val_loss = 0.7301961779594421\n",
      "epoch n°544 : train_loss = 1.919555902481079, val_loss = 0.7291899919509888\n",
      "epoch n°545 : train_loss = 1.9077311754226685, val_loss = 0.7325179576873779\n",
      "epoch n°546 : train_loss = 1.9138327836990356, val_loss = 0.7298210859298706\n",
      "epoch n°547 : train_loss = 1.9118400812149048, val_loss = 0.7385391592979431\n",
      "epoch n°548 : train_loss = 1.9201281070709229, val_loss = 0.7285159230232239\n",
      "epoch n°549 : train_loss = 1.9317315816879272, val_loss = 0.7277671098709106\n",
      "epoch n°550 : train_loss = 1.9182851314544678, val_loss = 0.7271064519882202\n",
      "epoch n°551 : train_loss = 1.9243841171264648, val_loss = 0.7294867634773254\n",
      "epoch n°552 : train_loss = 1.9125516414642334, val_loss = 0.7269960641860962\n",
      "epoch n°553 : train_loss = 1.9094072580337524, val_loss = 0.7331057786941528\n",
      "epoch n°554 : train_loss = 1.9153646230697632, val_loss = 0.7279789447784424\n",
      "epoch n°555 : train_loss = 1.9159588813781738, val_loss = 0.7271387577056885\n",
      "epoch n°556 : train_loss = 1.9135587215423584, val_loss = 0.7328327894210815\n",
      "epoch n°557 : train_loss = 1.918868899345398, val_loss = 0.7216283679008484\n",
      "epoch n°558 : train_loss = 1.922159194946289, val_loss = 0.72646564245224\n",
      "epoch n°559 : train_loss = 1.9037474393844604, val_loss = 0.7286741137504578\n",
      "epoch n°560 : train_loss = 1.9181597232818604, val_loss = 0.7283944487571716\n",
      "epoch n°561 : train_loss = 1.9163097143173218, val_loss = 0.7295258045196533\n",
      "epoch n°562 : train_loss = 1.9151231050491333, val_loss = 0.7211973667144775\n",
      "epoch n°563 : train_loss = 1.9097429513931274, val_loss = 0.7294027805328369\n",
      "epoch n°564 : train_loss = 1.913497805595398, val_loss = 0.7294519543647766\n",
      "epoch n°565 : train_loss = 1.9148869514465332, val_loss = 0.7249343395233154\n",
      "epoch n°566 : train_loss = 1.9141203165054321, val_loss = 0.7266449332237244\n",
      "epoch n°567 : train_loss = 1.9137593507766724, val_loss = 0.7254862785339355\n",
      "epoch n°568 : train_loss = 1.9118506908416748, val_loss = 0.7223416566848755\n",
      "epoch n°569 : train_loss = 1.9158966541290283, val_loss = 0.7285537719726562\n",
      "epoch n°570 : train_loss = 1.9126874208450317, val_loss = 0.7334067225456238\n",
      "epoch n°571 : train_loss = 1.91457998752594, val_loss = 0.7230285406112671\n",
      "epoch n°572 : train_loss = 1.9011471271514893, val_loss = 0.730254590511322\n",
      "epoch n°573 : train_loss = 1.9117108583450317, val_loss = 0.7310655117034912\n",
      "epoch n°574 : train_loss = 1.917312741279602, val_loss = 0.7270637154579163\n",
      "epoch n°575 : train_loss = 1.9204981327056885, val_loss = 0.7291145920753479\n",
      "epoch n°576 : train_loss = 1.9216731786727905, val_loss = 0.7228828072547913\n",
      "epoch n°577 : train_loss = 1.8983232975006104, val_loss = 0.7297057509422302\n",
      "epoch n°578 : train_loss = 1.9085193872451782, val_loss = 0.7253412008285522\n",
      "epoch n°579 : train_loss = 1.9235458374023438, val_loss = 0.7260844111442566\n",
      "epoch n°580 : train_loss = 1.9147733449935913, val_loss = 0.7260065674781799\n",
      "epoch n°581 : train_loss = 1.911846399307251, val_loss = 0.7278919219970703\n",
      "epoch n°582 : train_loss = 1.9171303510665894, val_loss = 0.7280861139297485\n",
      "epoch n°583 : train_loss = 1.9109920263290405, val_loss = 0.7269712090492249\n",
      "epoch n°584 : train_loss = 1.9085628986358643, val_loss = 0.7287892699241638\n",
      "epoch n°585 : train_loss = 1.9193698167800903, val_loss = 0.7290709614753723\n",
      "epoch n°586 : train_loss = 1.9071887731552124, val_loss = 0.728275716304779\n",
      "epoch n°587 : train_loss = 1.9169130325317383, val_loss = 0.7279126048088074\n",
      "epoch n°588 : train_loss = 1.9080156087875366, val_loss = 0.7212415337562561\n",
      "epoch n°589 : train_loss = 1.9102839231491089, val_loss = 0.7246643304824829\n",
      "epoch n°590 : train_loss = 1.9016480445861816, val_loss = 0.7231441140174866\n",
      "epoch n°591 : train_loss = 1.9098730087280273, val_loss = 0.73194420337677\n",
      "epoch n°592 : train_loss = 1.898684024810791, val_loss = 0.7283167243003845\n",
      "epoch n°593 : train_loss = 1.9007405042648315, val_loss = 0.7282299995422363\n",
      "epoch n°594 : train_loss = 1.9057917594909668, val_loss = 0.7253300547599792\n",
      "epoch n°595 : train_loss = 1.8999676704406738, val_loss = 0.7281712889671326\n",
      "epoch n°596 : train_loss = 1.9102818965911865, val_loss = 0.7291600704193115\n",
      "epoch n°597 : train_loss = 1.9152294397354126, val_loss = 0.7293140888214111\n",
      "epoch n°598 : train_loss = 1.903764247894287, val_loss = 0.7282239198684692\n",
      "epoch n°599 : train_loss = 1.9020379781723022, val_loss = 0.727242112159729\n",
      "epoch n°600 : train_loss = 1.9068703651428223, val_loss = 0.7328968644142151\n",
      "epoch n°601 : train_loss = 1.9182391166687012, val_loss = 0.7254199981689453\n",
      "epoch n°602 : train_loss = 1.9139710664749146, val_loss = 0.7231316566467285\n",
      "epoch n°603 : train_loss = 1.8931490182876587, val_loss = 0.7241566181182861\n",
      "epoch n°604 : train_loss = 1.9062464237213135, val_loss = 0.7273073196411133\n",
      "epoch n°605 : train_loss = 1.8967376947402954, val_loss = 0.7239375114440918\n",
      "epoch n°606 : train_loss = 1.9129674434661865, val_loss = 0.7312109470367432\n",
      "epoch n°607 : train_loss = 1.9087817668914795, val_loss = 0.7272306084632874\n",
      "epoch n°608 : train_loss = 1.9038455486297607, val_loss = 0.7283723950386047\n",
      "epoch n°609 : train_loss = 1.8914893865585327, val_loss = 0.724054753780365\n",
      "epoch n°610 : train_loss = 1.9040168523788452, val_loss = 0.7330276370048523\n",
      "epoch n°611 : train_loss = 1.8967583179473877, val_loss = 0.7238696217536926\n",
      "epoch n°612 : train_loss = 1.8961942195892334, val_loss = 0.7285934686660767\n",
      "epoch n°613 : train_loss = 1.9019132852554321, val_loss = 0.7296766042709351\n",
      "epoch n°614 : train_loss = 1.8986026048660278, val_loss = 0.730889081954956\n",
      "epoch n°615 : train_loss = 1.9052135944366455, val_loss = 0.7260228395462036\n",
      "epoch n°616 : train_loss = 1.901482343673706, val_loss = 0.7282307147979736\n",
      "epoch n°617 : train_loss = 1.8997784852981567, val_loss = 0.7294033169746399\n",
      "epoch n°618 : train_loss = 1.9050668478012085, val_loss = 0.7330262064933777\n",
      "epoch n°619 : train_loss = 1.906085729598999, val_loss = 0.7221141457557678\n",
      "epoch n°620 : train_loss = 1.8927760124206543, val_loss = 0.7263588309288025\n",
      "epoch n°621 : train_loss = 1.9128105640411377, val_loss = 0.7286633849143982\n",
      "epoch n°622 : train_loss = 1.9119212627410889, val_loss = 0.7227416038513184\n",
      "epoch n°623 : train_loss = 1.8960435390472412, val_loss = 0.7245503067970276\n",
      "epoch n°624 : train_loss = 1.8954933881759644, val_loss = 0.7270315289497375\n",
      "epoch n°625 : train_loss = 1.8978030681610107, val_loss = 0.7266672253608704\n",
      "epoch n°626 : train_loss = 1.8959031105041504, val_loss = 0.729935884475708\n",
      "epoch n°627 : train_loss = 1.8983970880508423, val_loss = 0.7297829985618591\n",
      "epoch n°628 : train_loss = 1.9010831117630005, val_loss = 0.7256289720535278\n",
      "epoch n°629 : train_loss = 1.9028446674346924, val_loss = 0.7289329171180725\n",
      "epoch n°630 : train_loss = 1.8900582790374756, val_loss = 0.7225163578987122\n",
      "epoch n°631 : train_loss = 1.892820954322815, val_loss = 0.724956214427948\n",
      "epoch n°632 : train_loss = 1.9046849012374878, val_loss = 0.7252417206764221\n",
      "epoch n°633 : train_loss = 1.9015953540802002, val_loss = 0.73149573802948\n",
      "epoch n°634 : train_loss = 1.9007270336151123, val_loss = 0.7278850078582764\n",
      "epoch n°635 : train_loss = 1.8938690423965454, val_loss = 0.7245106101036072\n",
      "epoch n°636 : train_loss = 1.9002933502197266, val_loss = 0.7270197868347168\n",
      "epoch n°637 : train_loss = 1.8949247598648071, val_loss = 0.7259407639503479\n",
      "epoch n°638 : train_loss = 1.88425612449646, val_loss = 0.7233738899230957\n",
      "epoch n°639 : train_loss = 1.8959152698516846, val_loss = 0.7279331088066101\n",
      "epoch n°640 : train_loss = 1.8944816589355469, val_loss = 0.7295572757720947\n",
      "epoch n°641 : train_loss = 1.8966612815856934, val_loss = 0.727067768573761\n",
      "epoch n°642 : train_loss = 1.892134189605713, val_loss = 0.7269478440284729\n",
      "epoch n°643 : train_loss = 1.8890267610549927, val_loss = 0.7222179174423218\n",
      "epoch n°644 : train_loss = 1.9034080505371094, val_loss = 0.7248401045799255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n°645 : train_loss = 1.9001492261886597, val_loss = 0.7273584008216858\n",
      "epoch n°646 : train_loss = 1.8930867910385132, val_loss = 0.7265209555625916\n",
      "epoch n°647 : train_loss = 1.8970509767532349, val_loss = 0.7248619198799133\n",
      "epoch n°648 : train_loss = 1.8865424394607544, val_loss = 0.7253111600875854\n",
      "epoch n°649 : train_loss = 1.9050257205963135, val_loss = 0.7253919839859009\n",
      "epoch n°650 : train_loss = 1.9017170667648315, val_loss = 0.7273049354553223\n",
      "epoch n°651 : train_loss = 1.8910820484161377, val_loss = 0.730283796787262\n",
      "epoch n°652 : train_loss = 1.8957760334014893, val_loss = 0.7265708446502686\n",
      "epoch n°653 : train_loss = 1.8865418434143066, val_loss = 0.7302185297012329\n",
      "epoch n°654 : train_loss = 1.8945828676223755, val_loss = 0.7266501784324646\n",
      "epoch n°655 : train_loss = 1.9016592502593994, val_loss = 0.7327569127082825\n",
      "epoch n°656 : train_loss = 1.8979665040969849, val_loss = 0.7288306355476379\n",
      "epoch n°657 : train_loss = 1.893412470817566, val_loss = 0.725906252861023\n",
      "epoch n°658 : train_loss = 1.8912161588668823, val_loss = 0.7254818081855774\n",
      "epoch n°659 : train_loss = 1.900338888168335, val_loss = 0.7267880439758301\n",
      "epoch n°660 : train_loss = 1.890336513519287, val_loss = 0.7267357707023621\n",
      "epoch n°661 : train_loss = 1.8893437385559082, val_loss = 0.7265012264251709\n",
      "epoch n°662 : train_loss = 1.8894765377044678, val_loss = 0.722072184085846\n"
     ]
    }
   ],
   "source": [
    "params = get_params(32,6,8,32,4,0.1)\n",
    "model = AttNet(*params,clf_dims=[1024,256,64])\n",
    "model = model.to(device)\n",
    "optim = torch.optim.AdamW(model.parameters(),lr = 0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim,16,2)\n",
    "state = State(model,optim,scheduler)\n",
    "\n",
    "fname = \"models/stateATT_6L_12cont_biased sampling.pth\" \n",
    "start = time.time()\n",
    "Train,Eval,_ = main(train_dataloader, val_dataloader,fname=fname,epochs=4080,state=state,use_mut=False)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Train)),Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(Eval)),Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c37ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_12cont.pth\" \n",
    "SCORES = eval_model(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030e689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_12cont.pth\" \n",
    "SCORES = eval_model_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd7c67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = \"models/stateATT_6L_12cont.pth\" \n",
    "SCORES = eval_model_acc_PID(fname,train_dataloader,test_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6798e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(SCORES[2*i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e097c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(SCORES[2*i+1].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
